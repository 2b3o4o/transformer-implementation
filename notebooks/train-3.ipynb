{"cells":[{"cell_type":"code","execution_count":101,"metadata":{},"outputs":[],"source":["import torch\n","from torch import tensor, sin, cos\n","from math import sqrt\n","from torch.nn.functional import softmax\n","import spacy\n","from torchtext.vocab import GloVe\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[],"source":["train_files = [\"../data/addition_problems-train.txt\"]\n","test_files = [\"../data/addition_problems-test.txt\"]\n","\n","train_texts = []\n","test_texts = []\n","\n","for file_name in train_files:\n","    with open(file_name, 'r', encoding='utf-8') as file:\n","        train_texts.append(file.read())\n","\n","for file_name in test_files:\n","    with open(file_name, 'r', encoding='utf-8') as file:\n","        test_texts.append(file.read())"]},{"cell_type":"markdown","metadata":{},"source":["Set up our tokenizer and 3rd party embedding library"]},{"cell_type":"code","execution_count":103,"metadata":{},"outputs":[],"source":["tokenizer = spacy.load(\"en_core_web_sm\")\n","all_tokens = []\n","all_tokens.extend(['<PAD>', '<UNK>']) # special tokens\n","\n","for text in train_texts + test_texts:\n","    doc = tokenizer(text)\n","    tokens = [token.text for token in doc]\n","    all_tokens.extend(tokens)\n","\n","unique_tokens = set(all_tokens)\n","vocab = {token: i for i, token in enumerate(unique_tokens)}\n","reverse_vocab = {i: token for i, token in enumerate(unique_tokens)}"]},{"cell_type":"markdown","metadata":{},"source":["Define key helper functions used throughout training and inference"]},{"cell_type":"code","execution_count":104,"metadata":{},"outputs":[],"source":["def par_attention(queries: tensor, keys: tensor, values: tensor, dim: int) -> tensor:\n","    raw_weights = torch.bmm(queries, keys.transpose(1, 2))\n","\n","    mask = torch.tril(torch.ones_like(raw_weights), diagonal=0)\n","    raw_weights = raw_weights.masked_fill(mask == 0, float('-inf'))\n","    # print(f\"raw_weights.shape:{raw_weights.shape}\\nraw_weights: {raw_weights}\")\n","\n","    scale_factor = sqrt(dim)\n","    scaled_weights = softmax(raw_weights / scale_factor, dim=2)\n","    # print(f\"scaled_weights.shape:{scaled_weights.shape}\\nscaled_weights: {scaled_weights}\")\n","\n","    # now scaled weights is a matrix where each row represents the scaled weights produced based on a given query.\n","    # meanwhile values just has a value vector on each row.\n","\n","    reshaped_scaled_weights = scaled_weights.view(scaled_weights.shape[0], scaled_weights.shape[1], scaled_weights.shape[2], 1)\n","    reshaped_values = values.view(values.shape[0], values.shape[1], 1, values.shape[2])\n","\n","    scaled_values = reshaped_scaled_weights * reshaped_values\n","\n","    contextualized_values = torch.sum(scaled_values, 2)\n","    return contextualized_values\n","\n","def prep_input_string(str, context_len) -> tensor:\n","    \"\"\"Takes an input string with up to context_len tokens and returns a tensor full of integers, which can be passed into the model\"\"\"\n","    tokens = tokenizer(str)\n","\n","    output = torch.full([context_len], vocab['<PAD>'])\n","    for in_pos in range(len(tokens)):\n","        out_pos = context_len - len(tokens) + in_pos\n","        output[out_pos] = vocab[tokens[in_pos].text]\n","\n","    return output\n","\n","def prep_tokens(tokens, length) -> tensor:\n","    output = torch.full([length], vocab['<PAD>'])\n","    for in_pos in range(len(tokens)):\n","        out_pos = length - len(tokens) + in_pos\n","        output[out_pos] = vocab[tokens[in_pos].text]\n","\n","    return output\n","\n","# slice_offset is the number of tokens separating the start of one slice from the start of the previous.\n","# slice_offset == slice_length means no overlap, slice_offset == 1 means maximum overlap.\n","def slice_text(text: str, slice_length, slice_offset, context_len) -> tensor:\n","    slices = []\n","    tokens = tokenizer(text)\n","\n","    for i in range(0, len(tokens), slice_offset):\n","        slices.append(tokens[i:i+slice_length])\n","\n","    output = torch.zeros([len(slices), context_len + 1])\n","    for i, slice in enumerate(slices):\n","        output[i] = prep_tokens(slice, context_len + 1)\n","\n","    assert output.shape[1] == context_len + 1\n","    return output"]},{"cell_type":"code","execution_count":105,"metadata":{},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, dims, context_len):\n","        super().__init__()\n","        self.dims = dims\n","        self.context_len = context_len\n","        self.proj = nn.Linear(1, self.dims)\n","\n","        positional_matrix = torch.zeros([self.context_len, self.dims])\n","        for pos in range(0, self.context_len):\n","            for i in range(0, self.dims // 2):\n","                positional_matrix[pos][2 * i] = sin(torch.tensor(pos / (10000 ** (2 * i / self.dims))))\n","                positional_matrix[pos][2 * i + 1] = cos(torch.tensor(pos / (10000 ** (2 * i / self.dims))))\n","\n","        self.register_buffer('positional_matrix', positional_matrix)\n","\n","\n","    def forward(self, x: tensor) -> tensor:\n","        # x is token ids. we'll say it's context_len integers packed into a tensor, where each one represents a token. it can also be batched.\n","        output = torch.zeros([x.shape[0], self.context_len, self.dims])\n","        for batch in range(0, x.shape[0]):\n","            output[batch] = self.proj(x[batch].view(x.shape[1], -1))\n","            output[batch] += self.positional_matrix\n","        # print(f\"self.context_len={self.context_len}\")\n","        # print(f\"Shape of x before assert: {x.shape}\")\n","        # assert x.shape[1] == self.context_len\n","        # output = self.proj(x)\n","        # output += self.positional_matrix\n","        return output"]},{"cell_type":"markdown","metadata":{},"source":["Define the architecture of the model, including all subcomponents"]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[],"source":["class AttentionHead(nn.Module):\n","    # For simplicity, I assume query, key, and value vectors have the same dimensionality\n","    def __init__(self, model_dim, vectors_dim):\n","        super().__init__()\n","        self.model_dim = model_dim\n","        self.vectors_dim = vectors_dim\n","        self.Q_proj = nn.Linear(model_dim, vectors_dim, bias=False)\n","        self.K_proj = nn.Linear(model_dim, vectors_dim, bias=False)\n","        self.V_proj = nn.Linear(model_dim, vectors_dim, bias=False)\n","\n","    def forward(self, x):\n","        # each row of x is a vector representing the meaning of the token at the corresponding position with whatever context we've attained so far.\n","        Q = self.Q_proj(x)\n","        K = self.K_proj(x)\n","        V = self.V_proj(x)\n","        # print(\"Shape of Q matrix: \", Q.shape)\n","        # print(\"Shape of K matrix: \", K.shape)\n","        # print(\"Shape of V matrix: \", V.shape)\n","        output = par_attention(Q, K, V, self.vectors_dim)\n","        return output\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, model_dim, num_heads):\n","        super().__init__()\n","        self.att_heads = nn.ModuleList([AttentionHead(model_dim, model_dim // num_heads) for _ in range(num_heads)])\n","        self.proj = nn.Linear(model_dim, model_dim, bias=False)\n","\n","    def forward(self, x):\n","        head_outputs = [head(x) for head in self.att_heads]\n","        x = torch.concat(head_outputs, dim=2)\n","        x = self.proj(x)\n","        return x\n","        \n","class TransformerLayer(nn.Module):\n","    def __init__(self, model_dim, num_heads, ff_hidden_dim, context_len):\n","        super().__init__()\n","        self.attention_block = MultiHeadAttention(model_dim, num_heads)\n","        self.norm1 = nn.LayerNorm(normalized_shape=[context_len, model_dim])\n","        self.ff1 = nn.Linear(model_dim, ff_hidden_dim)\n","        self.ff_relu = nn.ReLU()\n","        self.ff2 = nn.Linear(ff_hidden_dim, model_dim)\n","        self.norm2 = nn.LayerNorm(normalized_shape=[context_len, model_dim])\n","\n","    def forward(self, x):\n","        x_res = x\n","        x = self.attention_block(x)\n","        x += x_res\n","        x = self.norm1(x)\n","\n","        x_res = x\n","        x = self.ff1(x)\n","        x = self.ff_relu(x)\n","        x = self.ff2(x)\n","        x += x_res\n","        x = self.norm2(x)\n","\n","        return x\n","\n","class TransformerNetwork(nn.Module):\n","    def __init__(self, num_layers, model_dim, att_heads, ff_hidden_dim, context_len, output_dict_size):\n","        super().__init__()\n","        self.encode_embed = PositionalEncoding(model_dim, context_len)\n","        self.trans_layers = nn.ModuleList([TransformerLayer(model_dim, att_heads, ff_hidden_dim, context_len) for _ in range(num_layers)])\n","        self.word_predictor = nn.Linear(model_dim * context_len, output_dict_size)\n","        # print(f\"word_predictor input dimension: {model_dim * context_len}\\noutput dimension: {output_dict_size}\")\n","\n","    def forward(self, x):\n","        # print(f\"Received x of shape: {x.shape}\")\n","        x = self.encode_embed(x)\n","        for layer in self.trans_layers:\n","            x = layer.forward(x)\n","        x = x.view(x.shape[0], -1)\n","        # print(f\"Reshaped x to shape: {x.shape}\")\n","        x = self.word_predictor(x)\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["Tools to quickly build a dataset that can be fed into the model"]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset\n","\n","class CompletionDataset(Dataset):\n","    def __init__(self, features, labels):\n","        self.features = features\n","        self.labels = labels.long()\n","\n","    def __len__(self):\n","        return self.features.shape[0]\n","\n","    def __getitem__(self, index):\n","        return self.features[index], self.labels[index]\n","\n","# Note: slices include features + label. So if you have context length 256, you can set slice length 257 and be fine.\n","def build_dataset(slices: tensor) -> CompletionDataset:    \n","    features = slices[:, :-1]\n","    labels = slices[:, -1]\n","    \n","    dataset = CompletionDataset(features, labels)\n","    return dataset\n","\n","context_len = 8\n","slice_length = context_len + 1\n","slice_offset = slice_length\n","\n","# note: slice_text returns an n by slice_length tensor of ints. (from vocab)\n","train_slices = [] # list of tensors\n","for text in train_texts:\n","    train_slices.append(slice_text(text, slice_length, slice_offset, context_len))\n","    train_slices.append(slice_text(text, slice_length - 2, 1, context_len))\n","    train_slices.append(slice_text(text, 5, 1, context_len))\n","train_dataset = build_dataset(torch.cat(train_slices, dim=0))\n","\n","test_slices = [] # list of tensors\n","for text in train_texts:\n","    test_slices.append(slice_text(text, slice_length - 3, 1, context_len))\n","test_dataset = build_dataset(torch.cat(test_slices, dim=0))"]},{"cell_type":"code","execution_count":108,"metadata":{},"outputs":[{"data":{"text/plain":["torch.Size([9])"]},"execution_count":108,"metadata":{},"output_type":"execute_result"}],"source":["train_slices[0][0].shape"]},{"cell_type":"markdown","metadata":{},"source":["Initialize model. Output dict size is the size of the final layer."]},{"cell_type":"code","execution_count":119,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["dictionary_len: 24\n"]}],"source":["dictionary_len = len(vocab)\n","model = TransformerNetwork(num_layers=4, model_dim=256, att_heads=4, ff_hidden_dim=1024, context_len=context_len, output_dict_size=dictionary_len)\n","print(f\"dictionary_len: {dictionary_len}\")\n","\n","loss_func = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.2)\n","\n","train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)"]},{"cell_type":"code","execution_count":120,"metadata":{},"outputs":[],"source":["def train_one_epoch(do_validation: bool):\n","    model.train(True)\n","    torch.set_printoptions(profile=\"short\")\n","    batches = 0\n","    avg_loss = 0\n","    for step, (features, labels) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","        preds = model(features)\n","        # print(f\"preds:{preds}\\nlabels:{labels}\")\n","        loss = loss_func(preds, labels)\n","        loss.backward()\n","\n","        # if step % 10 == 0:  # Print every 10 batches\n","        #     for name, param in model.named_parameters():\n","        #         if param.requires_grad:\n","        #             print(f\"Gradient data for {name}:\", param.grad)\n","        #             print(f\"Checking if gradients are fully zeroed: {torch.all(param.grad == 0.0).item()}\")\n","        #             print(f\"Shape: {param.grad.shape}\")\n","        #             print(f\"Mean: {param.grad.mean()}\")\n","        #             print(f\"Std: {param.grad.std()}\")\n","        #             print(f\"Min: {param.grad.min()}\")\n","        #             print(f\"Max: {param.grad.max()}\")\n","\n","        optimizer.step()\n","\n","        if step % 20 == 0:\n","            print(f\"Loss on batch {step}: {loss}\")\n","\n","        avg_loss += loss\n","        batches = step + 1\n","    \n","    avg_loss = avg_loss / batches\n","    print(f\"Average loss for training batches in this epoch: {avg_loss}\")\n","\n","    if do_validation:\n","        model.train(False)\n","        batches = 0\n","        avg_loss = 0\n","        for step, (features, labels) in enumerate(test_loader):\n","            preds = model(features)\n","            # print(f\"preds:{preds}\\nlabels:{labels}\")\n","            loss = loss_func(preds, labels)\n","            \n","            if step % 20 == 0:\n","                print(f\"Loss on batch {step}: {loss}\")\n","\n","            avg_loss += loss\n","            batches = step + 1\n","\n","        avg_loss = avg_loss / batches\n","        print(f\"Average loss for validation batches in this epoch: {avg_loss}\")\n"]},{"cell_type":"code","execution_count":121,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Loss on batch 0: 3.3379645347595215\n","Loss on batch 20: 5.100069522857666\n","Loss on batch 40: 4.845412731170654\n","Loss on batch 60: 2.484283447265625\n","Loss on batch 80: 2.2703957557678223\n","Loss on batch 100: 2.3720014095306396\n","Loss on batch 120: 2.215528964996338\n","Loss on batch 140: 1.4908981323242188\n","Loss on batch 160: 1.6193722486495972\n","Loss on batch 180: 2.212831974029541\n","Loss on batch 200: 2.318749189376831\n","Loss on batch 220: 1.9479894638061523\n","Loss on batch 240: 1.988685131072998\n","Loss on batch 260: 2.3624958992004395\n","Loss on batch 280: 1.7550816535949707\n","Loss on batch 300: 2.3493967056274414\n","Loss on batch 320: 1.558413028717041\n","Loss on batch 340: 1.573714017868042\n","Loss on batch 360: 2.027968168258667\n","Loss on batch 380: 1.9940723180770874\n","Average loss for training batches in this epoch: 4.140743255615234\n","Loss on batch 0: 1.748185157775879\n","Loss on batch 20: 1.860714077949524\n","Loss on batch 40: 1.9710698127746582\n","Loss on batch 60: 1.6917333602905273\n","Loss on batch 80: 2.3773059844970703\n","Loss on batch 100: 1.9224964380264282\n","Loss on batch 120: 1.843306541442871\n","Loss on batch 140: 1.834043025970459\n","Loss on batch 160: 2.0531365871429443\n","Loss on batch 180: 2.02736234664917\n","Average loss for validation batches in this epoch: 1.9718209505081177\n"]}],"source":["train_one_epoch(True)\n","# train_one_epoch()\n"]},{"cell_type":"code","execution_count":122,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy on test set: 45.55%\n"]}],"source":["model.eval()  # Set the model to evaluation mode\n","correct = 0\n","total = 0\n","\n","with torch.no_grad():  # Deactivates autograd, reduces memory usage and speeds up computations\n","    for features, labels in test_loader:\n","        outputs = model(features)\n","        _, predicted = torch.max(outputs.data, 1)  # Get the index of the max log-probability\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = 100 * correct / total\n","print(f\"Accuracy on test set: {accuracy}%\")\n"]},{"cell_type":"code","execution_count":112,"metadata":{},"outputs":[],"source":["def infer_completion(input_text: str, context_len):\n","    encoded_input = prep_input_string(input_text, context_len).unsqueeze(0).float()\n","    \n","    model.train(False)\n","    pred = model(encoded_input)\n","    return reverse_vocab[torch.argmax(softmax(pred, dim=1), dim=1).item()]"]},{"cell_type":"code","execution_count":115,"metadata":{},"outputs":[{"data":{"text/plain":["'\\n'"]},"execution_count":115,"metadata":{},"output_type":"execute_result"}],"source":["infer_completion(\"2 + 4 =\", context_len)"]},{"cell_type":"code","execution_count":114,"metadata":{},"outputs":[{"data":{"text/plain":["4"]},"execution_count":114,"metadata":{},"output_type":"execute_result"}],"source":["tokens = tokenizer(\"2 + 2 = \")\n","len(tokens)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
