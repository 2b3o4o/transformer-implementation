{"cells":[{"cell_type":"code","execution_count":82,"metadata":{},"outputs":[],"source":["import torch\n","from torch import tensor, sin, cos\n","from math import sqrt\n","from torch.nn.functional import softmax\n","import spacy\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n"]},{"cell_type":"code","execution_count":83,"metadata":{},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":84,"metadata":{},"outputs":[],"source":["train_files = [\"../data/addition-problems-train.txt\"]\n","test_files = [\"../data/addition-problems-test.txt\"]\n","\n","train_texts = []\n","test_texts = []\n","\n","for file_name in train_files:\n","    with open(file_name, 'r', encoding='utf-8') as file:\n","        train_texts.append(file.read())\n","\n","for file_name in test_files:\n","    with open(file_name, 'r', encoding='utf-8') as file:\n","        test_texts.append(file.read())"]},{"cell_type":"markdown","metadata":{},"source":["Set up our tokenizer and 3rd party embedding library"]},{"cell_type":"code","execution_count":85,"metadata":{},"outputs":[],"source":["tokenizer = spacy.load(\"en_core_web_sm\")\n","all_tokens = []\n","all_tokens.extend(['<PAD>', '<UNK>']) # special tokens\n","\n","for text in train_texts + test_texts:\n","    doc = tokenizer(text)\n","    tokens = [token.text for token in doc]\n","    all_tokens.extend(tokens)\n","\n","unique_tokens = set(all_tokens)\n","vocab = {token: i for i, token in enumerate(unique_tokens)}\n","reverse_vocab = {i: token for i, token in enumerate(unique_tokens)}"]},{"cell_type":"markdown","metadata":{},"source":["Define key helper functions used throughout training and inference"]},{"cell_type":"code","execution_count":86,"metadata":{},"outputs":[],"source":["def par_attention(queries: tensor, keys: tensor, values: tensor, dim: int) -> tensor:\n","    raw_weights = torch.bmm(queries, keys.transpose(1, 2))\n","\n","    mask = torch.tril(torch.ones_like(raw_weights), diagonal=0)\n","    raw_weights = raw_weights.masked_fill(mask == 0, float('-inf'))\n","    # print(f\"raw_weights.shape:{raw_weights.shape}\\nraw_weights: {raw_weights}\")\n","\n","    scale_factor = sqrt(dim)\n","    scaled_weights = softmax(raw_weights / scale_factor, dim=2)\n","    # print(f\"scaled_weights.shape:{scaled_weights.shape}\\nscaled_weights: {scaled_weights}\")\n","\n","    # now scaled weights is a matrix where each row represents the scaled weights produced based on a given query.\n","    # meanwhile values just has a value vector on each row.\n","\n","    reshaped_scaled_weights = scaled_weights.view(scaled_weights.shape[0], scaled_weights.shape[1], scaled_weights.shape[2], 1)\n","    reshaped_values = values.view(values.shape[0], values.shape[1], 1, values.shape[2])\n","\n","    scaled_values = reshaped_scaled_weights * reshaped_values\n","\n","    contextualized_values = torch.sum(scaled_values, 2)\n","    return contextualized_values\n","\n","def prep_input_string(str, context_len) -> tensor:\n","    \"\"\"Takes an input string with up to context_len tokens and returns a tensor full of integers, which can be passed into the model\"\"\"\n","    tokens = tokenizer(str)\n","\n","    output = torch.full([context_len], vocab['<PAD>'])\n","    for in_pos in range(len(tokens)):\n","        out_pos = context_len - len(tokens) + in_pos\n","        output[out_pos] = vocab[tokens[in_pos].text]\n","\n","    return output\n","\n","def prep_tokens(tokens, length) -> tensor:\n","    output = torch.full([length], vocab['<PAD>'])\n","    for in_pos in range(len(tokens)):\n","        out_pos = length - len(tokens) + in_pos\n","        output[out_pos] = vocab[tokens[in_pos].text]\n","\n","    return output\n","\n","# slice_offset is the number of tokens separating the start of one slice from the start of the previous.\n","# slice_offset == slice_length means no overlap, slice_offset == 1 means maximum overlap.\n","def slice_text(text: str, slice_length, slice_offset, context_len) -> tensor:\n","    slices = []\n","    tokens = tokenizer(text)\n","\n","    for i in range(0, len(tokens), slice_offset):\n","        slices.append(tokens[i:i+slice_length])\n","\n","    output = torch.zeros([len(slices), context_len + 1]) # use context_len + 1 because we need to include the label\n","    for i, slice in enumerate(slices):\n","        output[i] = prep_tokens(slice, context_len + 1)\n","\n","    assert output.shape[1] == context_len + 1\n","    return output.to(device)\n","\n","def slice_by_line(text: str, context_len) -> tensor:\n","    slices = text.split(\"\\n\")\n","    tokens = [tokenizer(slice) for slice in slices]\n","\n","    output = torch.zeros([len(tokens), context_len + 1])\n","    for i, token_line in enumerate(tokens):\n","        output[i] = prep_tokens(token_line, context_len + 1)\n","\n","    return output.to(device)"]},{"cell_type":"code","execution_count":87,"metadata":{},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, dims, context_len):\n","        super().__init__()\n","        self.dims = dims\n","        self.context_len = context_len\n","        self.proj = nn.Linear(1, self.dims)\n","\n","        positional_matrix = torch.zeros([self.context_len, self.dims])\n","        for pos in range(0, self.context_len):\n","            for i in range(0, self.dims // 2):\n","                positional_matrix[pos][2 * i] = sin(torch.tensor(pos / (10000 ** (2 * i / self.dims))))\n","                positional_matrix[pos][2 * i + 1] = cos(torch.tensor(pos / (10000 ** (2 * i / self.dims))))\n","        positional_matrix = positional_matrix.to(device)\n","        self.register_buffer('positional_matrix', positional_matrix)\n","        self.positional_matrix = self.positional_matrix.to(device)\n","\n","\n","    def forward(self, x: tensor) -> tensor:\n","        # x is token ids. we'll say it's context_len integers packed into a tensor, where each one represents a token. it can also be batched.\n","        output = torch.zeros([x.shape[0], self.context_len, self.dims]).to(device)\n","        for batch in range(0, x.shape[0]):\n","            output[batch] = self.proj(x[batch].view(x.shape[1], -1))\n","            output[batch] += self.positional_matrix\n","        # print(f\"self.context_len={self.context_len}\")\n","        # print(f\"Shape of x before assert: {x.shape}\")\n","        # assert x.shape[1] == self.context_len\n","        # output = self.proj(x)\n","        # output += self.positional_matrix\n","        return output"]},{"cell_type":"markdown","metadata":{},"source":["Define the architecture of the model, including all subcomponents"]},{"cell_type":"code","execution_count":88,"metadata":{},"outputs":[],"source":["class AttentionHead(nn.Module):\n","    # For simplicity, I assume query, key, and value vectors have the same dimensionality\n","    def __init__(self, model_dim, vectors_dim):\n","        super().__init__()\n","        self.model_dim = model_dim\n","        self.vectors_dim = vectors_dim\n","        self.Q_proj = nn.Linear(model_dim, vectors_dim, bias=False)\n","        self.K_proj = nn.Linear(model_dim, vectors_dim, bias=False)\n","        self.V_proj = nn.Linear(model_dim, vectors_dim, bias=False)\n","\n","    def forward(self, x):\n","        # each row of x is a vector representing the meaning of the token at the corresponding position with whatever context we've attained so far.\n","        Q = self.Q_proj(x)\n","        K = self.K_proj(x)\n","        V = self.V_proj(x)\n","        # print(\"Shape of Q matrix: \", Q.shape)\n","        # print(\"Shape of K matrix: \", K.shape)\n","        # print(\"Shape of V matrix: \", V.shape)\n","        output = par_attention(Q, K, V, self.vectors_dim)\n","        return output\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, model_dim, num_heads):\n","        super().__init__()\n","        self.att_heads = nn.ModuleList([AttentionHead(model_dim, model_dim // num_heads) for _ in range(num_heads)])\n","        self.proj = nn.Linear(model_dim, model_dim, bias=False)\n","\n","    def forward(self, x):\n","        head_outputs = [head(x) for head in self.att_heads]\n","        x = torch.concat(head_outputs, dim=2)\n","        x = self.proj(x)\n","        return x\n","        \n","class TransformerLayer(nn.Module):\n","    def __init__(self, model_dim, num_heads, ff_hidden_dim, context_len):\n","        super().__init__()\n","        self.attention_block = MultiHeadAttention(model_dim, num_heads)\n","        self.norm1 = nn.LayerNorm(normalized_shape=[context_len, model_dim])\n","        self.ff1 = nn.Linear(model_dim, ff_hidden_dim)\n","        self.ff_relu = nn.ReLU()\n","        self.ff2 = nn.Linear(ff_hidden_dim, model_dim)\n","        self.norm2 = nn.LayerNorm(normalized_shape=[context_len, model_dim])\n","\n","    def forward(self, x):\n","        x_res = x\n","        x = self.attention_block(x)\n","        x += x_res\n","        x = self.norm1(x)\n","\n","        x_res = x\n","        x = self.ff1(x)\n","        x = self.ff_relu(x)\n","        x = self.ff2(x)\n","        x += x_res\n","        x = self.norm2(x)\n","\n","        return x\n","\n","class TransformerNetwork(nn.Module):\n","    def __init__(self, num_layers, model_dim, att_heads, ff_hidden_dim, context_len, output_dict_size):\n","        super().__init__()\n","        self.encode_embed = PositionalEncoding(model_dim, context_len)\n","        self.trans_layers = nn.ModuleList([TransformerLayer(model_dim, att_heads, ff_hidden_dim, context_len) for _ in range(num_layers)])\n","\n","        # self.test_ff = nn.Linear(model_dim * context_len, 2048)\n","        # self.test_relu = nn.ReLU()\n","        # self.test_ff2 = nn.Linear(2048, model_dim * context_len)\n","        # self.test_relu2 = nn.ReLU()\n","\n","        self.word_predictor = nn.Linear(model_dim * context_len, output_dict_size)\n","        # print(f\"word_predictor input dimension: {model_dim * context_len}\\noutput dimension: {output_dict_size}\")\n","\n","    def forward(self, x):\n","        # print(f\"Received x of shape: {x.shape}\")\n","        x = self.encode_embed(x)\n","        for layer in self.trans_layers:\n","            x = layer.forward(x)\n","        x = x.view(x.shape[0], -1)\n","        # print(f\"Reshaped x to shape: {x.shape}\")\n","\n","        # x = self.test_ff(x)\n","        # x = self.test_relu(x)\n","        # x = self.test_ff2(x)\n","        # x = self.test_relu2(x)\n","\n","        x = self.word_predictor(x)\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["Tools to quickly build a dataset that can be fed into the model"]},{"cell_type":"code","execution_count":89,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset\n","\n","class CompletionDataset(Dataset):\n","    def __init__(self, features, labels):\n","        self.features = features\n","        self.labels = labels.long()\n","\n","    def __len__(self):\n","        return self.features.shape[0]\n","\n","    def __getitem__(self, index):\n","        return self.features[index], self.labels[index]\n","\n","# Note: slices include features + label. So if you have context length 256, you can set slice length 257 and be fine.\n","def build_dataset(slices: tensor) -> CompletionDataset:    \n","    features = slices[:, :-1]\n","    labels = slices[:, -1]\n","    \n","    dataset = CompletionDataset(features, labels)\n","    return dataset\n","\n","context_len = 8\n","slice_length = context_len + 1\n","slice_offset = slice_length\n","\n","# note: slice_text returns an n by slice_length tensor of ints. (from vocab)\n","train_slices = [] # list of tensors\n","for text in train_texts:\n","    train_slices.append(slice_by_line(text, context_len))\n","    # train_slices.append(slice_text(text, slice_length, slice_offset, context_len))\n","    # train_slices.append(slice_text(text, slice_length - 2, 1, context_len))\n","    # train_slices.append(slice_text(text, 5, 1, context_len))\n","train_dataset = build_dataset(torch.cat(train_slices, dim=0))\n","\n","test_slices = [] # list of tensors\n","for text in test_texts:\n","    test_slices.append(slice_by_line(text, context_len))\n","    # test_slices.append(slice_text(text, slice_length - 3, 1, context_len))\n","test_dataset = build_dataset(torch.cat(test_slices, dim=0))"]},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Features:\n","['<PAD>', '<PAD>', '<PAD>', '<PAD>', '7', '+', '1', '=']\n","Label:\n","8\n","Features:\n","['<PAD>', '<PAD>', '<PAD>', '<PAD>', '5', '+', '0', '=']\n","Label:\n","5\n"]},{"data":{"text/plain":["1001"]},"execution_count":90,"metadata":{},"output_type":"execute_result"}],"source":["def check_input_data(input):\n","    features = input[0].int().tolist()\n","    label = input[1].int().item()\n","    features_str = [reverse_vocab[f] for f in features]\n","    label_str = reverse_vocab[label]\n","    print(f\"Features:\\n{features_str}\")\n","    print(f\"Label:\\n{label_str}\")\n","\n","check_input_data(test_dataset[0])\n","check_input_data(train_dataset[0])\n","len(test_dataset)"]},{"cell_type":"markdown","metadata":{},"source":["Initialize model. Output dict size is the size of the final layer."]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["dictionary_len: 24\n"]}],"source":["dictionary_len = len(vocab)\n","model = TransformerNetwork(num_layers=4, model_dim=256, att_heads=4, ff_hidden_dim=1024, context_len=context_len, output_dict_size=dictionary_len)\n","model.to(device)\n","print(f\"dictionary_len: {dictionary_len}\")\n","\n","loss_func = torch.nn.CrossEntropyLoss()\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=128, shuffle=False)"]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[],"source":["def train_one_epoch(do_validation: bool):\n","    model.train(True)\n","    torch.set_printoptions(profile=\"short\")\n","    batches = 0\n","    avg_loss = 0\n","    for step, (features, labels) in enumerate(train_loader):\n","        features, labels = features.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        preds = model(features)\n","        # print(f\"preds:{preds}\\nlabels:{labels}\")\n","        loss = loss_func(preds, labels)\n","        loss.backward()\n","\n","        # if step % 10 == 0:  # Print every 10 batches\n","        #     for name, param in model.named_parameters():\n","        #         if param.requires_grad:\n","        #             print(f\"Gradient data for {name}:\", param.grad)\n","        #             print(f\"Checking if gradients are fully zeroed: {torch.all(param.grad == 0.0).item()}\")\n","        #             print(f\"Shape: {param.grad.shape}\")\n","        #             print(f\"Mean: {param.grad.mean()}\")\n","        #             print(f\"Std: {param.grad.std()}\")\n","        #             print(f\"Min: {param.grad.min()}\")\n","        #             print(f\"Max: {param.grad.max()}\")\n","\n","        optimizer.step()\n","\n","        # if step % 20 == 0:\n","        #     print(f\"Loss on batch {step}: {loss}\")\n","\n","        avg_loss += loss\n","        batches = step + 1\n","    \n","    avg_loss = avg_loss / batches\n","    print(f\"Average loss for training batches in this epoch: {avg_loss}\")\n","\n","    if do_validation:\n","        model.train(False)\n","        batches = 0\n","        avg_loss = 0\n","        for step, (features, labels) in enumerate(test_loader):\n","            features, labels = features.to(device), labels.to(device)\n","            preds = model(features)\n","            # print(f\"preds:{preds}\\nlabels:{labels}\")\n","            loss = loss_func(preds, labels)\n","            \n","            # if step % 20 == 0:\n","            #     print(f\"Loss on batch {step}: {loss}\")\n","\n","            avg_loss += loss\n","            batches = step + 1\n","\n","        avg_loss = avg_loss / batches\n","        print(f\"Average loss for validation batches in this epoch: {avg_loss}\")\n"]},{"cell_type":"code","execution_count":108,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0\n","Average loss for training batches in this epoch: 3.180807113647461\n","Average loss for validation batches in this epoch: 2.953734874725342\n","Epoch 1\n","Average loss for training batches in this epoch: 2.874828338623047\n","Average loss for validation batches in this epoch: 2.886913537979126\n","Epoch 2\n","Average loss for training batches in this epoch: 2.811995029449463\n","Average loss for validation batches in this epoch: 2.833907127380371\n","Epoch 3\n","Average loss for training batches in this epoch: 2.805340528488159\n","Average loss for validation batches in this epoch: 2.806431770324707\n","Epoch 4\n","Average loss for training batches in this epoch: 2.790170907974243\n","Average loss for validation batches in this epoch: 2.8056769371032715\n","Epoch 5\n","Average loss for training batches in this epoch: 2.762911558151245\n","Average loss for validation batches in this epoch: 2.75252628326416\n","Epoch 6\n","Average loss for training batches in this epoch: 2.746647834777832\n","Average loss for validation batches in this epoch: 2.7531862258911133\n","Epoch 7\n","Average loss for training batches in this epoch: 2.683985471725464\n","Average loss for validation batches in this epoch: 2.6954598426818848\n","Epoch 8\n","Average loss for training batches in this epoch: 2.645988941192627\n","Average loss for validation batches in this epoch: 2.685168743133545\n","Epoch 9\n","Average loss for training batches in this epoch: 2.624969005584717\n","Average loss for validation batches in this epoch: 2.654603958129883\n","Epoch 10\n","Average loss for training batches in this epoch: 2.599708318710327\n","Average loss for validation batches in this epoch: 2.6246249675750732\n","Epoch 11\n","Average loss for training batches in this epoch: 2.569253444671631\n","Average loss for validation batches in this epoch: 2.62107515335083\n","Epoch 12\n","Average loss for training batches in this epoch: 2.562232494354248\n","Average loss for validation batches in this epoch: 2.598682165145874\n","Epoch 13\n","Average loss for training batches in this epoch: 2.5484299659729004\n","Average loss for validation batches in this epoch: 2.596043348312378\n","Epoch 14\n","Average loss for training batches in this epoch: 2.530400514602661\n","Average loss for validation batches in this epoch: 2.554387331008911\n","Epoch 15\n","Average loss for training batches in this epoch: 2.5147464275360107\n","Average loss for validation batches in this epoch: 2.5596187114715576\n","Epoch 16\n","Average loss for training batches in this epoch: 2.516091823577881\n","Average loss for validation batches in this epoch: 2.5557847023010254\n","Epoch 17\n","Average loss for training batches in this epoch: 2.5103821754455566\n","Average loss for validation batches in this epoch: 2.5543861389160156\n","Epoch 18\n","Average loss for training batches in this epoch: 2.486666202545166\n","Average loss for validation batches in this epoch: 2.5114142894744873\n","Epoch 19\n","Average loss for training batches in this epoch: 2.433631420135498\n","Average loss for validation batches in this epoch: 2.467869520187378\n","Epoch 20\n","Average loss for training batches in this epoch: 2.410426616668701\n","Average loss for validation batches in this epoch: 2.4318511486053467\n","Epoch 21\n","Average loss for training batches in this epoch: 2.371997117996216\n","Average loss for validation batches in this epoch: 2.3607535362243652\n","Epoch 22\n","Average loss for training batches in this epoch: 2.341381311416626\n","Average loss for validation batches in this epoch: 2.447227716445923\n","Epoch 23\n","Average loss for training batches in this epoch: 2.3362114429473877\n","Average loss for validation batches in this epoch: 2.375709295272827\n","Epoch 24\n","Average loss for training batches in this epoch: 2.3207614421844482\n","Average loss for validation batches in this epoch: 2.4070944786071777\n","Epoch 25\n","Average loss for training batches in this epoch: 2.246699571609497\n","Average loss for validation batches in this epoch: 2.2347254753112793\n","Epoch 26\n","Average loss for training batches in this epoch: 2.1861732006073\n","Average loss for validation batches in this epoch: 2.229257345199585\n","Epoch 27\n","Average loss for training batches in this epoch: 2.145350456237793\n","Average loss for validation batches in this epoch: 2.2572433948516846\n","Epoch 28\n","Average loss for training batches in this epoch: 2.1286017894744873\n","Average loss for validation batches in this epoch: 2.2153449058532715\n","Epoch 29\n","Average loss for training batches in this epoch: 2.0634446144104004\n","Average loss for validation batches in this epoch: 2.102078914642334\n","Epoch 30\n","Average loss for training batches in this epoch: 1.9895013570785522\n","Average loss for validation batches in this epoch: 2.056405544281006\n","Epoch 31\n","Average loss for training batches in this epoch: 1.9365293979644775\n","Average loss for validation batches in this epoch: 1.959839105606079\n","Epoch 32\n","Average loss for training batches in this epoch: 1.918196201324463\n","Average loss for validation batches in this epoch: 1.9548572301864624\n","Epoch 33\n","Average loss for training batches in this epoch: 1.8801363706588745\n","Average loss for validation batches in this epoch: 2.111893653869629\n","Epoch 34\n","Average loss for training batches in this epoch: 1.9665355682373047\n","Average loss for validation batches in this epoch: 1.8792469501495361\n","Epoch 35\n","Average loss for training batches in this epoch: 1.80491304397583\n","Average loss for validation batches in this epoch: 1.77958083152771\n","Epoch 36\n","Average loss for training batches in this epoch: 1.702729344367981\n","Average loss for validation batches in this epoch: 1.7197741270065308\n","Epoch 37\n","Average loss for training batches in this epoch: 1.6152323484420776\n","Average loss for validation batches in this epoch: 1.5437461137771606\n","Epoch 38\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\src\\alignment-study\\transformer-implementation\\notebooks\\train-3.ipynb Cell 17\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m100\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X21sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch\u001b[39m\u001b[39m\"\u001b[39m, i)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     train_one_epoch(\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# train_one_epoch()\u001b[39;00m\n","\u001b[1;32mc:\\src\\alignment-study\\transformer-implementation\\notebooks\\train-3.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# print(f\"preds:{preds}\\nlabels:{labels}\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_func(preds, labels)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X21sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X21sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# if step % 10 == 0:  # Print every 10 batches\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X21sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m#     for name, param in model.named_parameters():\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X21sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m#         if param.requires_grad:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X21sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m#             print(f\"Min: {param.grad.min()}\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X21sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m#             print(f\"Max: {param.grad.max()}\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X21sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n","File \u001b[1;32mc:\\Users\\lukew\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n","File \u001b[1;32mc:\\Users\\lukew\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["for i in range(100):\n","    print(\"Epoch\", i)\n","    train_one_epoch(True)\n","# train_one_epoch()\n"]},{"cell_type":"code","execution_count":102,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy on test set: 23.576423576423576%\n"]}],"source":["model.eval()  # Set the model to evaluation mode\n","correct = 0\n","total = 0\n","\n","with torch.no_grad():  # Deactivates autograd, reduces memory usage and speeds up computations\n","    for features, labels in test_loader:\n","        outputs = model(features)\n","        _, predicted = torch.max(outputs.data, 1)  # Get the index of the max log-probability\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = 100 * correct / total\n","print(f\"Accuracy on test set: {accuracy}%\")\n"]},{"cell_type":"code","execution_count":95,"metadata":{},"outputs":[],"source":["def infer_completion(input_text: str, context_len):\n","    encoded_input = prep_input_string(input_text, context_len).unsqueeze(0).float()\n","    \n","    model.train(False)\n","    pred = model(encoded_input)\n","    return reverse_vocab[torch.argmax(softmax(pred, dim=1), dim=1).item()]"]},{"cell_type":"code","execution_count":96,"metadata":{},"outputs":[{"ename":"RuntimeError","evalue":"Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[1;32mc:\\src\\alignment-study\\transformer-implementation\\notebooks\\train-3.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m infer_completion(\u001b[39m\"\u001b[39;49m\u001b[39m1 + 1 =\u001b[39;49m\u001b[39m\"\u001b[39;49m, context_len)\n","\u001b[1;32mc:\\src\\alignment-study\\transformer-implementation\\notebooks\\train-3.ipynb Cell 20\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m encoded_input \u001b[39m=\u001b[39m prep_input_string(input_text, context_len)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X24sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m model\u001b[39m.\u001b[39mtrain(\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X24sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m pred \u001b[39m=\u001b[39m model(encoded_input)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X24sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mreturn\u001b[39;00m reverse_vocab[torch\u001b[39m.\u001b[39margmax(softmax(pred, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mitem()]\n","File \u001b[1;32mc:\\Users\\lukew\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","\u001b[1;32mc:\\src\\alignment-study\\transformer-implementation\\notebooks\\train-3.ipynb Cell 20\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X24sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X24sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m     \u001b[39m# print(f\"Received x of shape: {x.shape}\")\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X24sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_embed(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X24sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrans_layers:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X24sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m         x \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mforward(x)\n","File \u001b[1;32mc:\\Users\\lukew\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","\u001b[1;32mc:\\src\\alignment-study\\transformer-implementation\\notebooks\\train-3.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X24sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros([x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontext_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdims])\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X24sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X24sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     output[batch] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproj(x[batch]\u001b[39m.\u001b[39;49mview(x\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m], \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X24sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     output[batch] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_matrix\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X24sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# print(f\"self.context_len={self.context_len}\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X24sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# print(f\"Shape of x before assert: {x.shape}\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X24sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39m# assert x.shape[1] == self.context_len\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X24sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# output = self.proj(x)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-3.ipynb#X24sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# output += self.positional_matrix\u001b[39;00m\n","File \u001b[1;32mc:\\Users\\lukew\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\Users\\lukew\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n","\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"]}],"source":["infer_completion(\"1 + 1 =\", context_len)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
