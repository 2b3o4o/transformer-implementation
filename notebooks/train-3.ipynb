{"cells":[{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["import torch\n","from torch import tensor, sin, cos\n","from math import sqrt\n","from torch.nn.functional import softmax\n","import spacy\n","from torchtext.vocab import GloVe\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["train_files = [\"../data/addition-problems-train.txt\"]\n","test_files = [\"../data/addition-problems-test.txt\"]\n","\n","train_texts = []\n","test_texts = []\n","\n","for file_name in train_files:\n","    with open(file_name, 'r', encoding='utf-8') as file:\n","        train_texts.append(file.read())\n","\n","for file_name in test_files:\n","    with open(file_name, 'r', encoding='utf-8') as file:\n","        test_texts.append(file.read())"]},{"cell_type":"markdown","metadata":{},"source":["Set up our tokenizer and 3rd party embedding library"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["tokenizer = spacy.load(\"en_core_web_sm\")\n","all_tokens = []\n","all_tokens.extend(['<PAD>', '<UNK>']) # special tokens\n","\n","for text in train_texts + test_texts:\n","    doc = tokenizer(text)\n","    tokens = [token.text for token in doc]\n","    all_tokens.extend(tokens)\n","\n","unique_tokens = set(all_tokens)\n","vocab = {token: i for i, token in enumerate(unique_tokens)}\n","reverse_vocab = {i: token for i, token in enumerate(unique_tokens)}"]},{"cell_type":"markdown","metadata":{},"source":["Define key helper functions used throughout training and inference"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["def par_attention(queries: tensor, keys: tensor, values: tensor, dim: int) -> tensor:\n","    raw_weights = torch.bmm(queries, keys.transpose(1, 2))\n","\n","    mask = torch.tril(torch.ones_like(raw_weights), diagonal=0)\n","    raw_weights = raw_weights.masked_fill(mask == 0, float('-inf'))\n","    # print(f\"raw_weights.shape:{raw_weights.shape}\\nraw_weights: {raw_weights}\")\n","\n","    scale_factor = sqrt(dim)\n","    scaled_weights = softmax(raw_weights / scale_factor, dim=2)\n","    # print(f\"scaled_weights.shape:{scaled_weights.shape}\\nscaled_weights: {scaled_weights}\")\n","\n","    # now scaled weights is a matrix where each row represents the scaled weights produced based on a given query.\n","    # meanwhile values just has a value vector on each row.\n","\n","    reshaped_scaled_weights = scaled_weights.view(scaled_weights.shape[0], scaled_weights.shape[1], scaled_weights.shape[2], 1)\n","    reshaped_values = values.view(values.shape[0], values.shape[1], 1, values.shape[2])\n","\n","    scaled_values = reshaped_scaled_weights * reshaped_values\n","\n","    contextualized_values = torch.sum(scaled_values, 2)\n","    return contextualized_values\n","\n","def prep_input_string(str, context_len) -> tensor:\n","    \"\"\"Takes an input string with up to context_len tokens and returns a tensor full of integers, which can be passed into the model\"\"\"\n","    tokens = tokenizer(str)\n","\n","    output = torch.full([context_len], vocab['<PAD>'])\n","    for in_pos in range(len(tokens)):\n","        out_pos = context_len - len(tokens) + in_pos\n","        output[out_pos] = vocab[tokens[in_pos].text]\n","\n","    return output\n","\n","def prep_tokens(tokens, length) -> tensor:\n","    output = torch.full([length], vocab['<PAD>'])\n","    for in_pos in range(len(tokens)):\n","        out_pos = length - len(tokens) + in_pos\n","        output[out_pos] = vocab[tokens[in_pos].text]\n","\n","    return output\n","\n","# slice_offset is the number of tokens separating the start of one slice from the start of the previous.\n","# slice_offset == slice_length means no overlap, slice_offset == 1 means maximum overlap.\n","def slice_text(text: str, slice_length, slice_offset, context_len) -> tensor:\n","    slices = []\n","    tokens = tokenizer(text)\n","\n","    for i in range(0, len(tokens), slice_offset):\n","        slices.append(tokens[i:i+slice_length])\n","\n","    output = torch.zeros([len(slices), context_len + 1]) # use context_len + 1 because we need to include the label\n","    for i, slice in enumerate(slices):\n","        output[i] = prep_tokens(slice, context_len + 1)\n","\n","    assert output.shape[1] == context_len + 1\n","    return output\n","\n","def slice_by_line(text: str, context_len) -> tensor:\n","    slices = text.split(\"\\n\")\n","    tokens = [tokenizer(slice) for slice in slices]\n","\n","    output = torch.zeros([len(tokens), context_len + 1])\n","    for i, token_line in enumerate(tokens):\n","        output[i] = prep_tokens(token_line, context_len + 1)\n","\n","    return output"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, dims, context_len):\n","        super().__init__()\n","        self.dims = dims\n","        self.context_len = context_len\n","        self.proj = nn.Linear(1, self.dims)\n","\n","        positional_matrix = torch.zeros([self.context_len, self.dims])\n","        for pos in range(0, self.context_len):\n","            for i in range(0, self.dims // 2):\n","                positional_matrix[pos][2 * i] = sin(torch.tensor(pos / (10000 ** (2 * i / self.dims))))\n","                positional_matrix[pos][2 * i + 1] = cos(torch.tensor(pos / (10000 ** (2 * i / self.dims))))\n","\n","        self.register_buffer('positional_matrix', positional_matrix)\n","\n","\n","    def forward(self, x: tensor) -> tensor:\n","        # x is token ids. we'll say it's context_len integers packed into a tensor, where each one represents a token. it can also be batched.\n","        output = torch.zeros([x.shape[0], self.context_len, self.dims])\n","        for batch in range(0, x.shape[0]):\n","            output[batch] = self.proj(x[batch].view(x.shape[1], -1))\n","            output[batch] += self.positional_matrix\n","        # print(f\"self.context_len={self.context_len}\")\n","        # print(f\"Shape of x before assert: {x.shape}\")\n","        # assert x.shape[1] == self.context_len\n","        # output = self.proj(x)\n","        # output += self.positional_matrix\n","        return output"]},{"cell_type":"markdown","metadata":{},"source":["Define the architecture of the model, including all subcomponents"]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[],"source":["class AttentionHead(nn.Module):\n","    # For simplicity, I assume query, key, and value vectors have the same dimensionality\n","    def __init__(self, model_dim, vectors_dim):\n","        super().__init__()\n","        self.model_dim = model_dim\n","        self.vectors_dim = vectors_dim\n","        self.Q_proj = nn.Linear(model_dim, vectors_dim, bias=False)\n","        self.K_proj = nn.Linear(model_dim, vectors_dim, bias=False)\n","        self.V_proj = nn.Linear(model_dim, vectors_dim, bias=False)\n","\n","    def forward(self, x):\n","        # each row of x is a vector representing the meaning of the token at the corresponding position with whatever context we've attained so far.\n","        Q = self.Q_proj(x)\n","        K = self.K_proj(x)\n","        V = self.V_proj(x)\n","        # print(\"Shape of Q matrix: \", Q.shape)\n","        # print(\"Shape of K matrix: \", K.shape)\n","        # print(\"Shape of V matrix: \", V.shape)\n","        output = par_attention(Q, K, V, self.vectors_dim)\n","        return output\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, model_dim, num_heads):\n","        super().__init__()\n","        self.att_heads = nn.ModuleList([AttentionHead(model_dim, model_dim // num_heads) for _ in range(num_heads)])\n","        self.proj = nn.Linear(model_dim, model_dim, bias=False)\n","\n","    def forward(self, x):\n","        head_outputs = [head(x) for head in self.att_heads]\n","        x = torch.concat(head_outputs, dim=2)\n","        x = self.proj(x)\n","        return x\n","        \n","class TransformerLayer(nn.Module):\n","    def __init__(self, model_dim, num_heads, ff_hidden_dim, context_len):\n","        super().__init__()\n","        self.attention_block = MultiHeadAttention(model_dim, num_heads)\n","        self.norm1 = nn.LayerNorm(normalized_shape=[context_len, model_dim])\n","        self.ff1 = nn.Linear(model_dim, ff_hidden_dim)\n","        self.ff_relu = nn.ReLU()\n","        self.ff2 = nn.Linear(ff_hidden_dim, model_dim)\n","        self.norm2 = nn.LayerNorm(normalized_shape=[context_len, model_dim])\n","\n","    def forward(self, x):\n","        x_res = x\n","        x = self.attention_block(x)\n","        x += x_res\n","        x = self.norm1(x)\n","\n","        x_res = x\n","        x = self.ff1(x)\n","        x = self.ff_relu(x)\n","        x = self.ff2(x)\n","        x += x_res\n","        x = self.norm2(x)\n","\n","        return x\n","\n","class TransformerNetwork(nn.Module):\n","    def __init__(self, num_layers, model_dim, att_heads, ff_hidden_dim, context_len, output_dict_size):\n","        super().__init__()\n","        self.encode_embed = PositionalEncoding(model_dim, context_len)\n","        self.trans_layers = nn.ModuleList([TransformerLayer(model_dim, att_heads, ff_hidden_dim, context_len) for _ in range(num_layers)])\n","\n","        # self.test_ff = nn.Linear(model_dim * context_len, 2048)\n","        # self.test_relu = nn.ReLU()\n","        # self.test_ff2 = nn.Linear(2048, model_dim * context_len)\n","        # self.test_relu2 = nn.ReLU()\n","\n","        self.word_predictor = nn.Linear(model_dim * context_len, output_dict_size)\n","        # print(f\"word_predictor input dimension: {model_dim * context_len}\\noutput dimension: {output_dict_size}\")\n","\n","    def forward(self, x):\n","        # print(f\"Received x of shape: {x.shape}\")\n","        x = self.encode_embed(x)\n","        for layer in self.trans_layers:\n","            x = layer.forward(x)\n","        x = x.view(x.shape[0], -1)\n","        # print(f\"Reshaped x to shape: {x.shape}\")\n","\n","        # x = self.test_ff(x)\n","        # x = self.test_relu(x)\n","        # x = self.test_ff2(x)\n","        # x = self.test_relu2(x)\n","\n","        x = self.word_predictor(x)\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["Tools to quickly build a dataset that can be fed into the model"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset\n","\n","class CompletionDataset(Dataset):\n","    def __init__(self, features, labels):\n","        self.features = features\n","        self.labels = labels.long()\n","\n","    def __len__(self):\n","        return self.features.shape[0]\n","\n","    def __getitem__(self, index):\n","        return self.features[index], self.labels[index]\n","\n","# Note: slices include features + label. So if you have context length 256, you can set slice length 257 and be fine.\n","def build_dataset(slices: tensor) -> CompletionDataset:    \n","    features = slices[:, :-1]\n","    labels = slices[:, -1]\n","    \n","    dataset = CompletionDataset(features, labels)\n","    return dataset\n","\n","context_len = 8\n","slice_length = context_len + 1\n","slice_offset = slice_length\n","\n","# note: slice_text returns an n by slice_length tensor of ints. (from vocab)\n","train_slices = [] # list of tensors\n","for text in train_texts:\n","    train_slices.append(slice_by_line(text, context_len))\n","    # train_slices.append(slice_text(text, slice_length, slice_offset, context_len))\n","    # train_slices.append(slice_text(text, slice_length - 2, 1, context_len))\n","    # train_slices.append(slice_text(text, 5, 1, context_len))\n","train_dataset = build_dataset(torch.cat(train_slices, dim=0))\n","\n","test_slices = [] # list of tensors\n","for text in test_texts:\n","    test_slices.append(slice_by_line(text, context_len))\n","    # test_slices.append(slice_text(text, slice_length - 3, 1, context_len))\n","test_dataset = build_dataset(torch.cat(test_slices, dim=0))"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Features:\n","['<PAD>', '<PAD>', '<PAD>', '<PAD>', '7', '+', '1', '=']\n","Label:\n","8\n","Features:\n","['<PAD>', '<PAD>', '<PAD>', '<PAD>', '5', '+', '0', '=']\n","Label:\n","5\n"]},{"data":{"text/plain":["1001"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["def check_input_data(input):\n","    features = input[0].int().tolist()\n","    label = input[1].int().item()\n","    features_str = [reverse_vocab[f] for f in features]\n","    label_str = reverse_vocab[label]\n","    print(f\"Features:\\n{features_str}\")\n","    print(f\"Label:\\n{label_str}\")\n","\n","check_input_data(test_dataset[0])\n","check_input_data(train_dataset[0])\n","len(test_dataset)"]},{"cell_type":"markdown","metadata":{},"source":["Initialize model. Output dict size is the size of the final layer."]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["dictionary_len: 24\n"]}],"source":["dictionary_len = len(vocab)\n","model = TransformerNetwork(num_layers=4, model_dim=256, att_heads=4, ff_hidden_dim=1024, context_len=context_len, output_dict_size=dictionary_len)\n","# model.to('cuda')\n","print(f\"dictionary_len: {dictionary_len}\")\n","\n","loss_func = torch.nn.CrossEntropyLoss()\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","train_loader = DataLoader(dataset=train_dataset, batch_size=128, shuffle=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=128, shuffle=False)"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["def train_one_epoch(do_validation: bool):\n","    model.train(True)\n","    torch.set_printoptions(profile=\"short\")\n","    batches = 0\n","    avg_loss = 0\n","    for step, (features, labels) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","        preds = model(features)\n","        # print(f\"preds:{preds}\\nlabels:{labels}\")\n","        loss = loss_func(preds, labels)\n","        loss.backward()\n","\n","        # if step % 10 == 0:  # Print every 10 batches\n","        #     for name, param in model.named_parameters():\n","        #         if param.requires_grad:\n","        #             print(f\"Gradient data for {name}:\", param.grad)\n","        #             print(f\"Checking if gradients are fully zeroed: {torch.all(param.grad == 0.0).item()}\")\n","        #             print(f\"Shape: {param.grad.shape}\")\n","        #             print(f\"Mean: {param.grad.mean()}\")\n","        #             print(f\"Std: {param.grad.std()}\")\n","        #             print(f\"Min: {param.grad.min()}\")\n","        #             print(f\"Max: {param.grad.max()}\")\n","\n","        optimizer.step()\n","\n","        # if step % 20 == 0:\n","        #     print(f\"Loss on batch {step}: {loss}\")\n","\n","        avg_loss += loss\n","        batches = step + 1\n","    \n","    avg_loss = avg_loss / batches\n","    print(f\"Average loss for training batches in this epoch: {avg_loss}\")\n","\n","    if do_validation:\n","        model.train(False)\n","        batches = 0\n","        avg_loss = 0\n","        for step, (features, labels) in enumerate(test_loader):\n","            preds = model(features)\n","            # print(f\"preds:{preds}\\nlabels:{labels}\")\n","            loss = loss_func(preds, labels)\n","            \n","            # if step % 20 == 0:\n","            #     print(f\"Loss on batch {step}: {loss}\")\n","\n","            avg_loss += loss\n","            batches = step + 1\n","\n","        avg_loss = avg_loss / batches\n","        print(f\"Average loss for validation batches in this epoch: {avg_loss}\")\n"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Average loss for training batches in this epoch: 3.3309237957000732\n","Average loss for validation batches in this epoch: 2.991720199584961\n","Average loss for training batches in this epoch: 2.9167795181274414\n","Average loss for validation batches in this epoch: 2.877723455429077\n","Average loss for training batches in this epoch: 2.8401975631713867\n","Average loss for validation batches in this epoch: 2.849173069000244\n","Average loss for training batches in this epoch: 2.8108601570129395\n","Average loss for validation batches in this epoch: 2.818108081817627\n","Average loss for training batches in this epoch: 2.8080899715423584\n","Average loss for validation batches in this epoch: 2.8182177543640137\n","Average loss for training batches in this epoch: 2.7802610397338867\n","Average loss for validation batches in this epoch: 2.7715864181518555\n","Average loss for training batches in this epoch: 2.739912271499634\n","Average loss for validation batches in this epoch: 2.759986162185669\n","Average loss for training batches in this epoch: 2.722104072570801\n","Average loss for validation batches in this epoch: 2.711190938949585\n","Average loss for training batches in this epoch: 2.700200080871582\n","Average loss for validation batches in this epoch: 2.732924461364746\n","Average loss for training batches in this epoch: 2.660954713821411\n","Average loss for validation batches in this epoch: 2.6930336952209473\n","Average loss for training batches in this epoch: 2.66326904296875\n","Average loss for validation batches in this epoch: 2.676117420196533\n","Average loss for training batches in this epoch: 2.6449875831604004\n","Average loss for validation batches in this epoch: 2.685542106628418\n","Average loss for training batches in this epoch: 2.633155345916748\n","Average loss for validation batches in this epoch: 2.6518585681915283\n","Average loss for training batches in this epoch: 2.614448070526123\n","Average loss for validation batches in this epoch: 2.654552459716797\n","Average loss for training batches in this epoch: 2.600074291229248\n","Average loss for validation batches in this epoch: 2.6622390747070312\n","Average loss for training batches in this epoch: 2.568725109100342\n","Average loss for validation batches in this epoch: 2.591156005859375\n","Average loss for training batches in this epoch: 2.5807814598083496\n","Average loss for validation batches in this epoch: 2.605297088623047\n","Average loss for training batches in this epoch: 2.5586047172546387\n","Average loss for validation batches in this epoch: 2.5540013313293457\n","Average loss for training batches in this epoch: 2.5075759887695312\n","Average loss for validation batches in this epoch: 2.4510438442230225\n","Average loss for training batches in this epoch: 2.360358476638794\n","Average loss for validation batches in this epoch: 2.288698673248291\n","Average loss for training batches in this epoch: 2.1646649837493896\n","Average loss for validation batches in this epoch: 2.2594547271728516\n","Average loss for training batches in this epoch: 2.2109997272491455\n","Average loss for validation batches in this epoch: 2.0487921237945557\n","Average loss for training batches in this epoch: 2.124727487564087\n","Average loss for validation batches in this epoch: 2.0684854984283447\n","Average loss for training batches in this epoch: 1.9468666315078735\n","Average loss for validation batches in this epoch: 1.8941128253936768\n","Average loss for training batches in this epoch: 1.8695905208587646\n","Average loss for validation batches in this epoch: 1.9309022426605225\n","Average loss for training batches in this epoch: 1.8408182859420776\n","Average loss for validation batches in this epoch: 1.648738145828247\n","Average loss for training batches in this epoch: 1.7403984069824219\n","Average loss for validation batches in this epoch: 1.7049401998519897\n","Average loss for training batches in this epoch: 1.5780342817306519\n","Average loss for validation batches in this epoch: 1.4875390529632568\n","Average loss for training batches in this epoch: 1.4990652799606323\n","Average loss for validation batches in this epoch: 1.497775673866272\n","Average loss for training batches in this epoch: 1.4647444486618042\n","Average loss for validation batches in this epoch: 1.3179773092269897\n","Average loss for training batches in this epoch: 1.3125466108322144\n","Average loss for validation batches in this epoch: 1.371509313583374\n","Average loss for training batches in this epoch: 1.3817651271820068\n","Average loss for validation batches in this epoch: 1.3919488191604614\n","Average loss for training batches in this epoch: 1.2737714052200317\n","Average loss for validation batches in this epoch: 1.1973111629486084\n","Average loss for training batches in this epoch: 1.029445767402649\n","Average loss for validation batches in this epoch: 0.9957870244979858\n","Average loss for training batches in this epoch: 0.9950558543205261\n","Average loss for validation batches in this epoch: 0.9260093569755554\n","Average loss for training batches in this epoch: 1.1584970951080322\n","Average loss for validation batches in this epoch: 1.3390493392944336\n","Average loss for training batches in this epoch: 1.1460657119750977\n","Average loss for validation batches in this epoch: 1.1106867790222168\n","Average loss for training batches in this epoch: 0.9885808229446411\n","Average loss for validation batches in this epoch: 0.7362015247344971\n","Average loss for training batches in this epoch: 0.8413906097412109\n","Average loss for validation batches in this epoch: 0.8800205588340759\n","Average loss for training batches in this epoch: 0.688897430896759\n","Average loss for validation batches in this epoch: 0.6269879341125488\n","Average loss for training batches in this epoch: 0.5471987128257751\n","Average loss for validation batches in this epoch: 0.5656545758247375\n","Average loss for training batches in this epoch: 0.4910528361797333\n","Average loss for validation batches in this epoch: 0.48597630858421326\n","Average loss for training batches in this epoch: 0.44466283917427063\n","Average loss for validation batches in this epoch: 0.4038248658180237\n","Average loss for training batches in this epoch: 0.3366754651069641\n","Average loss for validation batches in this epoch: 0.3264649510383606\n","Average loss for training batches in this epoch: 0.27208277583122253\n","Average loss for validation batches in this epoch: 0.2605929672718048\n","Average loss for training batches in this epoch: 0.31598785519599915\n","Average loss for validation batches in this epoch: 0.32534173130989075\n","Average loss for training batches in this epoch: 0.32369253039360046\n","Average loss for validation batches in this epoch: 0.33067014813423157\n","Average loss for training batches in this epoch: 0.4487135708332062\n","Average loss for validation batches in this epoch: 0.5894654989242554\n","Average loss for training batches in this epoch: 0.5531533360481262\n","Average loss for validation batches in this epoch: 0.5236565470695496\n","Average loss for training batches in this epoch: 0.5068327188491821\n","Average loss for validation batches in this epoch: 0.5200605392456055\n","Average loss for training batches in this epoch: 0.6912097930908203\n","Average loss for validation batches in this epoch: 0.559427797794342\n","Average loss for training batches in this epoch: 0.6805299520492554\n","Average loss for validation batches in this epoch: 0.7673481106758118\n","Average loss for training batches in this epoch: 0.4878234267234802\n","Average loss for validation batches in this epoch: 0.44169312715530396\n","Average loss for training batches in this epoch: 0.34216317534446716\n","Average loss for validation batches in this epoch: 0.2846912741661072\n","Average loss for training batches in this epoch: 0.2540144920349121\n","Average loss for validation batches in this epoch: 0.2194199562072754\n","Average loss for training batches in this epoch: 0.20223695039749146\n","Average loss for validation batches in this epoch: 0.17836613953113556\n","Average loss for training batches in this epoch: 0.1870928257703781\n","Average loss for validation batches in this epoch: 0.17253774404525757\n","Average loss for training batches in this epoch: 0.15908952057361603\n","Average loss for validation batches in this epoch: 0.16699905693531036\n","Average loss for training batches in this epoch: 0.13117851316928864\n","Average loss for validation batches in this epoch: 0.1700458526611328\n","Average loss for training batches in this epoch: 0.1383594423532486\n","Average loss for validation batches in this epoch: 0.1855088770389557\n","Average loss for training batches in this epoch: 0.15881061553955078\n","Average loss for validation batches in this epoch: 0.1298333704471588\n","Average loss for training batches in this epoch: 0.11442551761865616\n","Average loss for validation batches in this epoch: 0.11267682909965515\n","Average loss for training batches in this epoch: 0.06993219256401062\n","Average loss for validation batches in this epoch: 0.08699002116918564\n","Average loss for training batches in this epoch: 0.06303466111421585\n","Average loss for validation batches in this epoch: 0.06605593115091324\n","Average loss for training batches in this epoch: 0.051755376160144806\n","Average loss for validation batches in this epoch: 0.05567099526524544\n","Average loss for training batches in this epoch: 0.04029947519302368\n","Average loss for validation batches in this epoch: 0.053646769374608994\n","Average loss for training batches in this epoch: 0.0537349097430706\n","Average loss for validation batches in this epoch: 0.10510116070508957\n","Average loss for training batches in this epoch: 0.051295727491378784\n","Average loss for validation batches in this epoch: 0.050978608429431915\n","Average loss for training batches in this epoch: 0.07000774145126343\n","Average loss for validation batches in this epoch: 0.14402756094932556\n","Average loss for training batches in this epoch: 0.10711812973022461\n","Average loss for validation batches in this epoch: 0.1981852948665619\n","Average loss for training batches in this epoch: 0.11119041591882706\n","Average loss for validation batches in this epoch: 0.24160003662109375\n","Average loss for training batches in this epoch: 0.09088815748691559\n","Average loss for validation batches in this epoch: 0.07868263125419617\n","Average loss for training batches in this epoch: 0.05091989040374756\n","Average loss for validation batches in this epoch: 0.0471658855676651\n","Average loss for training batches in this epoch: 0.051809534430503845\n","Average loss for validation batches in this epoch: 0.09213375300168991\n","Average loss for training batches in this epoch: 0.05512705817818642\n","Average loss for validation batches in this epoch: 0.05047251284122467\n","Average loss for training batches in this epoch: 0.027459952980279922\n","Average loss for validation batches in this epoch: 0.031968820840120316\n","Average loss for training batches in this epoch: 0.02279549650847912\n","Average loss for validation batches in this epoch: 0.062256451696157455\n","Average loss for training batches in this epoch: 0.02982901968061924\n","Average loss for validation batches in this epoch: 0.03722236305475235\n","Average loss for training batches in this epoch: 0.07601219415664673\n","Average loss for validation batches in this epoch: 0.07146018743515015\n","Average loss for training batches in this epoch: 0.17383234202861786\n","Average loss for validation batches in this epoch: 0.21697303652763367\n","Average loss for training batches in this epoch: 0.5572576522827148\n","Average loss for validation batches in this epoch: 1.1448497772216797\n","Average loss for training batches in this epoch: 0.5365968942642212\n","Average loss for validation batches in this epoch: 0.26488369703292847\n","Average loss for training batches in this epoch: 0.2565852999687195\n","Average loss for validation batches in this epoch: 0.8114851117134094\n","Average loss for training batches in this epoch: 0.39669832587242126\n","Average loss for validation batches in this epoch: 0.2288980931043625\n","Average loss for training batches in this epoch: 0.17400705814361572\n","Average loss for validation batches in this epoch: 0.20493167638778687\n","Average loss for training batches in this epoch: 0.15459653735160828\n","Average loss for validation batches in this epoch: 0.16281533241271973\n","Average loss for training batches in this epoch: 0.11017266660928726\n","Average loss for validation batches in this epoch: 0.12237820029258728\n","Average loss for training batches in this epoch: 0.06822481006383896\n","Average loss for validation batches in this epoch: 0.06301753968000412\n","Average loss for training batches in this epoch: 0.04657507315278053\n","Average loss for validation batches in this epoch: 0.04605809599161148\n","Average loss for training batches in this epoch: 0.037201229482889175\n","Average loss for validation batches in this epoch: 0.04406760632991791\n","Average loss for training batches in this epoch: 0.027468502521514893\n","Average loss for validation batches in this epoch: 0.035300806164741516\n","Average loss for training batches in this epoch: 0.023093875497579575\n","Average loss for validation batches in this epoch: 0.03353030979633331\n","Average loss for training batches in this epoch: 0.01880442164838314\n","Average loss for validation batches in this epoch: 0.024671584367752075\n","Average loss for training batches in this epoch: 0.02046714350581169\n","Average loss for validation batches in this epoch: 0.020044498145580292\n","Average loss for training batches in this epoch: 0.018259476870298386\n","Average loss for validation batches in this epoch: 0.02300761081278324\n","Average loss for training batches in this epoch: 0.01386140938848257\n","Average loss for validation batches in this epoch: 0.01910042203962803\n","Average loss for training batches in this epoch: 0.0191123578697443\n","Average loss for validation batches in this epoch: 0.051162850111722946\n","Average loss for training batches in this epoch: 0.015496289357542992\n","Average loss for validation batches in this epoch: 0.014843016862869263\n","Average loss for training batches in this epoch: 0.009588293731212616\n","Average loss for validation batches in this epoch: 0.011676965281367302\n","Average loss for training batches in this epoch: 0.008918537758290768\n","Average loss for validation batches in this epoch: 0.011592288501560688\n","Average loss for training batches in this epoch: 0.009280950762331486\n","Average loss for validation batches in this epoch: 0.01090764906257391\n","Average loss for training batches in this epoch: 0.007597568444907665\n","Average loss for validation batches in this epoch: 0.014055358245968819\n","Average loss for training batches in this epoch: 0.007480298634618521\n","Average loss for validation batches in this epoch: 0.01086563989520073\n","Average loss for training batches in this epoch: 0.008005915209650993\n","Average loss for validation batches in this epoch: 0.013616936281323433\n","Average loss for training batches in this epoch: 0.008209345862269402\n","Average loss for validation batches in this epoch: 0.014919006265699863\n","Average loss for training batches in this epoch: 0.009848719462752342\n","Average loss for validation batches in this epoch: 0.00910586304962635\n","Average loss for training batches in this epoch: 0.007274836301803589\n","Average loss for validation batches in this epoch: 0.012831004336476326\n","Average loss for training batches in this epoch: 0.005924042314291\n","Average loss for validation batches in this epoch: 0.007001113146543503\n","Average loss for training batches in this epoch: 0.007549738511443138\n","Average loss for validation batches in this epoch: 0.007468058727681637\n","Average loss for training batches in this epoch: 0.007550736889243126\n","Average loss for validation batches in this epoch: 0.0073716528713703156\n","Average loss for training batches in this epoch: 0.007096666377037764\n","Average loss for validation batches in this epoch: 0.013236941769719124\n","Average loss for training batches in this epoch: 0.009843811392784119\n","Average loss for validation batches in this epoch: 0.011332003399729729\n","Average loss for training batches in this epoch: 0.005675099324434996\n","Average loss for validation batches in this epoch: 0.007715792395174503\n","Average loss for training batches in this epoch: 0.004516350105404854\n","Average loss for validation batches in this epoch: 0.004618858452886343\n","Average loss for training batches in this epoch: 0.003639974631369114\n","Average loss for validation batches in this epoch: 0.0043658968061208725\n","Average loss for training batches in this epoch: 0.003456955309957266\n","Average loss for validation batches in this epoch: 0.0042082443833351135\n","Average loss for training batches in this epoch: 0.003308032639324665\n","Average loss for validation batches in this epoch: 0.003943585325032473\n","Average loss for training batches in this epoch: 0.0031445410568267107\n","Average loss for validation batches in this epoch: 0.003812752664089203\n","Average loss for training batches in this epoch: 0.003032565815374255\n","Average loss for validation batches in this epoch: 0.0037170040886849165\n","Average loss for training batches in this epoch: 0.003049012739211321\n","Average loss for validation batches in this epoch: 0.003956084605306387\n","Average loss for training batches in this epoch: 0.0029666926711797714\n","Average loss for validation batches in this epoch: 0.0036295487079769373\n","Average loss for training batches in this epoch: 0.002812630031257868\n","Average loss for validation batches in this epoch: 0.0033438699319958687\n","Average loss for training batches in this epoch: 0.0026759132742881775\n","Average loss for validation batches in this epoch: 0.003299578558653593\n","Average loss for training batches in this epoch: 0.002615830162540078\n","Average loss for validation batches in this epoch: 0.003259957768023014\n","Average loss for training batches in this epoch: 0.0025258129462599754\n","Average loss for validation batches in this epoch: 0.0030635292641818523\n","Average loss for training batches in this epoch: 0.002436230890452862\n","Average loss for validation batches in this epoch: 0.0029953725170344114\n","Average loss for training batches in this epoch: 0.0023985151201486588\n","Average loss for validation batches in this epoch: 0.0029126680456101894\n","Average loss for training batches in this epoch: 0.0023362578358501196\n","Average loss for validation batches in this epoch: 0.002783520147204399\n","Average loss for training batches in this epoch: 0.0022616651840507984\n","Average loss for validation batches in this epoch: 0.002720676129683852\n","Average loss for training batches in this epoch: 0.0021904893219470978\n","Average loss for validation batches in this epoch: 0.0027149177622050047\n","Average loss for training batches in this epoch: 0.002196389716118574\n","Average loss for validation batches in this epoch: 0.0025579894427210093\n","Average loss for training batches in this epoch: 0.0020838864147663116\n","Average loss for validation batches in this epoch: 0.002507932484149933\n","Average loss for training batches in this epoch: 0.002052230993285775\n","Average loss for validation batches in this epoch: 0.002623406471684575\n","Average loss for training batches in this epoch: 0.002057091798633337\n","Average loss for validation batches in this epoch: 0.0024278259370476007\n","Average loss for training batches in this epoch: 0.0019927220419049263\n","Average loss for validation batches in this epoch: 0.0024264391977339983\n","Average loss for training batches in this epoch: 0.0019292111974209547\n","Average loss for validation batches in this epoch: 0.002293328521773219\n","Average loss for training batches in this epoch: 0.00185864488594234\n","Average loss for validation batches in this epoch: 0.0022074133157730103\n","Average loss for training batches in this epoch: 0.0017968951724469662\n","Average loss for validation batches in this epoch: 0.0021725171245634556\n","Average loss for training batches in this epoch: 0.0017624680185690522\n","Average loss for validation batches in this epoch: 0.002154846442863345\n","Average loss for training batches in this epoch: 0.0017305895453318954\n","Average loss for validation batches in this epoch: 0.002166117774322629\n","Average loss for training batches in this epoch: 0.0017262010369449854\n","Average loss for validation batches in this epoch: 0.0020358129404485226\n","Average loss for training batches in this epoch: 0.0016616795910522342\n","Average loss for validation batches in this epoch: 0.00197295262478292\n","Average loss for training batches in this epoch: 0.0016128858551383018\n","Average loss for validation batches in this epoch: 0.001955693121999502\n","Average loss for training batches in this epoch: 0.0015752606559544802\n","Average loss for validation batches in this epoch: 0.00187404896132648\n","Average loss for training batches in this epoch: 0.0015504647744819522\n","Average loss for validation batches in this epoch: 0.001861568889580667\n","Average loss for training batches in this epoch: 0.0015147760277613997\n","Average loss for validation batches in this epoch: 0.0018224370433017612\n","Average loss for training batches in this epoch: 0.0014890386955812573\n","Average loss for validation batches in this epoch: 0.0017859962536022067\n","Average loss for training batches in this epoch: 0.0014467638684436679\n","Average loss for validation batches in this epoch: 0.0017441397067159414\n","Average loss for training batches in this epoch: 0.0014441395178437233\n","Average loss for validation batches in this epoch: 0.0017055823700502515\n","Average loss for training batches in this epoch: 0.0013988374266773462\n","Average loss for validation batches in this epoch: 0.0016516579780727625\n","Average loss for training batches in this epoch: 0.0013641545083373785\n","Average loss for validation batches in this epoch: 0.0016280736308544874\n","Average loss for training batches in this epoch: 0.001331282197497785\n","Average loss for validation batches in this epoch: 0.0015931122470647097\n","Average loss for training batches in this epoch: 0.0013118610950186849\n","Average loss for validation batches in this epoch: 0.0015619269106537104\n","Average loss for training batches in this epoch: 0.0012983364285901189\n","Average loss for validation batches in this epoch: 0.0015327762812376022\n","Average loss for training batches in this epoch: 0.001259471639059484\n","Average loss for validation batches in this epoch: 0.001496193348430097\n","Average loss for training batches in this epoch: 0.0012429803609848022\n","Average loss for validation batches in this epoch: 0.0014801291981711984\n","Average loss for training batches in this epoch: 0.0012189855333417654\n","Average loss for validation batches in this epoch: 0.00144202692899853\n","Average loss for training batches in this epoch: 0.0011890692403540015\n","Average loss for validation batches in this epoch: 0.0014249965315684676\n","Average loss for training batches in this epoch: 0.001178462989628315\n","Average loss for validation batches in this epoch: 0.0014081464614719152\n","Average loss for training batches in this epoch: 0.0011542680440470576\n","Average loss for validation batches in this epoch: 0.001368946279399097\n","Average loss for training batches in this epoch: 0.0011402886593714356\n","Average loss for validation batches in this epoch: 0.0013442446943372488\n","Average loss for training batches in this epoch: 0.0011064470745623112\n","Average loss for validation batches in this epoch: 0.0013209236785769463\n","Average loss for training batches in this epoch: 0.00110413390211761\n","Average loss for validation batches in this epoch: 0.0013015407603234053\n","Average loss for training batches in this epoch: 0.0010797148570418358\n","Average loss for validation batches in this epoch: 0.0012690238654613495\n","Average loss for training batches in this epoch: 0.001055954722687602\n","Average loss for validation batches in this epoch: 0.0012509244261309505\n","Average loss for training batches in this epoch: 0.0010394639102742076\n","Average loss for validation batches in this epoch: 0.0012329785386100411\n","Average loss for training batches in this epoch: 0.001014687237329781\n","Average loss for validation batches in this epoch: 0.0012103862827643752\n","Average loss for training batches in this epoch: 0.0010115578770637512\n","Average loss for validation batches in this epoch: 0.0011893311748281121\n","Average loss for training batches in this epoch: 0.0009832822252064943\n","Average loss for validation batches in this epoch: 0.0011724689975380898\n","Average loss for training batches in this epoch: 0.0009780703112483025\n","Average loss for validation batches in this epoch: 0.0011438108049333096\n","Average loss for training batches in this epoch: 0.0009608004475012422\n","Average loss for validation batches in this epoch: 0.001128866570070386\n","Average loss for training batches in this epoch: 0.0009380055125802755\n","Average loss for validation batches in this epoch: 0.0011106055462732911\n","Average loss for training batches in this epoch: 0.000922702020034194\n","Average loss for validation batches in this epoch: 0.001084642019122839\n","Average loss for training batches in this epoch: 0.0009077813010662794\n","Average loss for validation batches in this epoch: 0.0010744398459792137\n","Average loss for training batches in this epoch: 0.0008974477532319725\n","Average loss for validation batches in this epoch: 0.0010562277166172862\n","Average loss for training batches in this epoch: 0.0008758545154705644\n","Average loss for validation batches in this epoch: 0.0010397477308288217\n","Average loss for training batches in this epoch: 0.0008651975076645613\n","Average loss for validation batches in this epoch: 0.0010202303528785706\n","Average loss for training batches in this epoch: 0.0008544382289983332\n","Average loss for validation batches in this epoch: 0.0010026853997260332\n","Average loss for training batches in this epoch: 0.0008370520081371069\n","Average loss for validation batches in this epoch: 0.0009865870233625174\n","Average loss for training batches in this epoch: 0.0008251402759924531\n","Average loss for validation batches in this epoch: 0.0009730930323712528\n","Average loss for training batches in this epoch: 0.0008091672789305449\n","Average loss for validation batches in this epoch: 0.0009563636849634349\n","Average loss for training batches in this epoch: 0.0008039666572585702\n","Average loss for validation batches in this epoch: 0.0009437226108275354\n","Average loss for training batches in this epoch: 0.0007888154359534383\n","Average loss for validation batches in this epoch: 0.0009345869184471667\n","Average loss for training batches in this epoch: 0.0007787755457684398\n","Average loss for validation batches in this epoch: 0.0009156967862509191\n","Average loss for training batches in this epoch: 0.0007691337377764285\n","Average loss for validation batches in this epoch: 0.0008971762144938111\n","Average loss for training batches in this epoch: 0.0007510060095228255\n","Average loss for validation batches in this epoch: 0.0008830346632748842\n","Average loss for training batches in this epoch: 0.0007383964257314801\n","Average loss for validation batches in this epoch: 0.0008683809428475797\n","Average loss for training batches in this epoch: 0.0007293135859072208\n","Average loss for validation batches in this epoch: 0.0008554860251024365\n","Average loss for training batches in this epoch: 0.000717233691830188\n","Average loss for validation batches in this epoch: 0.0008437834912911057\n","Average loss for training batches in this epoch: 0.0007088983547873795\n","Average loss for validation batches in this epoch: 0.0008327605901286006\n","Average loss for training batches in this epoch: 0.0006995234289206564\n","Average loss for validation batches in this epoch: 0.0008238099981099367\n","Average loss for training batches in this epoch: 0.000694408779963851\n","Average loss for validation batches in this epoch: 0.0008069772738963366\n","Average loss for training batches in this epoch: 0.0006862745503894985\n","Average loss for validation batches in this epoch: 0.0007959597860462964\n","Average loss for training batches in this epoch: 0.0006699262303300202\n","Average loss for validation batches in this epoch: 0.0007876168237999082\n","Average loss for training batches in this epoch: 0.0006593350553885102\n","Average loss for validation batches in this epoch: 0.0007758469437249005\n","Average loss for training batches in this epoch: 0.0006552985869348049\n","Average loss for validation batches in this epoch: 0.000769415928516537\n","Average loss for training batches in this epoch: 0.0006494396948255599\n","Average loss for validation batches in this epoch: 0.0007569896406494081\n","Average loss for training batches in this epoch: 0.0006349108880385756\n","Average loss for validation batches in this epoch: 0.0007411960395984352\n","Average loss for training batches in this epoch: 0.0006261100061237812\n","Average loss for validation batches in this epoch: 0.0007328367209993303\n","Average loss for training batches in this epoch: 0.000615288270637393\n","Average loss for validation batches in this epoch: 0.0007199550163932145\n","Average loss for training batches in this epoch: 0.0006064235931262374\n","Average loss for validation batches in this epoch: 0.0007130209123715758\n","Average loss for training batches in this epoch: 0.0006013487000018358\n","Average loss for validation batches in this epoch: 0.0007021207711659372\n","Average loss for training batches in this epoch: 0.0005969218909740448\n","Average loss for validation batches in this epoch: 0.0006967020453885198\n","Average loss for training batches in this epoch: 0.0005908738821744919\n","Average loss for validation batches in this epoch: 0.0006884862086735666\n","Average loss for training batches in this epoch: 0.0005816930788569152\n","Average loss for validation batches in this epoch: 0.0006755064823664725\n","Average loss for training batches in this epoch: 0.0005765463574789464\n","Average loss for validation batches in this epoch: 0.000677217380143702\n","Average loss for training batches in this epoch: 0.0005715199513360858\n","Average loss for validation batches in this epoch: 0.0006642548250965774\n","Average loss for training batches in this epoch: 0.0005562477745115757\n","Average loss for validation batches in this epoch: 0.0006517635774798691\n","Average loss for training batches in this epoch: 0.0005501105333678424\n","Average loss for validation batches in this epoch: 0.0006397931138053536\n","Average loss for training batches in this epoch: 0.0005437879590317607\n","Average loss for validation batches in this epoch: 0.0006330286269076169\n","Average loss for training batches in this epoch: 0.0005344527307897806\n","Average loss for validation batches in this epoch: 0.0006231354200281203\n","Average loss for training batches in this epoch: 0.0005315948510542512\n","Average loss for validation batches in this epoch: 0.0006185743259266019\n","Average loss for training batches in this epoch: 0.000522154790814966\n","Average loss for validation batches in this epoch: 0.0006052274256944656\n","Average loss for training batches in this epoch: 0.0005136611871421337\n","Average loss for validation batches in this epoch: 0.0005995193496346474\n","Average loss for training batches in this epoch: 0.0005072940839454532\n","Average loss for validation batches in this epoch: 0.00058941007591784\n","Average loss for training batches in this epoch: 0.0005002912366762757\n","Average loss for validation batches in this epoch: 0.0005826065898872912\n","Average loss for training batches in this epoch: 0.0004953971365466714\n","Average loss for validation batches in this epoch: 0.0005785874673165381\n","Average loss for training batches in this epoch: 0.0004899105988442898\n","Average loss for validation batches in this epoch: 0.0005689203971996903\n","Average loss for training batches in this epoch: 0.0004852522106375545\n","Average loss for validation batches in this epoch: 0.0005616956041194499\n","Average loss for training batches in this epoch: 0.00047958389041014016\n","Average loss for validation batches in this epoch: 0.0005555702955462039\n","Average loss for training batches in this epoch: 0.00047314638504758477\n","Average loss for validation batches in this epoch: 0.0005510752089321613\n","Average loss for training batches in this epoch: 0.00047120609087869525\n","Average loss for validation batches in this epoch: 0.0005418297369033098\n","Average loss for training batches in this epoch: 0.00046708318404853344\n","Average loss for validation batches in this epoch: 0.0005371523438952863\n","Average loss for training batches in this epoch: 0.00045513411168940365\n","Average loss for validation batches in this epoch: 0.0005282346974126995\n","Average loss for training batches in this epoch: 0.000451426487416029\n","Average loss for validation batches in this epoch: 0.000523701251950115\n","Average loss for training batches in this epoch: 0.00044456718023866415\n","Average loss for validation batches in this epoch: 0.0005155691178515553\n","Average loss for training batches in this epoch: 0.00044212533975951374\n","Average loss for validation batches in this epoch: 0.0005118119297549129\n","Average loss for training batches in this epoch: 0.00043563416693359613\n","Average loss for validation batches in this epoch: 0.0005026764702051878\n","Average loss for training batches in this epoch: 0.0004293781239539385\n","Average loss for validation batches in this epoch: 0.0004983720718882978\n","Average loss for training batches in this epoch: 0.0004316608246881515\n","Average loss for validation batches in this epoch: 0.0004948959103785455\n","Average loss for training batches in this epoch: 0.0004252572834957391\n","Average loss for validation batches in this epoch: 0.0004866827221121639\n","Average loss for training batches in this epoch: 0.0004163089906796813\n","Average loss for validation batches in this epoch: 0.000486198317958042\n","Average loss for training batches in this epoch: 0.0004126333515159786\n","Average loss for validation batches in this epoch: 0.0004745299811474979\n","Average loss for training batches in this epoch: 0.00040581292705610394\n","Average loss for validation batches in this epoch: 0.00047269134665839374\n","Average loss for training batches in this epoch: 0.0004038374172523618\n","Average loss for validation batches in this epoch: 0.0004655671655200422\n","Average loss for training batches in this epoch: 0.0004011898417957127\n","Average loss for validation batches in this epoch: 0.00046192790614441037\n","Average loss for training batches in this epoch: 0.0003929960075765848\n","Average loss for validation batches in this epoch: 0.0004544845432974398\n","Average loss for training batches in this epoch: 0.0003886932972818613\n","Average loss for validation batches in this epoch: 0.0004494014719966799\n","Average loss for training batches in this epoch: 0.00038662966107949615\n","Average loss for validation batches in this epoch: 0.0004464921948965639\n","Average loss for training batches in this epoch: 0.000380965240765363\n","Average loss for validation batches in this epoch: 0.00043914307025261223\n","Average loss for training batches in this epoch: 0.0003776650410145521\n","Average loss for validation batches in this epoch: 0.0004352072428446263\n","Average loss for training batches in this epoch: 0.00037241229438222945\n","Average loss for validation batches in this epoch: 0.0004305621550884098\n","Average loss for training batches in this epoch: 0.00036780742811970413\n","Average loss for validation batches in this epoch: 0.0004256149986758828\n","Average loss for training batches in this epoch: 0.00036347625427879393\n","Average loss for validation batches in this epoch: 0.00042130189831368625\n","Average loss for training batches in this epoch: 0.00036052471841685474\n","Average loss for validation batches in this epoch: 0.0004165506979916245\n","Average loss for training batches in this epoch: 0.0003571664565242827\n","Average loss for validation batches in this epoch: 0.0004147167783230543\n","Average loss for training batches in this epoch: 0.0003558423195499927\n","Average loss for validation batches in this epoch: 0.0004122382670175284\n","Average loss for training batches in this epoch: 0.00035065750125795603\n","Average loss for validation batches in this epoch: 0.00040774690569378436\n","Average loss for training batches in this epoch: 0.0003483725304249674\n","Average loss for validation batches in this epoch: 0.000401380006223917\n","Average loss for training batches in this epoch: 0.00034212032915093005\n","Average loss for validation batches in this epoch: 0.0003940780879929662\n","Average loss for training batches in this epoch: 0.0003405519819352776\n","Average loss for validation batches in this epoch: 0.0003920791205018759\n","Average loss for training batches in this epoch: 0.0003372241626493633\n","Average loss for validation batches in this epoch: 0.0003873849636875093\n","Average loss for training batches in this epoch: 0.00033504742896184325\n","Average loss for validation batches in this epoch: 0.0003848127380479127\n","Average loss for training batches in this epoch: 0.0003293422923889011\n","Average loss for validation batches in this epoch: 0.0003791261406149715\n","Average loss for training batches in this epoch: 0.0003257092321291566\n","Average loss for validation batches in this epoch: 0.0003757585654966533\n","Average loss for training batches in this epoch: 0.0003212959272786975\n","Average loss for validation batches in this epoch: 0.0003703892871271819\n","Average loss for training batches in this epoch: 0.00031940272310748696\n","Average loss for validation batches in this epoch: 0.00036942269071005285\n","Average loss for training batches in this epoch: 0.0003169344854541123\n","Average loss for validation batches in this epoch: 0.0003641427028924227\n","Average loss for training batches in this epoch: 0.00031396711710840464\n","Average loss for validation batches in this epoch: 0.0003595839662011713\n","Average loss for training batches in this epoch: 0.00031159998616203666\n","Average loss for validation batches in this epoch: 0.00035839685006067157\n","Average loss for training batches in this epoch: 0.000307622947730124\n","Average loss for validation batches in this epoch: 0.00035318121081218123\n","Average loss for training batches in this epoch: 0.00030361878452822566\n","Average loss for validation batches in this epoch: 0.00034929614048451185\n","Average loss for training batches in this epoch: 0.00029984465800225735\n","Average loss for validation batches in this epoch: 0.0003468542417977005\n","Average loss for training batches in this epoch: 0.0002971910289488733\n","Average loss for validation batches in this epoch: 0.00034206255804747343\n","Average loss for training batches in this epoch: 0.00029542669653892517\n","Average loss for validation batches in this epoch: 0.0003393909428268671\n","Average loss for training batches in this epoch: 0.00029191086650826037\n","Average loss for validation batches in this epoch: 0.00033596594585105777\n","Average loss for training batches in this epoch: 0.00029073553741909564\n","Average loss for validation batches in this epoch: 0.00033417530357837677\n","Average loss for training batches in this epoch: 0.0002867183065973222\n","Average loss for validation batches in this epoch: 0.00033029232872650027\n","Average loss for training batches in this epoch: 0.00028513811412267387\n","Average loss for validation batches in this epoch: 0.00032649701461195946\n","Average loss for training batches in this epoch: 0.0002812737657222897\n","Average loss for validation batches in this epoch: 0.00032310106325894594\n","Average loss for training batches in this epoch: 0.0002812056627590209\n","Average loss for validation batches in this epoch: 0.00031966884853318334\n","Average loss for training batches in this epoch: 0.00027795456117019057\n","Average loss for validation batches in this epoch: 0.00031836453126743436\n","Average loss for training batches in this epoch: 0.0002750344865489751\n","Average loss for validation batches in this epoch: 0.00031413440592586994\n","Average loss for training batches in this epoch: 0.000269846263108775\n","Average loss for validation batches in this epoch: 0.00031139739439822733\n","Average loss for training batches in this epoch: 0.0002686448278836906\n","Average loss for validation batches in this epoch: 0.0003073787083849311\n","Average loss for training batches in this epoch: 0.00026635624817572534\n","Average loss for validation batches in this epoch: 0.00030618321034125984\n","Average loss for training batches in this epoch: 0.0002640134480316192\n","Average loss for validation batches in this epoch: 0.0003022982564289123\n","Average loss for training batches in this epoch: 0.0002607726491987705\n","Average loss for validation batches in this epoch: 0.0002998023119289428\n","Average loss for training batches in this epoch: 0.00026293459814041853\n","Average loss for validation batches in this epoch: 0.00029796722810715437\n","Average loss for training batches in this epoch: 0.00025922845816239715\n","Average loss for validation batches in this epoch: 0.0002944527950603515\n","Average loss for training batches in this epoch: 0.00025573637685738504\n","Average loss for validation batches in this epoch: 0.0002944793086498976\n","Average loss for training batches in this epoch: 0.0002516780514270067\n","Average loss for validation batches in this epoch: 0.0002889026654884219\n","Average loss for training batches in this epoch: 0.0002496349043212831\n","Average loss for validation batches in this epoch: 0.0002859012165572494\n","Average loss for training batches in this epoch: 0.0002483497082721442\n","Average loss for validation batches in this epoch: 0.0002838786749634892\n","Average loss for training batches in this epoch: 0.0002486512530595064\n","Average loss for validation batches in this epoch: 0.0002809881989378482\n","Average loss for training batches in this epoch: 0.0002463839191477746\n","Average loss for validation batches in this epoch: 0.00027821172261610627\n","Average loss for training batches in this epoch: 0.0002449706953484565\n","Average loss for validation batches in this epoch: 0.0002770758292172104\n","Average loss for training batches in this epoch: 0.00023979321122169495\n","Average loss for validation batches in this epoch: 0.00027389152091927826\n","Average loss for training batches in this epoch: 0.00023698712175246328\n","Average loss for validation batches in this epoch: 0.00027316896012052894\n","Average loss for training batches in this epoch: 0.0002350243303226307\n","Average loss for validation batches in this epoch: 0.0002681070764083415\n","Average loss for training batches in this epoch: 0.00023265191703103483\n","Average loss for validation batches in this epoch: 0.00026615773094817996\n","Average loss for training batches in this epoch: 0.00022929918486624956\n","Average loss for validation batches in this epoch: 0.0002634452539496124\n","Average loss for training batches in this epoch: 0.00022842925682198256\n","Average loss for validation batches in this epoch: 0.0002607954083941877\n","Average loss for training batches in this epoch: 0.00022652174811810255\n","Average loss for validation batches in this epoch: 0.00025882298359647393\n","Average loss for training batches in this epoch: 0.00022533218725584447\n","Average loss for validation batches in this epoch: 0.00025683760759420693\n","Average loss for training batches in this epoch: 0.00022341814474202693\n","Average loss for validation batches in this epoch: 0.00025446334620937705\n","Average loss for training batches in this epoch: 0.00022377270215656608\n","Average loss for validation batches in this epoch: 0.0002530794881749898\n","Average loss for training batches in this epoch: 0.00022257321688812226\n","Average loss for validation batches in this epoch: 0.0002511650091037154\n","Average loss for training batches in this epoch: 0.00021840137196704745\n","Average loss for validation batches in this epoch: 0.0002492425555828959\n","Average loss for training batches in this epoch: 0.0002156270493287593\n","Average loss for validation batches in this epoch: 0.0002463115379214287\n","Average loss for training batches in this epoch: 0.00021334625489544123\n","Average loss for validation batches in this epoch: 0.0002454147906973958\n","Average loss for training batches in this epoch: 0.000211059843422845\n","Average loss for validation batches in this epoch: 0.00024054500681813806\n","Average loss for training batches in this epoch: 0.00020983850117772818\n","Average loss for validation batches in this epoch: 0.00023908386356197298\n","Average loss for training batches in this epoch: 0.00020778669568244368\n","Average loss for validation batches in this epoch: 0.0002369120338698849\n","Average loss for training batches in this epoch: 0.00020642581512220204\n","Average loss for validation batches in this epoch: 0.00023518825764767826\n","Average loss for training batches in this epoch: 0.0002069600741378963\n","Average loss for validation batches in this epoch: 0.00023524431162513793\n","Average loss for training batches in this epoch: 0.00020394130842760205\n","Average loss for validation batches in this epoch: 0.00023379108461085707\n","Average loss for training batches in this epoch: 0.00020209155627526343\n","Average loss for validation batches in this epoch: 0.00023079747916199267\n","Average loss for training batches in this epoch: 0.00020014337496832013\n","Average loss for validation batches in this epoch: 0.00022678512323182076\n","Average loss for training batches in this epoch: 0.00019711365166585892\n","Average loss for validation batches in this epoch: 0.00022701431589666754\n","Average loss for training batches in this epoch: 0.0001950024743564427\n","Average loss for validation batches in this epoch: 0.0002231599937658757\n","Average loss for training batches in this epoch: 0.00019423152843955904\n","Average loss for validation batches in this epoch: 0.00022141508816275746\n","Average loss for training batches in this epoch: 0.0001922731607919559\n","Average loss for validation batches in this epoch: 0.00021925708279013634\n","Average loss for training batches in this epoch: 0.00019139383221045136\n","Average loss for validation batches in this epoch: 0.00021821085829287767\n","Average loss for training batches in this epoch: 0.00018990127136930823\n","Average loss for validation batches in this epoch: 0.0002158277202397585\n","Average loss for training batches in this epoch: 0.00018748540605884045\n","Average loss for validation batches in this epoch: 0.00021437543909996748\n","Average loss for training batches in this epoch: 0.00018714253383222967\n","Average loss for validation batches in this epoch: 0.00021198294416535646\n","Average loss for training batches in this epoch: 0.00018508112407289445\n","Average loss for validation batches in this epoch: 0.00021099680452607572\n","Average loss for training batches in this epoch: 0.00018293914035893977\n","Average loss for validation batches in this epoch: 0.0002089611516566947\n","Average loss for training batches in this epoch: 0.00018159672617912292\n","Average loss for validation batches in this epoch: 0.00020725557988043875\n","Average loss for training batches in this epoch: 0.00018209093832410872\n","Average loss for validation batches in this epoch: 0.00020649124053306878\n","Average loss for training batches in this epoch: 0.00017931126058101654\n","Average loss for validation batches in this epoch: 0.00020423030946403742\n","Average loss for training batches in this epoch: 0.00017894328630063683\n","Average loss for validation batches in this epoch: 0.0002029111492447555\n","Average loss for training batches in this epoch: 0.00017636609845794737\n","Average loss for validation batches in this epoch: 0.00020051935280207545\n","Average loss for training batches in this epoch: 0.0001755750272423029\n","Average loss for validation batches in this epoch: 0.00020036216301377863\n","Average loss for training batches in this epoch: 0.00017424803809262812\n","Average loss for validation batches in this epoch: 0.0001973210892174393\n","Average loss for training batches in this epoch: 0.00017210292571689934\n","Average loss for validation batches in this epoch: 0.00019645996508188546\n","Average loss for training batches in this epoch: 0.00016976651386357844\n","Average loss for validation batches in this epoch: 0.00019401423924136907\n","Average loss for training batches in this epoch: 0.00017007473798003048\n","Average loss for validation batches in this epoch: 0.00019311351934447885\n","Average loss for training batches in this epoch: 0.00016877971938811243\n","Average loss for validation batches in this epoch: 0.00019116590556222945\n","Average loss for training batches in this epoch: 0.00016741138824727386\n","Average loss for validation batches in this epoch: 0.00019042422354687005\n","Average loss for training batches in this epoch: 0.00016542759840376675\n","Average loss for validation batches in this epoch: 0.00018826940504368395\n","Average loss for training batches in this epoch: 0.00016389231313951313\n","Average loss for validation batches in this epoch: 0.0001871280837804079\n","Average loss for training batches in this epoch: 0.00016288907499983907\n","Average loss for validation batches in this epoch: 0.0001851918059401214\n","Average loss for training batches in this epoch: 0.0001613499625818804\n","Average loss for validation batches in this epoch: 0.00018346407159697264\n","Average loss for training batches in this epoch: 0.00016027173842303455\n","Average loss for validation batches in this epoch: 0.00018223578808829188\n","Average loss for training batches in this epoch: 0.00015866366447880864\n","Average loss for validation batches in this epoch: 0.00018176581943407655\n","Average loss for training batches in this epoch: 0.0001581541437190026\n","Average loss for validation batches in this epoch: 0.00017954132636077702\n","Average loss for training batches in this epoch: 0.00015661497309338301\n","Average loss for validation batches in this epoch: 0.0001783235347829759\n","Average loss for training batches in this epoch: 0.00015561707550659776\n","Average loss for validation batches in this epoch: 0.00017660725279711187\n","Average loss for training batches in this epoch: 0.00015427566540893167\n","Average loss for validation batches in this epoch: 0.00017517914238851517\n","Average loss for training batches in this epoch: 0.00015334021009039134\n","Average loss for validation batches in this epoch: 0.00017481259419582784\n","Average loss for training batches in this epoch: 0.00015156464360188693\n","Average loss for validation batches in this epoch: 0.0001731676165945828\n","Average loss for training batches in this epoch: 0.00015177881869021803\n","Average loss for validation batches in this epoch: 0.00017138372641056776\n","Average loss for training batches in this epoch: 0.0001498874044045806\n","Average loss for validation batches in this epoch: 0.00017002748791128397\n","Average loss for training batches in this epoch: 0.00014951403136365116\n","Average loss for validation batches in this epoch: 0.00016867944214027375\n","Average loss for training batches in this epoch: 0.0001478531485190615\n","Average loss for validation batches in this epoch: 0.00016756021068431437\n","Average loss for training batches in this epoch: 0.00014632687089033425\n","Average loss for validation batches in this epoch: 0.00016620122187305242\n","Average loss for training batches in this epoch: 0.0001450633571948856\n","Average loss for validation batches in this epoch: 0.00016526601393707097\n","Average loss for training batches in this epoch: 0.00014484972052741796\n","Average loss for validation batches in this epoch: 0.00016428511298727244\n","Average loss for training batches in this epoch: 0.00014325453958008438\n","Average loss for validation batches in this epoch: 0.00016355681873392314\n","Average loss for training batches in this epoch: 0.00014248969091568142\n","Average loss for validation batches in this epoch: 0.00016100922948680818\n","Average loss for training batches in this epoch: 0.00014082709094509482\n","Average loss for validation batches in this epoch: 0.00016014373977668583\n","Average loss for training batches in this epoch: 0.00014010148879606277\n","Average loss for validation batches in this epoch: 0.00015875075769145042\n","Average loss for training batches in this epoch: 0.00013855045835953206\n","Average loss for validation batches in this epoch: 0.0001577348739374429\n","Average loss for training batches in this epoch: 0.00013802765170112252\n","Average loss for validation batches in this epoch: 0.00015660568897146732\n","Average loss for training batches in this epoch: 0.00013714459782931954\n","Average loss for validation batches in this epoch: 0.00015541852917522192\n","Average loss for training batches in this epoch: 0.00013565062545239925\n","Average loss for validation batches in this epoch: 0.00015410137712024152\n","Average loss for training batches in this epoch: 0.00013577916251961142\n","Average loss for validation batches in this epoch: 0.00015320030797738582\n","Average loss for training batches in this epoch: 0.0001347942161373794\n","Average loss for validation batches in this epoch: 0.00015198935579974204\n","Average loss for training batches in this epoch: 0.00013336453412193805\n","Average loss for validation batches in this epoch: 0.00015104992780834436\n","Average loss for training batches in this epoch: 0.0001330624072579667\n","Average loss for validation batches in this epoch: 0.00014968843606766313\n","Average loss for training batches in this epoch: 0.00013120414223521948\n","Average loss for validation batches in this epoch: 0.00014845568512100726\n","Average loss for training batches in this epoch: 0.00013014160504098982\n","Average loss for validation batches in this epoch: 0.00014763258513994515\n","Average loss for training batches in this epoch: 0.00012964379857294261\n","Average loss for validation batches in this epoch: 0.00014645287592429668\n","Average loss for training batches in this epoch: 0.0001289351494051516\n","Average loss for validation batches in this epoch: 0.00014622944581788033\n","Average loss for training batches in this epoch: 0.00012742343824356794\n","Average loss for validation batches in this epoch: 0.00014441678649745882\n","Average loss for training batches in this epoch: 0.00012648142001125962\n","Average loss for validation batches in this epoch: 0.00014342293434310704\n","Average loss for training batches in this epoch: 0.00012546175275929272\n","Average loss for validation batches in this epoch: 0.00014222596655599773\n","Average loss for training batches in this epoch: 0.00012467267515603453\n","Average loss for validation batches in this epoch: 0.00014095685037318617\n","Average loss for training batches in this epoch: 0.00012358211097307503\n","Average loss for validation batches in this epoch: 0.00014039322559256107\n","Average loss for training batches in this epoch: 0.00012309457815717906\n","Average loss for validation batches in this epoch: 0.000139496274641715\n","Average loss for training batches in this epoch: 0.0001226085441885516\n","Average loss for validation batches in this epoch: 0.00013830704847350717\n","Average loss for training batches in this epoch: 0.00012171548587502912\n","Average loss for validation batches in this epoch: 0.00013720992137677968\n","Average loss for training batches in this epoch: 0.00012068530486430973\n","Average loss for validation batches in this epoch: 0.0001361034665023908\n","Average loss for training batches in this epoch: 0.00011961640848312527\n","Average loss for validation batches in this epoch: 0.00013494498853106052\n","Average loss for training batches in this epoch: 0.00011835804616566747\n","Average loss for validation batches in this epoch: 0.00013405394565779716\n","Average loss for training batches in this epoch: 0.00011919462122023106\n","Average loss for validation batches in this epoch: 0.00013339110591914505\n","Average loss for training batches in this epoch: 0.00011759990593418479\n","Average loss for validation batches in this epoch: 0.00013237431994639337\n","Average loss for training batches in this epoch: 0.00011597282718867064\n","Average loss for validation batches in this epoch: 0.0001325166958849877\n","Average loss for training batches in this epoch: 0.00011569444177439436\n","Average loss for validation batches in this epoch: 0.00013033692084718496\n","Average loss for training batches in this epoch: 0.00011438447108957916\n","Average loss for validation batches in this epoch: 0.0001298253337154165\n","Average loss for training batches in this epoch: 0.00011379468924133107\n","Average loss for validation batches in this epoch: 0.00012844151933677495\n","Average loss for training batches in this epoch: 0.00011274218559265137\n","Average loss for validation batches in this epoch: 0.0001280319702345878\n","Average loss for training batches in this epoch: 0.0001121616514865309\n","Average loss for validation batches in this epoch: 0.00012680258078034967\n","Average loss for training batches in this epoch: 0.0001121944296755828\n","Average loss for validation batches in this epoch: 0.0001262308214791119\n","Average loss for training batches in this epoch: 0.00011159395944559947\n","Average loss for validation batches in this epoch: 0.0001254170056199655\n","Average loss for training batches in this epoch: 0.00011013622861355543\n","Average loss for validation batches in this epoch: 0.00012428464833647013\n","Average loss for training batches in this epoch: 0.00010928083065664396\n","Average loss for validation batches in this epoch: 0.00012314850755501539\n","Average loss for training batches in this epoch: 0.00010849897807929665\n","Average loss for validation batches in this epoch: 0.00012251270527485758\n","Average loss for training batches in this epoch: 0.00010756272968137637\n","Average loss for validation batches in this epoch: 0.00012151950795669109\n","Average loss for training batches in this epoch: 0.00010705386375775561\n","Average loss for validation batches in this epoch: 0.00012100060121156275\n","Average loss for training batches in this epoch: 0.00010622812988003716\n","Average loss for validation batches in this epoch: 0.0001201041231979616\n","Average loss for training batches in this epoch: 0.0001055751636158675\n","Average loss for validation batches in this epoch: 0.0001193922507809475\n","Average loss for training batches in this epoch: 0.00010556218330748379\n","Average loss for validation batches in this epoch: 0.00011845761036965996\n","Average loss for training batches in this epoch: 0.00010391206160420552\n","Average loss for validation batches in this epoch: 0.00011776831524912268\n","Average loss for training batches in this epoch: 0.00010360262240283191\n","Average loss for validation batches in this epoch: 0.00011710783292073756\n","Average loss for training batches in this epoch: 0.00010257351095788181\n","Average loss for validation batches in this epoch: 0.00011582679871935397\n","Average loss for training batches in this epoch: 0.00010225231380900368\n","Average loss for validation batches in this epoch: 0.00011528600589372218\n","Average loss for training batches in this epoch: 0.00010165810090256855\n","Average loss for validation batches in this epoch: 0.0001147644070442766\n","Average loss for training batches in this epoch: 0.00010097139602294192\n","Average loss for validation batches in this epoch: 0.00011377131886547431\n","Average loss for training batches in this epoch: 0.00010013479186454788\n","Average loss for validation batches in this epoch: 0.00011361385986674577\n","Average loss for training batches in this epoch: 9.922515891958028e-05\n","Average loss for validation batches in this epoch: 0.00011182755406480283\n","Average loss for training batches in this epoch: 9.860031423158944e-05\n","Average loss for validation batches in this epoch: 0.00011182281014043838\n","Average loss for training batches in this epoch: 9.813886572374031e-05\n","Average loss for validation batches in this epoch: 0.00011062099656555802\n","Average loss for training batches in this epoch: 9.753054473549128e-05\n","Average loss for validation batches in this epoch: 0.0001098369830287993\n","Average loss for training batches in this epoch: 9.678399510448799e-05\n","Average loss for validation batches in this epoch: 0.00010930895223282278\n","Average loss for training batches in this epoch: 9.623667574487627e-05\n","Average loss for validation batches in this epoch: 0.00010846530494745821\n","Average loss for training batches in this epoch: 9.539612801745534e-05\n","Average loss for validation batches in this epoch: 0.00010789104999275878\n","Average loss for training batches in this epoch: 9.509284427622333e-05\n","Average loss for validation batches in this epoch: 0.00010686987661756575\n","Average loss for training batches in this epoch: 9.447116462979466e-05\n","Average loss for validation batches in this epoch: 0.000106650753878057\n","Average loss for training batches in this epoch: 9.398626571055502e-05\n","Average loss for validation batches in this epoch: 0.000105831197288353\n","Average loss for training batches in this epoch: 9.349786705570295e-05\n","Average loss for validation batches in this epoch: 0.00010497577750356868\n","Average loss for training batches in this epoch: 9.256581688532606e-05\n","Average loss for validation batches in this epoch: 0.00010489659325685352\n","Average loss for training batches in this epoch: 9.19274243642576e-05\n","Average loss for validation batches in this epoch: 0.00010362533794250339\n","Average loss for training batches in this epoch: 9.107935329666361e-05\n","Average loss for validation batches in this epoch: 0.00010289511556038633\n","Average loss for training batches in this epoch: 9.08259826246649e-05\n","Average loss for validation batches in this epoch: 0.00010229997133137658\n","Average loss for training batches in this epoch: 9.036008850671351e-05\n","Average loss for validation batches in this epoch: 0.00010140620724996552\n","Average loss for training batches in this epoch: 8.952446660259739e-05\n","Average loss for validation batches in this epoch: 0.0001015535817714408\n","Average loss for training batches in this epoch: 8.899734530132264e-05\n","Average loss for validation batches in this epoch: 0.00010012467100750655\n","Average loss for training batches in this epoch: 8.873094338923693e-05\n","Average loss for validation batches in this epoch: 9.945254714693874e-05\n","Average loss for training batches in this epoch: 8.789802814135328e-05\n","Average loss for validation batches in this epoch: 9.89937616395764e-05\n","Average loss for training batches in this epoch: 8.744360820855945e-05\n","Average loss for validation batches in this epoch: 9.83022473519668e-05\n","Average loss for training batches in this epoch: 8.73135359142907e-05\n","Average loss for validation batches in this epoch: 9.862285514827818e-05\n","Average loss for training batches in this epoch: 8.62309243530035e-05\n","Average loss for validation batches in this epoch: 9.728538861963898e-05\n","Average loss for training batches in this epoch: 8.60070576891303e-05\n","Average loss for validation batches in this epoch: 9.673887689132243e-05\n","Average loss for training batches in this epoch: 8.529834303772077e-05\n","Average loss for validation batches in this epoch: 9.570155089022592e-05\n","Average loss for training batches in this epoch: 8.469695603707805e-05\n","Average loss for validation batches in this epoch: 9.497947758063674e-05\n","Average loss for training batches in this epoch: 8.407907444052398e-05\n","Average loss for validation batches in this epoch: 9.469711949350312e-05\n","Average loss for training batches in this epoch: 8.343678928213194e-05\n","Average loss for validation batches in this epoch: 9.410897473571822e-05\n","Average loss for training batches in this epoch: 8.288068056572229e-05\n","Average loss for validation batches in this epoch: 9.317591320723295e-05\n","Average loss for training batches in this epoch: 8.262915798695758e-05\n","Average loss for validation batches in this epoch: 9.264811524190009e-05\n","Average loss for training batches in this epoch: 8.248281665146351e-05\n","Average loss for validation batches in this epoch: 9.197435429086909e-05\n","Average loss for training batches in this epoch: 8.127375622279942e-05\n","Average loss for validation batches in this epoch: 9.162708010990173e-05\n","Average loss for training batches in this epoch: 8.077781967585906e-05\n","Average loss for validation batches in this epoch: 9.137774031842127e-05\n","Average loss for training batches in this epoch: 8.046585571719334e-05\n","Average loss for validation batches in this epoch: 9.031613444676623e-05\n","Average loss for training batches in this epoch: 7.98038236098364e-05\n","Average loss for validation batches in this epoch: 8.983795123640448e-05\n","Average loss for training batches in this epoch: 7.963384268805385e-05\n","Average loss for validation batches in this epoch: 8.929380419431254e-05\n","Average loss for training batches in this epoch: 7.882170029915869e-05\n","Average loss for validation batches in this epoch: 8.844461262924597e-05\n","Average loss for training batches in this epoch: 7.8284036135301e-05\n","Average loss for validation batches in this epoch: 8.82935564732179e-05\n","Average loss for training batches in this epoch: 7.764589099679142e-05\n","Average loss for validation batches in this epoch: 8.746795356273651e-05\n","Average loss for training batches in this epoch: 7.756741979392245e-05\n","Average loss for validation batches in this epoch: 8.708916720934212e-05\n","Average loss for training batches in this epoch: 7.6780874223914e-05\n","Average loss for validation batches in this epoch: 8.63609675434418e-05\n","Average loss for training batches in this epoch: 7.64252181397751e-05\n","Average loss for validation batches in this epoch: 8.591004007030278e-05\n","Average loss for training batches in this epoch: 7.595437637064606e-05\n","Average loss for validation batches in this epoch: 8.520824485458434e-05\n","Average loss for training batches in this epoch: 7.543753599748015e-05\n","Average loss for validation batches in this epoch: 8.499303658027202e-05\n","Average loss for training batches in this epoch: 7.500522769987583e-05\n","Average loss for validation batches in this epoch: 8.425905252806842e-05\n","Average loss for training batches in this epoch: 7.483961235266179e-05\n","Average loss for validation batches in this epoch: 8.438813529210165e-05\n","Average loss for training batches in this epoch: 7.389124948531389e-05\n","Average loss for validation batches in this epoch: 8.332417928613722e-05\n","Average loss for training batches in this epoch: 7.351140811806545e-05\n","Average loss for validation batches in this epoch: 8.308882388519123e-05\n","Average loss for training batches in this epoch: 7.315584662137553e-05\n","Average loss for validation batches in this epoch: 8.21653738967143e-05\n","Average loss for training batches in this epoch: 7.294118404388428e-05\n","Average loss for validation batches in this epoch: 8.193926623789594e-05\n","Average loss for training batches in this epoch: 7.22988261259161e-05\n","Average loss for validation batches in this epoch: 8.109101327136159e-05\n","Average loss for training batches in this epoch: 7.167186413425952e-05\n","Average loss for validation batches in this epoch: 8.105170854832977e-05\n","Average loss for training batches in this epoch: 7.131855818443e-05\n","Average loss for validation batches in this epoch: 8.022466499824077e-05\n","Average loss for training batches in this epoch: 7.124610419850796e-05\n","Average loss for validation batches in this epoch: 7.963878306327388e-05\n","Average loss for training batches in this epoch: 7.040336640784517e-05\n","Average loss for validation batches in this epoch: 7.938899216242135e-05\n","Average loss for training batches in this epoch: 7.00476230122149e-05\n","Average loss for validation batches in this epoch: 7.887058745836839e-05\n","Average loss for training batches in this epoch: 6.970218964852393e-05\n","Average loss for validation batches in this epoch: 7.805726636433974e-05\n","Average loss for training batches in this epoch: 6.919083534739912e-05\n","Average loss for validation batches in this epoch: 7.816340803401545e-05\n","Average loss for training batches in this epoch: 6.888299685670063e-05\n","Average loss for validation batches in this epoch: 7.722215377725661e-05\n","Average loss for training batches in this epoch: 6.877420673845336e-05\n","Average loss for validation batches in this epoch: 7.687313336646184e-05\n","Average loss for training batches in this epoch: 6.827994366176426e-05\n","Average loss for validation batches in this epoch: 7.695115345995873e-05\n","Average loss for training batches in this epoch: 6.84447877574712e-05\n","Average loss for validation batches in this epoch: 7.605217979289591e-05\n","Average loss for training batches in this epoch: 6.697625940432772e-05\n","Average loss for validation batches in this epoch: 7.62994823162444e-05\n","Average loss for training batches in this epoch: 6.704335828544572e-05\n","Average loss for validation batches in this epoch: 7.510199066018686e-05\n","Average loss for training batches in this epoch: 6.670109723927453e-05\n","Average loss for validation batches in this epoch: 7.452013960573822e-05\n","Average loss for training batches in this epoch: 6.61220692563802e-05\n","Average loss for validation batches in this epoch: 7.393397390842438e-05\n","Average loss for training batches in this epoch: 6.561385816894472e-05\n","Average loss for validation batches in this epoch: 7.363261829596013e-05\n","Average loss for training batches in this epoch: 6.524705531774089e-05\n","Average loss for validation batches in this epoch: 7.300839206436649e-05\n","Average loss for training batches in this epoch: 6.487491918960586e-05\n","Average loss for validation batches in this epoch: 7.272506627487019e-05\n","Average loss for training batches in this epoch: 6.497798312921077e-05\n","Average loss for validation batches in this epoch: 7.244720472954214e-05\n","Average loss for training batches in this epoch: 6.446232146117836e-05\n","Average loss for validation batches in this epoch: 7.18047158443369e-05\n","Average loss for training batches in this epoch: 6.392977229552343e-05\n","Average loss for validation batches in this epoch: 7.177551015047356e-05\n","Average loss for training batches in this epoch: 6.343032873701304e-05\n","Average loss for validation batches in this epoch: 7.096472836565226e-05\n","Average loss for training batches in this epoch: 6.287537689786404e-05\n","Average loss for validation batches in this epoch: 7.048808038234711e-05\n","Average loss for training batches in this epoch: 6.254074105527252e-05\n","Average loss for validation batches in this epoch: 6.9989197072573e-05\n","Average loss for training batches in this epoch: 6.216925976332277e-05\n","Average loss for validation batches in this epoch: 6.962688348721713e-05\n","Average loss for training batches in this epoch: 6.214577297214419e-05\n","Average loss for validation batches in this epoch: 6.935609417269006e-05\n","Average loss for training batches in this epoch: 6.145510997157544e-05\n","Average loss for validation batches in this epoch: 6.882656452944502e-05\n","Average loss for training batches in this epoch: 6.10577262705192e-05\n","Average loss for validation batches in this epoch: 6.836534885223955e-05\n","Average loss for training batches in this epoch: 6.073606346035376e-05\n","Average loss for validation batches in this epoch: 6.80768716847524e-05\n","Average loss for training batches in this epoch: 6.0234149714233354e-05\n","Average loss for validation batches in this epoch: 6.815984670538455e-05\n","Average loss for training batches in this epoch: 6.031587690813467e-05\n","Average loss for validation batches in this epoch: 6.716529605910182e-05\n","Average loss for training batches in this epoch: 5.972976578050293e-05\n","Average loss for validation batches in this epoch: 6.669975846307352e-05\n","Average loss for training batches in this epoch: 5.9522411902435124e-05\n","Average loss for validation batches in this epoch: 6.653142190771177e-05\n","Average loss for training batches in this epoch: 5.894147761864588e-05\n","Average loss for validation batches in this epoch: 6.595371814910322e-05\n","Average loss for training batches in this epoch: 5.878714000573382e-05\n","Average loss for validation batches in this epoch: 6.559715984622017e-05\n","Average loss for training batches in this epoch: 5.8309800806455314e-05\n","Average loss for validation batches in this epoch: 6.560511246789247e-05\n","Average loss for training batches in this epoch: 5.7899393141269684e-05\n","Average loss for validation batches in this epoch: 6.484089681180194e-05\n","Average loss for training batches in this epoch: 5.7630084484117106e-05\n","Average loss for validation batches in this epoch: 6.489954830612987e-05\n","Average loss for training batches in this epoch: 5.7342014770256355e-05\n","Average loss for validation batches in this epoch: 6.405843305401504e-05\n","Average loss for training batches in this epoch: 5.704482100554742e-05\n","Average loss for validation batches in this epoch: 6.360046972986311e-05\n","Average loss for training batches in this epoch: 5.656214852933772e-05\n","Average loss for validation batches in this epoch: 6.394401134457439e-05\n","Average loss for training batches in this epoch: 5.632101965602487e-05\n","Average loss for validation batches in this epoch: 6.289934390224516e-05\n","Average loss for training batches in this epoch: 5.5943019106052816e-05\n","Average loss for validation batches in this epoch: 6.289270095294341e-05\n","Average loss for training batches in this epoch: 5.565675382968038e-05\n","Average loss for validation batches in this epoch: 6.210719584487379e-05\n","Average loss for training batches in this epoch: 5.5362565035466105e-05\n","Average loss for validation batches in this epoch: 6.178532203193754e-05\n","Average loss for training batches in this epoch: 5.4879565141163766e-05\n","Average loss for validation batches in this epoch: 6.157402822282165e-05\n","Average loss for training batches in this epoch: 5.463136767502874e-05\n","Average loss for validation batches in this epoch: 6.109124660724774e-05\n"]}],"source":["for _ in range(500):\n","    train_one_epoch(True)\n","# train_one_epoch()\n"]},{"cell_type":"code","execution_count":61,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy on test set: 100.0%\n"]}],"source":["model.eval()  # Set the model to evaluation mode\n","correct = 0\n","total = 0\n","\n","with torch.no_grad():  # Deactivates autograd, reduces memory usage and speeds up computations\n","    for features, labels in test_loader:\n","        outputs = model(features)\n","        _, predicted = torch.max(outputs.data, 1)  # Get the index of the max log-probability\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = 100 * correct / total\n","print(f\"Accuracy on test set: {accuracy}%\")\n"]},{"cell_type":"code","execution_count":62,"metadata":{},"outputs":[],"source":["def infer_completion(input_text: str, context_len):\n","    encoded_input = prep_input_string(input_text, context_len).unsqueeze(0).float()\n","    \n","    model.train(False)\n","    pred = model(encoded_input)\n","    return reverse_vocab[torch.argmax(softmax(pred, dim=1), dim=1).item()]"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[{"data":{"text/plain":["'16'"]},"execution_count":67,"metadata":{},"output_type":"execute_result"}],"source":["infer_completion(\"7 + 9 =\", context_len)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
