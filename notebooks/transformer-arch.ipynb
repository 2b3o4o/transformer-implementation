{"cells":[{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[],"source":["import torch\n","from torch import tensor, sin, cos\n","from math import sqrt\n","from torch.nn.functional import softmax\n","import spacy\n","from torchtext.vocab import GloVe\n","\n","glove = GloVe(dim=300)\n","\n","def par_attention(queries: tensor, keys: tensor, values: tensor, dim: int) -> tensor:\n","    raw_weights = torch.bmm(queries, keys.transpose(1, 2))\n","\n","    mask = torch.tril(torch.ones_like(raw_weights), diagonal=0)\n","    raw_weights = raw_weights.masked_fill(mask == 0, float('-inf'))\n","    print(f\"raw_weights.shape:{raw_weights.shape}\\nraw_weights: {raw_weights}\")\n","\n","    scale_factor = sqrt(dim)\n","    scaled_weights = softmax(raw_weights / scale_factor, dim=2)\n","    print(f\"scaled_weights.shape:{scaled_weights.shape}\\nscaled_weights: {scaled_weights}\")\n","\n","    # now scaled weights is a matrix where each row represents the scaled weights produced based on a given query.\n","    # meanwhile values just has a value vector on each row.\n","\n","    reshaped_scaled_weights = scaled_weights.view(scaled_weights.shape[0], scaled_weights.shape[1], scaled_weights.shape[2], 1)\n","    reshaped_values = values.view(1, values.shape[0], values.shape[1], values.shape[2])\n","\n","    scaled_values = reshaped_scaled_weights * reshaped_values\n","\n","    contextualized_values = torch.sum(scaled_values, 2)\n","    return contextualized_values\n","\n","def build_dictionary(file_path) -> (dict, dict):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        content = file.read()\n","    \n","    tokenizer = spacy.load(\"en_core_web_sm\")\n","    tokens = tokenizer(content)\n","\n","    unique_words = set(tokens)\n","    word_to_id = {word: i for i, word in enumerate(unique_words)}\n","    id_to_word = {i: word for i, word in enumerate(unique_words)}\n","    return word_to_id, id_to_word\n","\n","def positional_embedding(word, pos) -> tensor:\n","    model_dims = 300\n","\n","    positional_encoding = torch.tensor([0.0] * model_dims)\n","    for i in range(0, model_dims // 2):\n","        positional_encoding[2 * i] = sin(torch.tensor(pos / (10000 ** (2 * i / model_dims))))\n","        positional_encoding[2 * i + 1] = cos(torch.tensor(pos / (10000 ** (2 * i / model_dims))))\n","\n","    embedding = glove[word]\n","    embedding += positional_encoding\n","    return embedding\n","\n","\n","def encode_input_string(str, context_len) -> tensor:\n","    tokenizer = spacy.load(\"en_core_web_sm\")\n","    tokens = tokenizer(str)\n","\n","    output = torch.zeros(size=[context_len, 300])\n","    for i, token in enumerate(tokens):\n","        output[i] = positional_embedding(token.text, i)\n","\n","    return output"]},{"cell_type":"markdown","metadata":{},"source":["Goal for this notebook is to implement an attention block, and then to build out the rest of the transformer architecture. I'll start with a single-headed version, then build out multi-headedness (which shouldn't be a lot of additional work, I think). The original paper uses a model dimensionality of 512 and 8 heads, which each work on 512 / 8 = 64 dimensions. In my case my embedding function produces 300 dimension vectors, so I think I'll experiment with using 6 heads with 50 dimensions each.\n","\n","An attention head includes:\n","1. Separate, learned linear projections for Q, K, V vectors. I think this is basically just a feed forward layer without a nonlinearity?\n","In any case, this linear projection reduces the dimensionality of the input \n","2. The scaled dot product attention function.\n","\n","Then, a multi-head attention block contains:\n","1. Some number of attention heads.\n","2. A concatenation step. This just takes the model_dim / h length vectors that are output from the attention head and concatenates them.\n","3. A learned linear projection. My intuition is that this projection \"blends\" the h concatenated vectors into a more meaningful and cohesive whole.\n","\n","The whole transformer layer / block varies from encode, to decode, to decode-only. In the original paper's decode block, a transformer layer includes two separate attention blocks, one of which allows for queries to be drawn from the previous decoder layer while values and keys are drawn from the encoder. In a decode-only architecture I don't think there's any meaningful or useful analogy for this, so instead I'll be using only a single attention block per transformer layer.\n","\n","With all that said, my transformer layers will include:\n","1. Masked multi-head attention - i.e. the attention block outlined above. The input is just some number of model_dim length embedding vectors, which comes from either the previous transformer layer or, for the first transformer layer, the positional embedding function. These are differentiated into Q, K, V vectors by the linear projections in the attention heads.\n","2. Residual connection defined as: LayerNorm(x + Sublayer(x))\n","3. Feed forward block. This is two feed forward layers, with a single ReLU in between. In the paper, the layers share a hidden dimension which is four times larger than the model dimension. I'll experiment with something similar.\n","4. Another residual connection.\n","\n","The complete architecture:\n","1. Positional encoding function applied to input tokens/words. (update: in this case I'm not training any aspect of this as part of my model, so I think it makes sense to keep it outside of the model itself.)\n","2. Sequential transformer layers. In the paper, there are 6.\n","3. A linear layer that takes in all the vectors output by the final transformer layer, and has outputs for each possible next word.\n","4. Softmax function over outputs gives us probabilities for next word, the final output of the network.\n","\n","Note to self: It might be good to build an API that lets the user specify a custom/different embedding function, but that's probably not a priority."]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[],"source":["import torch.nn as nn\n","\n","class AttentionHead(nn.Module):\n","    # For simplicity, I assume query, key, and value vectors have the same dimensionality\n","    def __init__(self, model_dim, vectors_dim):\n","        super().__init__()\n","        self.model_dim = model_dim\n","        self.vectors_dim = vectors_dim\n","        self.Q_proj = nn.Linear(model_dim, vectors_dim)\n","        self.K_proj = nn.Linear(model_dim, vectors_dim)\n","        self.V_proj = nn.Linear(model_dim, vectors_dim)\n","\n","    def forward(self, x):\n","        # each row of x is a vector representing the meaning of the token at the corresponding position with whatever context we've attained so far.\n","        Q = self.Q_proj(x)\n","        K = self.K_proj(x)\n","        V = self.V_proj(x)\n","        print(\"Shape of Q matrix: \", Q.shape)\n","        print(\"Shape of K matrix: \", K.shape)\n","        print(\"Shape of V matrix: \", V.shape)\n","        output = par_attention(Q, K, V, self.vectors_dim)\n","        return output\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, model_dim, num_heads):\n","        super().__init__()\n","        self.att_heads = nn.ModuleList([AttentionHead(model_dim, model_dim // num_heads) for _ in range(num_heads)])\n","        self.proj = nn.Linear(model_dim, model_dim)\n","\n","    def forward(self, x):\n","        head_outputs = [head(x) for head in self.att_heads]\n","        x = torch.concat(head_outputs, dim=1)\n","        x = self.proj(x)\n","        return x\n","        \n","class TransformerLayer(nn.Module):\n","    def __init__(self, model_dim, num_heads, ff_hidden_dim, context_len):\n","        super().__init__()\n","        self.attention_block = MultiHeadAttention(model_dim, num_heads)\n","        self.norm1 = nn.LayerNorm(normalized_shape=[context_len, model_dim])\n","        self.ff1 = nn.Linear(model_dim, ff_hidden_dim)\n","        self.ff_relu = nn.ReLU()\n","        self.ff2 = nn.Linear(ff_hidden_dim, model_dim)\n","        self.norm2 = nn.LayerNorm(normalized_shape=[context_len, model_dim])\n","\n","    def forward(self, x):\n","        x_res = x\n","        x = self.attention_block(x)\n","        x += x_res\n","        x = self.norm1(x)\n","\n","        x_res = x\n","        x = self.ff1(x)\n","        x = self.ff_relu(x)\n","        x = self.ff2(x)\n","        x += x_res\n","        x = self.norm2(x)\n","\n","        return x\n"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["\n","class TransformerNetwork(nn.Module):\n","    def __init__(self, num_layers, model_dim, att_heads, ff_hidden_dim, context_len, output_dict_size):\n","        super().__init__()\n","        self.trans_layers = nn.ModuleList([TransformerLayer(model_dim, att_heads, ff_hidden_dim, context_len) for _ in range(num_layers)])\n","        self.word_predictor = nn.Linear(model_dim * context_len, output_dict_size)\n","        print(\"model_dim * context_len = \", model_dim * context_len)\n","\n","    def forward(self, x):\n","        for layer in self.trans_layers:\n","            x = layer.forward(x)\n","        print(\"Shape of x before view: \", x.shape)\n","        x = x.view(x.shape[0], -1)\n","        print(\"Shape of x after view: \", x.shape)\n","        x = self.word_predictor(x)\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["There's a lot of complexity I still need to figure out in terms of how I need to package this model. That includes the correct use of my input encoding functions, use of the output dictionary, and probably some debugging of the model itself. Gonna try to get the model to make a random (untrained) prediction on some dummy text."]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["model_dim * context_len =  76800\n"]}],"source":["# These parameters match what's described in \"attention is all you need\". Not sure what their dictionary structure/size is though.\n","# Also not sure how they handle context length...\n","# paper_model = TransformerNetwork(num_layers=6, model_dim=512, att_heads=8, ff_hidden_dim=2048, context_len=256, output_dict_size=1)\n","\n","word_to_id, id_to_word = build_dictionary('../data/much_ado_about_nothing_gut.txt')\n","dictionary_len = len(id_to_word)\n","context_len = 256\n","basic_model = TransformerNetwork(num_layers=1, model_dim=300, att_heads=6, ff_hidden_dim=1200, context_len=context_len, output_dict_size=dictionary_len)\n","\n","test_input = \"The next word is\"\n","encoded_input = encode_input_string(test_input, context_len)\n","\n"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of Q matrix:  torch.Size([1, 256, 50])\n","Shape of K matrix:  torch.Size([1, 256, 50])\n","Shape of V matrix:  torch.Size([1, 256, 50])\n","raw_weights.shape:torch.Size([1, 256, 256])\n","raw_weights: tensor([[[-2.9087,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n","         [-3.0389, -2.4463,    -inf,  ...,    -inf,    -inf,    -inf],\n","         [-1.4133, -0.9801, -0.1038,  ...,    -inf,    -inf,    -inf],\n","         ...,\n","         [ 0.0836,  0.1701,  0.1496,  ...,  0.0048,    -inf,    -inf],\n","         [ 0.0836,  0.1701,  0.1496,  ...,  0.0048,  0.0048,    -inf],\n","         [ 0.0836,  0.1701,  0.1496,  ...,  0.0048,  0.0048,  0.0048]]],\n","       grad_fn=<MaskedFillBackward0>)\n","scaled_weights.shape:torch.Size([1, 256, 256])\n","scaled_weights: tensor([[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","         [0.4791, 0.5209, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","         [0.3061, 0.3255, 0.3684,  ..., 0.0000, 0.0000, 0.0000],\n","         ...,\n","         [0.0040, 0.0040, 0.0040,  ..., 0.0039, 0.0000, 0.0000],\n","         [0.0040, 0.0040, 0.0040,  ..., 0.0039, 0.0039, 0.0000],\n","         [0.0039, 0.0040, 0.0040,  ..., 0.0039, 0.0039, 0.0039]]],\n","       grad_fn=<SoftmaxBackward0>)\n","Shape of Q matrix:  torch.Size([1, 256, 50])\n","Shape of K matrix:  torch.Size([1, 256, 50])\n","Shape of V matrix:  torch.Size([1, 256, 50])\n","raw_weights.shape:torch.Size([1, 256, 256])\n","raw_weights: tensor([[[ 0.6981,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n","         [ 1.8410,  0.0435,    -inf,  ...,    -inf,    -inf,    -inf],\n","         [ 1.0278,  0.1299,  0.1493,  ...,    -inf,    -inf,    -inf],\n","         ...,\n","         [-0.1130, -0.0734, -0.0454,  ..., -0.0070,    -inf,    -inf],\n","         [-0.1130, -0.0734, -0.0454,  ..., -0.0070, -0.0070,    -inf],\n","         [-0.1130, -0.0734, -0.0454,  ..., -0.0070, -0.0070, -0.0070]]],\n","       grad_fn=<MaskedFillBackward0>)\n","scaled_weights.shape:torch.Size([1, 256, 256])\n","scaled_weights: tensor([[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","         [0.5632, 0.4368, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","         [0.3618, 0.3187, 0.3195,  ..., 0.0000, 0.0000, 0.0000],\n","         ...,\n","         [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0000, 0.0000],\n","         [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0000],\n","         [0.0038, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039]]],\n","       grad_fn=<SoftmaxBackward0>)\n","Shape of Q matrix:  torch.Size([1, 256, 50])\n","Shape of K matrix:  torch.Size([1, 256, 50])\n","Shape of V matrix:  torch.Size([1, 256, 50])\n","raw_weights.shape:torch.Size([1, 256, 256])\n","raw_weights: tensor([[[ 1.9769,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n","         [ 2.4035,  2.0634,    -inf,  ...,    -inf,    -inf,    -inf],\n","         [ 3.2440,  3.1534,  2.1715,  ...,    -inf,    -inf,    -inf],\n","         ...,\n","         [ 0.0642,  0.0177, -0.0619,  ..., -0.0118,    -inf,    -inf],\n","         [ 0.0642,  0.0177, -0.0619,  ..., -0.0118, -0.0118,    -inf],\n","         [ 0.0642,  0.0177, -0.0619,  ..., -0.0118, -0.0118, -0.0118]]],\n","       grad_fn=<MaskedFillBackward0>)\n","scaled_weights.shape:torch.Size([1, 256, 256])\n","scaled_weights: tensor([[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","         [0.5120, 0.4880, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","         [0.3513, 0.3468, 0.3019,  ..., 0.0000, 0.0000, 0.0000],\n","         ...,\n","         [0.0040, 0.0040, 0.0039,  ..., 0.0039, 0.0000, 0.0000],\n","         [0.0040, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0000],\n","         [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039]]],\n","       grad_fn=<SoftmaxBackward0>)\n","Shape of Q matrix:  torch.Size([1, 256, 50])\n","Shape of K matrix:  torch.Size([1, 256, 50])\n","Shape of V matrix:  torch.Size([1, 256, 50])\n","raw_weights.shape:torch.Size([1, 256, 256])\n","raw_weights: tensor([[[-1.0972e+00,        -inf,        -inf,  ...,        -inf,\n","                 -inf,        -inf],\n","         [-3.3047e-01,  2.8872e-02,        -inf,  ...,        -inf,\n","                 -inf,        -inf],\n","         [-7.2955e-01, -6.7660e-02, -3.3345e-01,  ...,        -inf,\n","                 -inf,        -inf],\n","         ...,\n","         [-1.8420e-01, -2.0757e-01, -1.1888e-01,  ...,  6.6603e-04,\n","                 -inf,        -inf],\n","         [-1.8420e-01, -2.0757e-01, -1.1888e-01,  ...,  6.6603e-04,\n","           6.6603e-04,        -inf],\n","         [-1.8420e-01, -2.0757e-01, -1.1888e-01,  ...,  6.6603e-04,\n","           6.6603e-04,  6.6603e-04]]], grad_fn=<MaskedFillBackward0>)\n","scaled_weights.shape:torch.Size([1, 256, 256])\n","scaled_weights: tensor([[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","         [0.4873, 0.5127, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","         [0.3169, 0.3480, 0.3351,  ..., 0.0000, 0.0000, 0.0000],\n","         ...,\n","         [0.0038, 0.0038, 0.0039,  ..., 0.0039, 0.0000, 0.0000],\n","         [0.0038, 0.0038, 0.0039,  ..., 0.0039, 0.0039, 0.0000],\n","         [0.0038, 0.0038, 0.0038,  ..., 0.0039, 0.0039, 0.0039]]],\n","       grad_fn=<SoftmaxBackward0>)\n","Shape of Q matrix:  torch.Size([1, 256, 50])\n","Shape of K matrix:  torch.Size([1, 256, 50])\n","Shape of V matrix:  torch.Size([1, 256, 50])\n","raw_weights.shape:torch.Size([1, 256, 256])\n","raw_weights: tensor([[[ 0.4209,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n","         [ 1.2892,  1.2817,    -inf,  ...,    -inf,    -inf,    -inf],\n","         [ 1.0792,  0.6342,  0.8485,  ...,    -inf,    -inf,    -inf],\n","         ...,\n","         [-0.0679, -0.0291, -0.1233,  ..., -0.0070,    -inf,    -inf],\n","         [-0.0679, -0.0291, -0.1233,  ..., -0.0070, -0.0070,    -inf],\n","         [-0.0679, -0.0291, -0.1233,  ..., -0.0070, -0.0070, -0.0070]]],\n","       grad_fn=<MaskedFillBackward0>)\n","scaled_weights.shape:torch.Size([1, 256, 256])\n","scaled_weights: tensor([[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","         [0.5003, 0.4997, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","         [0.3440, 0.3230, 0.3330,  ..., 0.0000, 0.0000, 0.0000],\n","         ...,\n","         [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0000, 0.0000],\n","         [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0000],\n","         [0.0039, 0.0039, 0.0038,  ..., 0.0039, 0.0039, 0.0039]]],\n","       grad_fn=<SoftmaxBackward0>)\n","Shape of Q matrix:  torch.Size([1, 256, 50])\n","Shape of K matrix:  torch.Size([1, 256, 50])\n","Shape of V matrix:  torch.Size([1, 256, 50])\n","raw_weights.shape:torch.Size([1, 256, 256])\n","raw_weights: tensor([[[-0.3687,    -inf,    -inf,  ...,    -inf,    -inf,    -inf],\n","         [-1.2982, -1.3240,    -inf,  ...,    -inf,    -inf,    -inf],\n","         [-1.3572, -1.7994, -2.2708,  ...,    -inf,    -inf,    -inf],\n","         ...,\n","         [-0.0140, -0.0726,  0.0382,  ..., -0.0042,    -inf,    -inf],\n","         [-0.0140, -0.0726,  0.0382,  ..., -0.0042, -0.0042,    -inf],\n","         [-0.0140, -0.0726,  0.0382,  ..., -0.0042, -0.0042, -0.0042]]],\n","       grad_fn=<MaskedFillBackward0>)\n","scaled_weights.shape:torch.Size([1, 256, 256])\n","scaled_weights: tensor([[[1.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","         [0.5009, 0.4991, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n","         [0.3548, 0.3333, 0.3118,  ..., 0.0000, 0.0000, 0.0000],\n","         ...,\n","         [0.0039, 0.0039, 0.0040,  ..., 0.0039, 0.0000, 0.0000],\n","         [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0000],\n","         [0.0039, 0.0039, 0.0039,  ..., 0.0039, 0.0039, 0.0039]]],\n","       grad_fn=<SoftmaxBackward0>)\n"]},{"ename":"RuntimeError","evalue":"mat1 and mat2 shapes cannot be multiplied (1536x50 and 300x300)","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m preds \u001b[39m=\u001b[39m basic_model(encoded_input\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m))\n","File \u001b[1;32mc:\\src\\alignment-study\\transformer-implementation\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[1;32mIn[46], line 10\u001b[0m, in \u001b[0;36mTransformerNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m      9\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrans_layers:\n\u001b[1;32m---> 10\u001b[0m         x \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39;49mforward(x)\n\u001b[0;32m     11\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mShape of x before view: \u001b[39m\u001b[39m\"\u001b[39m, x\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     12\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n","Cell \u001b[1;32mIn[45], line 48\u001b[0m, in \u001b[0;36mTransformerLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     47\u001b[0m     x_res \u001b[39m=\u001b[39m x\n\u001b[1;32m---> 48\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention_block(x)\n\u001b[0;32m     49\u001b[0m     x \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m x_res\n\u001b[0;32m     50\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm1(x)\n","File \u001b[1;32mc:\\src\\alignment-study\\transformer-implementation\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","Cell \u001b[1;32mIn[45], line 33\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     31\u001b[0m head_outputs \u001b[39m=\u001b[39m [head(x) \u001b[39mfor\u001b[39;00m head \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39matt_heads]\n\u001b[0;32m     32\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mconcat(head_outputs, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m---> 33\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mproj(x)\n\u001b[0;32m     34\u001b[0m \u001b[39mreturn\u001b[39;00m x\n","File \u001b[1;32mc:\\src\\alignment-study\\transformer-implementation\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n","File \u001b[1;32mc:\\src\\alignment-study\\transformer-implementation\\venv\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n","\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1536x50 and 300x300)"]}],"source":["preds = basic_model(encoded_input.unsqueeze(0))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
