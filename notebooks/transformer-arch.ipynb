{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import torch\n","from torch import tensor\n","from math import sqrt\n","from torch.nn.functional import softmax\n","\n","def par_attention(queries: tensor, keys: tensor, values: tensor, dim: int) -> tensor:\n","    raw_weights = queries @ keys.T\n","\n","    mask = torch.tril(torch.ones_like(raw_weights), diagonal=0)\n","    raw_weights = raw_weights.masked_fill(mask == 0, float('-inf'))\n","\n","    scale_factor = sqrt(dim)\n","    scaled_weights = softmax(raw_weights / scale_factor, dim=1)\n","\n","    # now scaled weights is a matrix where each row represents the scaled weights produced based on a given query.\n","    # meanwhile values just has a value vector on each row.\n","\n","    reshaped_scaled_weights = scaled_weights.view(scaled_weights.shape[0], scaled_weights.shape[1], 1)\n","    reshaped_values = values.view(1, values.shape[0], values.shape[1])\n","\n","    scaled_values = reshaped_scaled_weights * reshaped_values\n","\n","    contextualized_values = torch.sum(scaled_values, 1)\n","    return contextualized_values"]},{"cell_type":"markdown","metadata":{},"source":["Goal for this notebook is to implement an attention block, and then to build out the rest of the transformer architecture. I'll start with a single-headed version, then build out multi-headedness (which shouldn't be a lot of additional work, I think). The original paper uses a model dimensionality of 512 and 8 heads, which each work on 512 / 8 = 64 dimensions. In my case my embedding function produces 300 dimension vectors, so I think I'll experiment with using 6 heads with 50 dimensions each.\n","\n","An attention head includes:\n","1. Separate, learned linear projections for Q, K, V vectors. I think this is basically just a feed forward layer without a nonlinearity?\n","In any case, this linear projection reduces the dimensionality of the input \n","2. The scaled dot product attention function.\n","\n","Then, a multi-head attention block contains:\n","1. Some number of attention heads.\n","2. A concatenation step. This just takes the model_dim / h length vectors that are output from the attention head and concatenates them.\n","3. A learned linear projection. My intuition is that this projection \"blends\" the h concatenated vectors into a more meaningful and cohesive whole.\n","\n","The whole transformer layer / block varies from encode, to decode, to decode-only. In the original paper's decode block, a transformer layer includes two separate attention blocks, one of which allows for queries to be drawn from the previous decoder layer while values and keys are drawn from the encoder. In a decode-only architecture I don't think there's any meaningful or useful analogy for this, so instead I'll be using only a single attention block per transformer layer.\n","\n","With all that said, my transformer layers will include:\n","1. Masked multi-head attention - i.e. the attention block outlined above. The input is just some number of model_dim length embedding vectors, which comes from either the previous transformer layer or, for the first transformer layer, the positional embedding function. These are differentiated into Q, K, V vectors by the linear projections in the attention heads.\n","2. Residual connection defined as: LayerNorm(x + Sublayer(x))\n","3. Feed forward block. This is two feed forward layers, with a single ReLU in between. In the paper, the layers share a hidden dimension which is four times larger than the model dimension. I'll experiment with something similar.\n","4. Another residual connection.\n","\n","The complete architecture:\n","1. Positional encoding function applied to input tokens/words. (update: in this case I'm not training any aspect of this as part of my model, so I think it makes sense to keep it outside of the model itself.)\n","2. Sequential transformer layers. In the paper, there are 6.\n","3. A linear layer that takes in all the vectors output by the final transformer layer, and has outputs for each possible next word.\n","4. Softmax function over outputs gives us probabilities for next word, the final output of the network.\n","\n","Note to self: It might be good to build an API that lets the user specify a custom/different embedding function, but that's probably not a priority."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import torch.nn as nn\n","\n","class AttentionHead(nn.Module):\n","    # For simplicity, I assume query, key, and value vectors have the same dimensionality\n","    def __init__(self, model_dim, vectors_dim):\n","        super().__init__()\n","        self.model_dim = model_dim\n","        self.vectors_dim = vectors_dim\n","        self.Q_proj = nn.Linear(model_dim, vectors_dim)\n","        self.K_proj = nn.Linear(model_dim, vectors_dim)\n","        self.V_proj = nn.Linear(model_dim, vectors_dim)\n","\n","    def forward(self, x):\n","        # each row of x is a vector representing the meaning of the token at the corresponding position with whatever context we've attained so far.\n","        Q = self.Q_proj(x)\n","        K = self.K_proj(x)\n","        V = self.V_proj(x)\n","\n","        output = par_attention(Q, K, V, self.vectors_dim)\n","        return output\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, model_dim, num_heads):\n","        super().__init__()\n","        self.att_heads = nn.ModuleList([AttentionHead(model_dim, model_dim // num_heads) for _ in range(num_heads)])\n","        self.proj = nn.Linear(model_dim, model_dim)\n","\n","    def forward(self, x):\n","        head_outputs = [head(x) for head in self.att_heads]\n","        x = torch.concat(head_outputs, dim=1)\n","        x = self.proj(x)\n","        return x\n","        \n","class TransformerLayer(nn.Module):\n","    def __init__(self, model_dim, num_heads, ff_hidden_dim):\n","        super().__init__()\n","        self.attention_block = MultiHeadAttention(model_dim, num_heads)\n","        self.norm1 = nn.LayerNorm()\n","        self.ff1 = nn.Linear(model_dim, ff_hidden_dim)\n","        self.ff_relu = nn.ReLU()\n","        self.ff2 = nn.Linear(ff_hidden_dim, model_dim)\n","        self.norm2 = nn.LayerNorm()\n","\n","    def forward(self, x):\n","        x_res = x\n","        x = self.attention_block(x)\n","        x += x_res\n","        x = self.norm1(x)\n","\n","        x_res = x\n","        x = self.ff1(x)\n","        x = self.ff_relu(x)\n","        x = self.ff2(x)\n","        x += x_res\n","        x = self.norm2(x)\n","\n","        return x\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["\n","class TransformerNetwork(nn.Module):\n","    def __init__(self, num_layers, model_dim, att_heads, ff_hidden_dim, context_len, output_dict_size):\n","        super().__init__()\n","        self.trans_layers = nn.ModuleList([TransformerLayer(model_dim, att_heads, ff_hidden_dim) for _ in range(num_layers)])\n","        self.word_predictor = nn.Linear(model_dim * context_len, output_dict_size)\n","\n","    def forward(self, x):\n","        for layer in self.trans_layers:\n","            x = layer.forward(x)\n","\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"ename":"TypeError","evalue":"TransformerNetwork.__init__() got an unexpected keyword argument 'model_dims'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m TransformerNetwork(num_layers\u001b[39m=\u001b[39;49m\u001b[39m6\u001b[39;49m, model_dims\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m, att_heads\u001b[39m=\u001b[39;49m\u001b[39m8\u001b[39;49m, att_key_dims\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m8\u001b[39;49m, att_value_dims\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m8\u001b[39;49m, ff_dims\u001b[39m=\u001b[39;49m\u001b[39m2048\u001b[39;49m)\n","\u001b[1;31mTypeError\u001b[0m: TransformerNetwork.__init__() got an unexpected keyword argument 'model_dims'"]}],"source":["model = TransformerNetwork(num_layers=6, model_dims=512, att_heads=8, att_key_dims=512//8, att_value_dims=512//8, ff_dims=2048)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
