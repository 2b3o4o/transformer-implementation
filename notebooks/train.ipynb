{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import torch\n","from torch import tensor, sin, cos\n","from math import sqrt\n","from torch.nn.functional import softmax\n","import spacy\n","from torchtext.vocab import GloVe"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["glove = GloVe(dim=300)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def par_attention(queries: tensor, keys: tensor, values: tensor, dim: int) -> tensor:\n","    raw_weights = torch.bmm(queries, keys.transpose(1, 2))\n","\n","    mask = torch.tril(torch.ones_like(raw_weights), diagonal=0)\n","    raw_weights = raw_weights.masked_fill(mask == 0, float('-inf'))\n","    # print(f\"raw_weights.shape:{raw_weights.shape}\\nraw_weights: {raw_weights}\")\n","\n","    scale_factor = sqrt(dim)\n","    scaled_weights = softmax(raw_weights / scale_factor, dim=2)\n","    # print(f\"scaled_weights.shape:{scaled_weights.shape}\\nscaled_weights: {scaled_weights}\")\n","\n","    # now scaled weights is a matrix where each row represents the scaled weights produced based on a given query.\n","    # meanwhile values just has a value vector on each row.\n","\n","    reshaped_scaled_weights = scaled_weights.view(scaled_weights.shape[0], scaled_weights.shape[1], scaled_weights.shape[2], 1)\n","    reshaped_values = values.view(1, values.shape[0], values.shape[1], values.shape[2])\n","\n","    scaled_values = reshaped_scaled_weights * reshaped_values\n","\n","    contextualized_values = torch.sum(scaled_values, 2)\n","    return contextualized_values\n","\n","def build_dictionary(file_path) -> (dict, dict):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        content = file.read()\n","    \n","    tokenizer = spacy.load(\"en_core_web_sm\")\n","    tokens = tokenizer(content)\n","    unique_words = set()\n","    for token in tokens:\n","        unique_words.add(str(token))\n","    word_to_id = {str(word): i for i, word in enumerate(unique_words)}\n","    id_to_word = {i: str(word) for i, word in enumerate(unique_words)}\n","\n","    return word_to_id, id_to_word\n","\n","def positional_embedding(word, pos) -> tensor:\n","    model_dims = 300\n","\n","    positional_encoding = torch.tensor([0.0] * model_dims)\n","    for i in range(0, model_dims // 2):\n","        positional_encoding[2 * i] = sin(torch.tensor(pos / (10000 ** (2 * i / model_dims))))\n","        positional_encoding[2 * i + 1] = cos(torch.tensor(pos / (10000 ** (2 * i / model_dims))))\n","\n","    embedding = glove[word]\n","    embedding += positional_encoding\n","    return embedding\n","\n","\n","def encode_input_string(str, context_len) -> tensor:\n","    tokenizer = spacy.load(\"en_core_web_sm\")\n","    tokens = tokenizer(str)\n","\n","    output = torch.zeros(size=[context_len, 300])\n","    for i, token in enumerate(tokens):\n","        output[i] = positional_embedding(token.text, i)\n","\n","    return output"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["import torch.nn as nn\n","\n","class AttentionHead(nn.Module):\n","    # For simplicity, I assume query, key, and value vectors have the same dimensionality\n","    def __init__(self, model_dim, vectors_dim):\n","        super().__init__()\n","        self.model_dim = model_dim\n","        self.vectors_dim = vectors_dim\n","        self.Q_proj = nn.Linear(model_dim, vectors_dim)\n","        self.K_proj = nn.Linear(model_dim, vectors_dim)\n","        self.V_proj = nn.Linear(model_dim, vectors_dim)\n","\n","    def forward(self, x):\n","        # each row of x is a vector representing the meaning of the token at the corresponding position with whatever context we've attained so far.\n","        Q = self.Q_proj(x)\n","        K = self.K_proj(x)\n","        V = self.V_proj(x)\n","        # print(\"Shape of Q matrix: \", Q.shape)\n","        # print(\"Shape of K matrix: \", K.shape)\n","        # print(\"Shape of V matrix: \", V.shape)\n","        output = par_attention(Q, K, V, self.vectors_dim)\n","        return output\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, model_dim, num_heads):\n","        super().__init__()\n","        self.att_heads = nn.ModuleList([AttentionHead(model_dim, model_dim // num_heads) for _ in range(num_heads)])\n","        self.proj = nn.Linear(model_dim, model_dim)\n","\n","    def forward(self, x):\n","        head_outputs = [head(x) for head in self.att_heads]\n","        x = torch.concat(head_outputs, dim=2)\n","        x = self.proj(x)\n","        return x\n","        \n","class TransformerLayer(nn.Module):\n","    def __init__(self, model_dim, num_heads, ff_hidden_dim, context_len):\n","        super().__init__()\n","        self.attention_block = MultiHeadAttention(model_dim, num_heads)\n","        self.norm1 = nn.LayerNorm(normalized_shape=[context_len, model_dim])\n","        self.ff1 = nn.Linear(model_dim, ff_hidden_dim)\n","        self.ff_relu = nn.ReLU()\n","        self.ff2 = nn.Linear(ff_hidden_dim, model_dim)\n","        self.norm2 = nn.LayerNorm(normalized_shape=[context_len, model_dim])\n","\n","    def forward(self, x):\n","        x_res = x\n","        x = self.attention_block(x)\n","        x += x_res\n","        x = self.norm1(x)\n","\n","        x_res = x\n","        x = self.ff1(x)\n","        x = self.ff_relu(x)\n","        x = self.ff2(x)\n","        x += x_res\n","        x = self.norm2(x)\n","\n","        return x\n"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["\n","class TransformerNetwork(nn.Module):\n","    def __init__(self, num_layers, model_dim, att_heads, ff_hidden_dim, context_len, output_dict_size):\n","        super().__init__()\n","        self.trans_layers = nn.ModuleList([TransformerLayer(model_dim, att_heads, ff_hidden_dim, context_len) for _ in range(num_layers)])\n","        self.word_predictor = nn.Linear(model_dim * context_len, output_dict_size)\n","        print(\"model_dim * context_len = \", model_dim * context_len)\n","\n","    def forward(self, x):\n","        for layer in self.trans_layers:\n","            x = layer.forward(x)\n","        # print(\"Shape of x before view: \", x.shape)\n","        x = x.view(x.shape[0], -1)\n","        # print(\"Shape of x after view: \", x.shape)\n","        x = self.word_predictor(x)\n","        return x\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# These parameters match what's described in \"attention is all you need\". The exception is that they probably use a different tokenizer and have the ability to output any token.\n","# Also not sure how they handle context length...\n","# paper_model = TransformerNetwork(num_layers=6, model_dim=512, att_heads=8, ff_hidden_dim=2048, context_len=256, output_dict_size=1)\n","\n","word_to_id, id_to_word = build_dictionary('../data/much_ado_about_nothing_gut.txt')"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["model_dim * context_len =  9600\n"]}],"source":["\n","dictionary_len = len(id_to_word)\n","context_len = 32\n","model = TransformerNetwork(num_layers=2, model_dim=300, att_heads=6, ff_hidden_dim=1200, context_len=context_len, output_dict_size=dictionary_len)\n","\n","test_input = \"The next word is\"\n","encoded_input = encode_input_string(test_input, context_len)\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# This approach kinda sucks ay? Cause it assumes I already know what the last token is... Whatever, will iterate on this lol\n","def encode_inputs(input_list, context_len) -> tensor:\n","    output = torch.zeros(size=[len(input_list), context_len, 300])\n","    for i, input in enumerate(input_list):\n","        output[i] = encode_input_string(input, context_len)\n","    return output\n","\n","def encode_outputs(output_tokens: [str]) -> tensor:\n","    output_cats = torch.zeros(size=[len(output_tokens)]).long()\n","    for i, token in enumerate(output_tokens):\n","        output_cats[i] = word_to_id[token]\n","    return output_cats"]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([9])\n","tensor([1037])\n"]}],"source":["train_features = encode_inputs([\"Did I not tell you she was innocent\"], context_len)\n","train_labels = encode_outputs([\"?\"])\n","print(train_labels)\n","\n","val_features = encode_inputs([\"Well, I am glad that all things sort so well\"], context_len)\n","val_labels = encode_outputs([\".\"])\n","print(val_labels)"]},{"cell_type":"code","execution_count":67,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset\n","\n","class CompletionDataset(Dataset):\n","    def __init__(self, features, labels):\n","        self.features = features\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return self.features.shape[0]\n","\n","    def __getitem__(self, index):\n","        return self.features[index], self.labels[index]\n","\n","train_dataset = CompletionDataset(train_features, train_labels)\n","val_dataset = CompletionDataset(val_features, val_labels)"]},{"cell_type":"code","execution_count":68,"metadata":{},"outputs":[],"source":["from torch.utils.data import DataLoader\n","\n","loss_func = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.2)\n","train_loader = DataLoader(dataset=train_dataset, batch_size=1, shuffle=True)\n","val_loader = DataLoader(dataset=val_dataset, batch_size=1, shuffle=False)"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["step= 0\n","preds:tensor([[ 0.3106,  0.7455,  0.0710,  ..., -0.4766, -0.6115, -0.2710]],\n","       grad_fn=<AddmmBackward0>)\n","labels:tensor([9])\n","loss= tensor(961.1148, grad_fn=<NllLossBackward0>)\n"]}],"source":["# model.train(False)\n","# batches = 0\n","# avg_loss = 0\n","# for step, (features, labels) in enumerate(train_loader):\n","#     # optimizer.zero_grad()\n","#     print(\"step=\", step)\n","#     preds = model(features)\n","#     print(f\"preds:{preds}\\nlabels:{labels}\")\n","#     loss = loss_func(preds, labels)\n","#     # loss.backward()\n","#     # optimizer.step()\n","#     print(\"loss=\", loss)\n","\n","# model.train(True)\n","# batches = 0\n","# avg_loss = 0\n","# for step, (features, labels) in enumerate(train_loader):\n","#     optimizer.zero_grad()\n","#     preds = model(features)\n","#     print(f\"preds:{preds}\\nlabels:{labels}\")\n","#     loss = loss_func(preds, labels)\n","#     loss.backward()\n","#     optimizer.step()\n","    \n","#     print(f\"Loss on step {step}: {loss}\")\n"]},{"cell_type":"code","execution_count":71,"metadata":{},"outputs":[],"source":["\n","\n","def train_one_epoch():\n","    model.train(True)\n","    batches = 0\n","    avg_loss = 0\n","    for step, (features, labels) in enumerate(train_loader):\n","        optimizer.zero_grad()\n","        preds = model(features)\n","        print(f\"preds:{preds}\\nlabels:{labels}\")\n","        loss = loss_func(preds, labels)\n","        loss.backward()\n","        optimizer.step()\n","        \n","        print(f\"Loss on step {step}: {loss}\")\n","\n","        avg_loss += loss\n","        batches = step + 1\n","    \n","    avg_loss = avg_loss / batches\n","    print(f\"Average loss for training batches in this epoch: {avg_loss}\")\n","\n","    model.train(False)\n","    batches = 0\n","    avg_loss = 0\n","    for step, (features, labels) in enumerate(val_loader):\n","        preds = model(features)\n","        print(f\"preds:{preds}\\nlabels:{labels}\")\n","        loss = loss_func(preds, labels)\n","        \n","        print(f\"Loss on step {step}: {loss}\")\n","\n","\n","        avg_loss += loss\n","        batches = step + 1\n","\n","    avg_loss = avg_loss / batches\n","    print(f\"Average loss for validation batches in this epoch: {avg_loss}\")\n"]},{"cell_type":"code","execution_count":72,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["preds:tensor([[ 0.3106,  0.7455,  0.0710,  ..., -0.4766, -0.6115, -0.2710]],\n","       grad_fn=<AddmmBackward0>)\n","labels:tensor([9])\n","Loss on step 0: 961.1148071289062\n","Average loss for training batches in this epoch: 961.1148071289062\n","preds:tensor([[ 0.4711,  0.7358,  0.1544,  ..., -0.4126, -0.5657, -0.2135]],\n","       grad_fn=<AddmmBackward0>)\n","labels:tensor([1037])\n","Loss on step 0: 2874.47607421875\n","Average loss for validation batches in this epoch: 2874.47607421875\n"]}],"source":["train_one_epoch()\n","# train_one_epoch()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
