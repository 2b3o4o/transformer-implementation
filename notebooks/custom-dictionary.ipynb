{"cells":[{"cell_type":"markdown","metadata":{},"source":["Goal for this notebook is to create a custom dictionary, and tools to quickly create a custom dictionary from any text. I'll use shakespeare to start since I already have that text available."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["963027\n","71927\n"]}],"source":["# Open the file in read mode ('r')\n","with open('../data/shakespeare.txt', 'r', encoding='utf-8') as file:\n","    # Read the entire file into a string variable\n","    content = file.read()\n","\n","# Split the content by spaces to get words\n","words = content.split()\n","\n","# Now the variable `words` is a list containing all the words\n","print(len(words))\n","\n","unique_words = set(words)\n","print(len(unique_words))"]},{"cell_type":"markdown","metadata":{},"source":["Shakespeare has a big vocabulary! But also, just splitting on whitespace as I did here has much worse performance than actual tokenization. For example, for many words I have multiple versions that contain punctuation, capitalization, or which are hyphenated, and these are all treated separately. I suspect this will lead to very poor performance when used with my actual model, but at this point I'm just concerned about getting a working MVP quickly so whatever. This is very low hanging fruit for improvement later though!"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["mural\n","500\n"]}],"source":["word_to_id = {word: i for i, word in enumerate(unique_words)}\n","id_to_word = {i: word for i, word in enumerate(unique_words)}\n","\n","print(id_to_word[500])\n","print(word_to_id['mural'])"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# putting it all together:\n","def build_dictionary(file_path) -> (dict, dict):\n","    with open(file_path, 'r', encoding='utf-8') as file:\n","        content = file.read()\n","    words = content.split()\n","    unique_words = set(words)\n","    word_to_id = {word: i for i, word in enumerate(unique_words)}\n","    id_to_word = {i: word for i, word in enumerate(unique_words)}\n","    return word_to_id, id_to_word"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["mural\n","500\n","71927\n","71927\n"]}],"source":["word_to_id, id_to_word = build_dictionary('../data/shakespeare.txt')\n","print(id_to_word[500])\n","print(word_to_id['mural'])\n","print(len(word_to_id))\n","print(len(id_to_word))"]},{"cell_type":"markdown","metadata":{},"source":["Cool, we can now quickly build a (crappy) dictionary from any file. Again, lots of room to improve here with an actually intelligent tokenization approach, but for now it's time to build a quick and dirty dictionary and incorporate it into my model."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
