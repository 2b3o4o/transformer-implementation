{"cells":[{"cell_type":"markdown","metadata":{},"source":["Goal for this notebook is to get an idea of how Chinchilla scaling laws apply to my model development here. I suspect this model may be too small for these laws to be fully relevant, but I'm still going to use them as a general guideline."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of tokens in training + validation data: 1383670\n"]}],"source":["import spacy\n","\n","tokenizer = spacy.load(\"en_core_web_sm\")\n","\n","files = [\n","        \"../data/_part1.txt\",\n","        \"../data/_part2.txt\",\n","        \"../data/_part3.txt\",\n","        \"../data/_part4.txt\",\n","        \"../data/_part5.txt\",\n","        \"../data/_part6.txt\",\n","        \"../data/_part7.txt\"\n","    ]\n","\n","texts = []\n","for file_name in files:\n","    with open(file_name, 'r', encoding='utf-8') as file:\n","        texts.append(file.read())\n","\n","all_tokens = []\n","all_tokens.extend(['<PAD>', '<UNK>'])\n","\n","for text in texts:\n","    doc = tokenizer(text)\n","    tokens = [token.text for token in doc]\n","    all_tokens.extend(tokens)\n","\n","print(f\"Total number of tokens in training + validation data: {len(all_tokens)}\")"]},{"cell_type":"markdown","metadata":{},"source":["Chinchilla is a 70B parameter model trained on 1.4T tokens. 1.4T/70B = 20."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["For a dataset with 1383670 tokens to be optimal in training, the model should have about 69183 parameters\n"]}],"source":["num_tokens = len(all_tokens)\n","num_parameters = num_tokens // 20\n","num_unique_tokens = len(set(all_tokens))\n","\n","print(f\"For a dataset with {num_tokens} tokens to be optimal in training, the model should have about {num_parameters} parameters\")"]},{"cell_type":"markdown","metadata":{},"source":["Let's check how many parameters I've actually been training with"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["import sys\n","sys.path.append(\"../decoder-transformer\")"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["4m0s - DEBUG - Initializing model...\n","4m1s - DEBUG - Initializing model...\n"]},{"name":"stdout","output_type":"stream","text":["small has 281494493 total parameters, of which 281494493 are trainable.\n"]},{"name":"stderr","output_type":"stream","text":["4m1s - DEBUG - Initializing model...\n"]},{"name":"stdout","output_type":"stream","text":["tiny has 69447645 total parameters, of which 69447645 are trainable.\n","micro has 8684925 total parameters, of which 8684925 are trainable.\n"]}],"source":["from model import TransformerNetwork\n","\n","def count_parameters(model):\n","    total_params = sum(p.numel() for p in model.parameters())\n","    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    return total_params, trainable_params\n","\n","model = TransformerNetwork(output_dict_size=num_unique_tokens, context_len=32, num_layers=6, model_dim=256, att_heads=8, ff_hidden_dim=1024, name=\"small\")\n","total, trainable = count_parameters(model)\n","print(f\"{model.name} has {total} total parameters, of which {trainable} are trainable.\")\n","\n","model = TransformerNetwork(output_dict_size=num_unique_tokens, context_len=16, num_layers=2, model_dim=128, att_heads=4, ff_hidden_dim=256, name=\"tiny\")\n","total, trainable = count_parameters(model)\n","print(f\"{model.name} has {total} total parameters, of which {trainable} are trainable.\")\n","\n","model = TransformerNetwork(output_dict_size=num_unique_tokens, context_len=8, num_layers=1, model_dim=32, att_heads=4, ff_hidden_dim=64, name=\"micro\")\n","total, trainable = count_parameters(model)\n","print(f\"{model.name} has {total} total parameters, of which {trainable} are trainable.\")"]},{"cell_type":"markdown","metadata":{},"source":["I did not expect even the tiniest preset to have dramatically too many parameters, but I guess the complete works of shakespeare ARE relatively small."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"}},"nbformat":4,"nbformat_minor":2}
