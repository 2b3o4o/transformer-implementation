{"cells":[{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["import torch\n","from torch import tensor, sin, cos\n","from math import sqrt\n","from torch.nn.functional import softmax\n","import spacy\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","assert torch.cuda.is_available()\n","device = torch.device(\"cuda\")"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["train_files = [\n","    \"../data/_part1.txt\",\n","    # \"../data/_part2.txt\",\n","    # \"../data/_part3.txt\",\n","    # \"../data/_part4.txt\",\n","    # \"../data/_part5.txt\",\n","    # \"../data/_part6.txt\",\n","    # \"../data/_part7.txt\"\n","]\n","test_files = [\"../data/much_ado_about_nothing_gut.txt\"]\n","\n","train_texts = []\n","test_texts = []\n","\n","for file_name in train_files:\n","    with open(file_name, 'r', encoding='utf-8') as file:\n","        train_texts.append(file.read())\n","\n","for file_name in test_files:\n","    with open(file_name, 'r', encoding='utf-8') as file:\n","        test_texts.append(file.read())"]},{"cell_type":"markdown","metadata":{},"source":["Set up our tokenizer and 3rd party embedding library"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["tokenizer = spacy.load(\"en_core_web_sm\")\n","all_tokens = []\n","all_tokens.extend(['<PAD>', '<UNK>']) # special tokens\n","\n","for text in train_texts + test_texts:\n","    doc = tokenizer(text)\n","    tokens = [token.text for token in doc]\n","    all_tokens.extend(tokens)\n","\n","unique_tokens = set(all_tokens)\n","vocab = {token: i for i, token in enumerate(unique_tokens)}\n","reverse_vocab = {i: token for i, token in enumerate(unique_tokens)}"]},{"cell_type":"markdown","metadata":{},"source":["Define key helper functions used throughout training and inference"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["def par_attention(queries: tensor, keys: tensor, values: tensor, dim: int) -> tensor:\n","    raw_weights = torch.bmm(queries, keys.transpose(1, 2))\n","\n","    mask = torch.tril(torch.ones_like(raw_weights), diagonal=0)\n","    raw_weights = raw_weights.masked_fill(mask == 0, float('-inf'))\n","    # print(f\"raw_weights.shape:{raw_weights.shape}\\nraw_weights: {raw_weights}\")\n","\n","    scale_factor = sqrt(dim)\n","    scaled_weights = softmax(raw_weights / scale_factor, dim=2)\n","    # print(f\"scaled_weights.shape:{scaled_weights.shape}\\nscaled_weights: {scaled_weights}\")\n","\n","    # now scaled weights is a matrix where each row represents the scaled weights produced based on a given query.\n","    # meanwhile values just has a value vector on each row.\n","\n","    reshaped_scaled_weights = scaled_weights.view(scaled_weights.shape[0], scaled_weights.shape[1], scaled_weights.shape[2], 1)\n","    reshaped_values = values.view(values.shape[0], values.shape[1], 1, values.shape[2])\n","\n","    scaled_values = reshaped_scaled_weights * reshaped_values\n","\n","    contextualized_values = torch.sum(scaled_values, 2)\n","    return contextualized_values\n","\n","def prep_input_string(str, context_len) -> tensor:\n","    \"\"\"Takes an input string with up to context_len tokens and returns a tensor full of integers, which can be passed into the model\"\"\"\n","    tokens = tokenizer(str)\n","\n","    output = torch.full([context_len], vocab['<PAD>'])\n","    for in_pos in range(len(tokens)):\n","        out_pos = context_len - len(tokens) + in_pos\n","        output[out_pos] = vocab[tokens[in_pos].text]\n","\n","    return output\n","\n","def prep_tokens(tokens, length) -> tensor:\n","    output = torch.full([length], vocab['<PAD>'])\n","    for in_pos in range(len(tokens)):\n","        out_pos = length - len(tokens) + in_pos\n","        output[out_pos] = vocab[tokens[in_pos].text]\n","\n","    return output\n","\n","# slice_offset is the number of tokens separating the start of one slice from the start of the previous.\n","# slice_offset == slice_length means no overlap, slice_offset == 1 means maximum overlap.\n","def slice_text(text: str, slice_length, slice_offset, context_len) -> tensor:\n","    slices = []\n","    tokens = tokenizer(text)\n","\n","    for i in range(0, len(tokens), slice_offset):\n","        slices.append(tokens[i:i+slice_length])\n","\n","    output = torch.zeros([len(slices), context_len + 1]) # use context_len + 1 because we need to include the label\n","    for i, slice in enumerate(slices):\n","        output[i] = prep_tokens(slice, context_len + 1)\n","\n","    assert output.shape[1] == context_len + 1\n","    return output.to(device)\n","\n","def slice_by_line(text: str, context_len) -> tensor:\n","    slices = text.split(\"\\n\")\n","    tokens = [tokenizer(slice) for slice in slices]\n","\n","    output = torch.zeros([len(tokens), context_len + 1])\n","    for i, token_line in enumerate(tokens):\n","        output[i] = prep_tokens(token_line, context_len + 1)\n","\n","    return output.to(device)"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, dims, context_len):\n","        super().__init__()\n","        self.dims = dims\n","        self.context_len = context_len\n","        self.proj = nn.Linear(1, self.dims)\n","\n","        positional_matrix = torch.zeros([self.context_len, self.dims])\n","        for pos in range(0, self.context_len):\n","            for i in range(0, self.dims // 2):\n","                positional_matrix[pos][2 * i] = sin(torch.tensor(pos / (10000 ** (2 * i / self.dims))))\n","                positional_matrix[pos][2 * i + 1] = cos(torch.tensor(pos / (10000 ** (2 * i / self.dims))))\n","        positional_matrix = positional_matrix.to(device)\n","        self.register_buffer('positional_matrix', positional_matrix)\n","        self.positional_matrix = self.positional_matrix.to(device)\n","\n","\n","    def forward(self, x: tensor) -> tensor:\n","        # x is token ids. we'll say it's context_len integers packed into a tensor, where each one represents a token. it can also be batched.\n","        output = torch.zeros([x.shape[0], self.context_len, self.dims]).to(device)\n","        for batch in range(0, x.shape[0]):\n","            output[batch] = self.proj(x[batch].view(x.shape[1], -1))\n","            output[batch] += self.positional_matrix\n","        # print(f\"self.context_len={self.context_len}\")\n","        # print(f\"Shape of x before assert: {x.shape}\")\n","        # assert x.shape[1] == self.context_len\n","        # output = self.proj(x)\n","        # output += self.positional_matrix\n","        return output"]},{"cell_type":"markdown","metadata":{},"source":["Define the architecture of the model, including all subcomponents"]},{"cell_type":"code","execution_count":33,"metadata":{},"outputs":[],"source":["class AttentionHead(nn.Module):\n","    # For simplicity, I assume query, key, and value vectors have the same dimensionality\n","    def __init__(self, model_dim, vectors_dim):\n","        super().__init__()\n","        self.model_dim = model_dim\n","        self.vectors_dim = vectors_dim\n","        self.Q_proj = nn.Linear(model_dim, vectors_dim, bias=False)\n","        self.K_proj = nn.Linear(model_dim, vectors_dim, bias=False)\n","        self.V_proj = nn.Linear(model_dim, vectors_dim, bias=False)\n","\n","    def forward(self, x):\n","        # each row of x is a vector representing the meaning of the token at the corresponding position with whatever context we've attained so far.\n","        Q = self.Q_proj(x)\n","        K = self.K_proj(x)\n","        V = self.V_proj(x)\n","        # print(\"Shape of Q matrix: \", Q.shape)\n","        # print(\"Shape of K matrix: \", K.shape)\n","        # print(\"Shape of V matrix: \", V.shape)\n","        output = par_attention(Q, K, V, self.vectors_dim)\n","        return output\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, model_dim, num_heads):\n","        super().__init__()\n","        self.att_heads = nn.ModuleList([AttentionHead(model_dim, model_dim // num_heads) for _ in range(num_heads)])\n","        self.proj = nn.Linear(model_dim, model_dim, bias=False)\n","\n","    def forward(self, x):\n","        head_outputs = [head(x) for head in self.att_heads]\n","        x = torch.concat(head_outputs, dim=2)\n","        x = self.proj(x)\n","        return x\n","        \n","class TransformerLayer(nn.Module):\n","    def __init__(self, model_dim, num_heads, ff_hidden_dim, context_len):\n","        super().__init__()\n","        self.attention_block = MultiHeadAttention(model_dim, num_heads)\n","        self.norm1 = nn.LayerNorm(normalized_shape=[context_len, model_dim])\n","        self.ff1 = nn.Linear(model_dim, ff_hidden_dim)\n","        self.ff_relu = nn.ReLU()\n","        self.ff2 = nn.Linear(ff_hidden_dim, model_dim)\n","        self.norm2 = nn.LayerNorm(normalized_shape=[context_len, model_dim])\n","\n","    def forward(self, x):\n","        x_res = x\n","        x = self.attention_block(x)\n","        x += x_res\n","        x = self.norm1(x)\n","\n","        x_res = x\n","        x = self.ff1(x)\n","        x = self.ff_relu(x)\n","        x = self.ff2(x)\n","        x += x_res\n","        x = self.norm2(x)\n","\n","        return x\n","\n","class TransformerNetwork(nn.Module):\n","    def __init__(self, num_layers, model_dim, att_heads, ff_hidden_dim, context_len, output_dict_size):\n","        super().__init__()\n","        self.encode_embed = PositionalEncoding(model_dim, context_len)\n","        self.trans_layers = nn.ModuleList([TransformerLayer(model_dim, att_heads, ff_hidden_dim, context_len) for _ in range(num_layers)])\n","\n","        # self.test_ff = nn.Linear(model_dim * context_len, 2048)\n","        # self.test_relu = nn.ReLU()\n","        # self.test_ff2 = nn.Linear(2048, model_dim * context_len)\n","        # self.test_relu2 = nn.ReLU()\n","\n","        self.word_predictor = nn.Linear(model_dim * context_len, output_dict_size)\n","        # print(f\"word_predictor input dimension: {model_dim * context_len}\\noutput dimension: {output_dict_size}\")\n","\n","    def forward(self, x):\n","        # print(f\"Received x of shape: {x.shape}\")\n","        x = self.encode_embed(x)\n","        for layer in self.trans_layers:\n","            x = layer.forward(x)\n","        x = x.view(x.shape[0], -1)\n","        # print(f\"Reshaped x to shape: {x.shape}\")\n","\n","        # x = self.test_ff(x)\n","        # x = self.test_relu(x)\n","        # x = self.test_ff2(x)\n","        # x = self.test_relu2(x)\n","\n","        x = self.word_predictor(x)\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["Tools to quickly build a dataset that can be fed into the model"]},{"cell_type":"code","execution_count":34,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset\n","\n","class CompletionDataset(Dataset):\n","    def __init__(self, features, labels):\n","        self.features = features\n","        self.labels = labels.long()\n","\n","    def __len__(self):\n","        return self.features.shape[0]\n","\n","    def __getitem__(self, index):\n","        return self.features[index], self.labels[index]\n","\n","# Note: slices include features + label. So if you have context length 256, you can set slice length 257 and be fine.\n","def build_dataset(slices: tensor) -> CompletionDataset:\n","    features = slices[:, :-1]\n","    labels = slices[:, -1]\n","    \n","    dataset = CompletionDataset(features, labels)\n","    return dataset\n","\n","context_len = 16\n","slice_length = context_len + 1\n","slice_offset = slice_length\n","\n","# note: slice_text returns an n by slice_length tensor of ints. (from vocab)\n","train_slices = [] # list of tensors\n","for text in train_texts:\n","    # train_slices.append(slice_by_line(text, context_len))\n","    train_slices.append(slice_text(text, slice_length, slice_offset, context_len))\n","    train_slices.append(slice_text(text, slice_length - 2, 1, context_len))\n","    train_slices.append(slice_text(text, 5, 1, context_len))\n","train_dataset = build_dataset(torch.cat(train_slices, dim=0))\n","\n","test_slices = [] # list of tensors\n","for text in test_texts:\n","    # test_slices.append(slice_by_line(text, context_len))\n","    test_slices.append(slice_text(text, slice_length - 3, 1, context_len))\n","test_dataset = build_dataset(torch.cat(test_slices, dim=0))"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Features:\n","['<PAD>', '<PAD>', '<PAD>', 'MUCH', 'ADO', 'ABOUT', 'NOTHING', '\\n\\n', 'by', 'William', 'Shakespeare', '\\n\\n\\n\\n\\n', 'DRAMATIS', 'PERSONAE', '\\n\\n', 'DON']\n","Label:\n","PEDRO\n","Features:\n","['The', 'Complete', 'Works', 'of', 'William', 'Shakespeare', '\\n\\n', 'by', 'William', 'Shakespeare', '\\n\\n\\n\\n\\n                    ', 'Contents', '\\n\\n    ', 'THE', 'SONNETS', '\\n    ']\n","Label:\n","ALL\n"]},{"data":{"text/plain":["32167"]},"execution_count":35,"metadata":{},"output_type":"execute_result"}],"source":["def check_input_data(input):\n","    features = input[0].int().tolist()\n","    label = input[1].int().item()\n","    features_str = [reverse_vocab[f] for f in features]\n","    label_str = reverse_vocab[label]\n","    print(f\"Features:\\n{features_str}\")\n","    print(f\"Label:\\n{label_str}\")\n","\n","check_input_data(test_dataset[0])\n","check_input_data(train_dataset[0])\n","len(test_dataset)"]},{"cell_type":"markdown","metadata":{},"source":["Initialize model. Output dict size is the size of the final layer."]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["dictionary_len: 13075\n"]}],"source":["dictionary_len = len(vocab)\n","model = TransformerNetwork(num_layers=4, model_dim=256, att_heads=4, ff_hidden_dim=1024, context_len=context_len, output_dict_size=dictionary_len)\n","model.to(device)\n","print(f\"dictionary_len: {dictionary_len}\")\n","\n","loss_func = torch.nn.CrossEntropyLoss()\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)"]},{"cell_type":"code","execution_count":37,"metadata":{},"outputs":[],"source":["def train_one_epoch(do_validation: bool):\n","    model.train(True)\n","    torch.set_printoptions(profile=\"short\")\n","    batches = 0\n","    avg_loss = 0\n","    for step, (features, labels) in enumerate(train_loader):\n","        features, labels = features.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        preds = model(features)\n","        # print(f\"preds:{preds}\\nlabels:{labels}\")\n","        loss = loss_func(preds, labels)\n","        loss.backward()\n","\n","        # if step % 10 == 0:  # Print every 10 batches\n","        #     for name, param in model.named_parameters():\n","        #         if param.requires_grad:\n","        #             print(f\"Gradient data for {name}:\", param.grad)\n","        #             print(f\"Checking if gradients are fully zeroed: {torch.all(param.grad == 0.0).item()}\")\n","        #             print(f\"Shape: {param.grad.shape}\")\n","        #             print(f\"Mean: {param.grad.mean()}\")\n","        #             print(f\"Std: {param.grad.std()}\")\n","        #             print(f\"Min: {param.grad.min()}\")\n","        #             print(f\"Max: {param.grad.max()}\")\n","\n","        optimizer.step()\n","\n","        # if step % 20 == 0:\n","        #     print(f\"Loss on batch {step}: {loss}\")\n","\n","        avg_loss += loss\n","        batches = step + 1\n","    \n","    avg_loss = avg_loss / batches\n","    print(f\"Average loss for training batches in this epoch: {avg_loss}\")\n","\n","    if do_validation:\n","        model.train(False)\n","        batches = 0\n","        avg_loss = 0\n","        for step, (features, labels) in enumerate(test_loader):\n","            features, labels = features.to(device), labels.to(device)\n","            preds = model(features)\n","            # print(f\"preds:{preds}\\nlabels:{labels}\")\n","            loss = loss_func(preds, labels)\n","            \n","            # if step % 20 == 0:\n","            #     print(f\"Loss on batch {step}: {loss}\")\n","\n","            avg_loss += loss\n","            batches = step + 1\n","\n","        avg_loss = avg_loss / batches\n","        print(f\"Average loss for validation batches in this epoch: {avg_loss}\")\n"]},{"cell_type":"code","execution_count":38,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0\n","Average loss for training batches in this epoch: 6.347843170166016\n","Average loss for validation batches in this epoch: 6.881432056427002\n","Epoch 1\n","Average loss for training batches in this epoch: 6.25460147857666\n","Average loss for validation batches in this epoch: 7.114598274230957\n","Epoch 2\n","Average loss for training batches in this epoch: 6.181738376617432\n","Average loss for validation batches in this epoch: 7.053977012634277\n","Epoch 3\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\src\\alignment-study\\transformer-implementation\\notebooks\\train-4.ipynb Cell 17\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-4.ipynb#X22sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-4.ipynb#X22sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch\u001b[39m\u001b[39m\"\u001b[39m, i)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-4.ipynb#X22sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     train_one_epoch(\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-4.ipynb#X22sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# train_one_epoch()\u001b[39;00m\n","\u001b[1;32mc:\\src\\alignment-study\\transformer-implementation\\notebooks\\train-4.ipynb Cell 17\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-4.ipynb#X22sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m features, labels \u001b[39m=\u001b[39m features\u001b[39m.\u001b[39mto(device), labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-4.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-4.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m preds \u001b[39m=\u001b[39m model(features)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-4.ipynb#X22sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# print(f\"preds:{preds}\\nlabels:{labels}\")\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-4.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_func(preds, labels)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","\u001b[1;32mc:\\src\\alignment-study\\transformer-implementation\\notebooks\\train-4.ipynb Cell 17\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-4.ipynb#X22sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-4.ipynb#X22sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m     \u001b[39m# print(f\"Received x of shape: {x.shape}\")\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-4.ipynb#X22sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode_embed(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-4.ipynb#X22sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrans_layers:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-4.ipynb#X22sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m         x \u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mforward(x)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n","\u001b[1;32mc:\\src\\alignment-study\\transformer-implementation\\notebooks\\train-4.ipynb Cell 17\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-4.ipynb#X22sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m tensor:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-4.ipynb#X22sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39m# x is token ids. we'll say it's context_len integers packed into a tensor, where each one represents a token. it can also be batched.\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-4.ipynb#X22sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mzeros([x\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcontext_len, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdims])\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-4.ipynb#X22sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-4.ipynb#X22sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         output[batch] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj(x[batch]\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n","\u001b[1;31mKeyboardInterrupt\u001b[0m: "]}],"source":["for i in range(10):\n","    print(\"Epoch\", i)\n","    train_one_epoch(True)\n","# train_one_epoch()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy on test set: 10.240308390586627%\n"]}],"source":["model.eval()  # Set the model to evaluation mode\n","correct = 0\n","total = 0\n","\n","with torch.no_grad():  # Deactivates autograd, reduces memory usage and speeds up computations\n","    for features, labels in test_loader:\n","        outputs = model(features)\n","        _, predicted = torch.max(outputs.data, 1)  # Get the index of the max log-probability\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = 100 * correct / total\n","print(f\"Accuracy on test set: {accuracy}%\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def infer_completion(input_text: str, context_len):\n","    encoded_input = prep_input_string(input_text, context_len).unsqueeze(0).float().to(device)\n","    \n","    model.train(False)\n","    pred = model(encoded_input)\n","    return reverse_vocab[torch.argmax(softmax(pred, dim=1), dim=1).item()]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["','"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["infer_completion(\"From fairest creatures we desire\", context_len)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
