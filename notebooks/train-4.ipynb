{"cells":[{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[],"source":["import torch\n","from torch import tensor, sin, cos\n","from math import sqrt\n","from torch.nn.functional import softmax\n","import spacy\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[],"source":["# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","assert torch.cuda.is_available()\n","device = torch.device(\"cuda\")"]},{"cell_type":"code","execution_count":48,"metadata":{},"outputs":[],"source":["train_files = [\n","    \"../data/_part1.txt\",\n","    # \"../data/_part2.txt\",\n","    # \"../data/_part3.txt\",\n","    # \"../data/_part4.txt\",\n","    # \"../data/_part5.txt\",\n","    # \"../data/_part6.txt\",\n","    # \"../data/_part7.txt\"\n","]\n","test_files = [\"../data/much_ado_about_nothing_gut.txt\"]\n","\n","train_texts = []\n","test_texts = []\n","\n","for file_name in train_files:\n","    with open(file_name, 'r', encoding='utf-8') as file:\n","        train_texts.append(file.read())\n","\n","for file_name in test_files:\n","    with open(file_name, 'r', encoding='utf-8') as file:\n","        test_texts.append(file.read())"]},{"cell_type":"markdown","metadata":{},"source":["Set up our tokenizer and 3rd party embedding library"]},{"cell_type":"code","execution_count":49,"metadata":{},"outputs":[],"source":["tokenizer = spacy.load(\"en_core_web_sm\")\n","all_tokens = []\n","all_tokens.extend(['<PAD>', '<UNK>']) # special tokens\n","\n","for text in train_texts + test_texts:\n","    doc = tokenizer(text)\n","    tokens = [token.text for token in doc]\n","    all_tokens.extend(tokens)\n","\n","unique_tokens = set(all_tokens)\n","vocab = {token: i for i, token in enumerate(unique_tokens)}\n","reverse_vocab = {i: token for i, token in enumerate(unique_tokens)}"]},{"cell_type":"markdown","metadata":{},"source":["Define key helper functions used throughout training and inference"]},{"cell_type":"code","execution_count":50,"metadata":{},"outputs":[],"source":["def par_attention(queries: tensor, keys: tensor, values: tensor, dim: int) -> tensor:\n","    raw_weights = torch.bmm(queries, keys.transpose(1, 2))\n","\n","    mask = torch.tril(torch.ones_like(raw_weights), diagonal=0)\n","    raw_weights = raw_weights.masked_fill(mask == 0, float('-inf'))\n","    # print(f\"raw_weights.shape:{raw_weights.shape}\\nraw_weights: {raw_weights}\")\n","\n","    scale_factor = sqrt(dim)\n","    scaled_weights = softmax(raw_weights / scale_factor, dim=2)\n","    # print(f\"scaled_weights.shape:{scaled_weights.shape}\\nscaled_weights: {scaled_weights}\")\n","\n","    # now scaled weights is a matrix where each row represents the scaled weights produced based on a given query.\n","    # meanwhile values just has a value vector on each row.\n","\n","    reshaped_scaled_weights = scaled_weights.view(scaled_weights.shape[0], scaled_weights.shape[1], scaled_weights.shape[2], 1)\n","    reshaped_values = values.view(values.shape[0], values.shape[1], 1, values.shape[2])\n","\n","    scaled_values = reshaped_scaled_weights * reshaped_values\n","\n","    contextualized_values = torch.sum(scaled_values, 2)\n","    return contextualized_values\n","\n","def prep_input_string(str, context_len) -> tensor:\n","    \"\"\"Takes an input string with up to context_len tokens and returns a tensor full of integers, which can be passed into the model\"\"\"\n","    tokens = tokenizer(str)\n","\n","    output = torch.full([context_len], vocab['<PAD>'])\n","    for in_pos in range(len(tokens)):\n","        out_pos = context_len - len(tokens) + in_pos\n","        output[out_pos] = vocab[tokens[in_pos].text]\n","\n","    return output\n","\n","def prep_tokens(tokens, length) -> tensor:\n","    output = torch.full([length], vocab['<PAD>'])\n","    for in_pos in range(len(tokens)):\n","        out_pos = length - len(tokens) + in_pos\n","        output[out_pos] = vocab[tokens[in_pos].text]\n","\n","    return output\n","\n","# slice_offset is the number of tokens separating the start of one slice from the start of the previous.\n","# slice_offset == slice_length means no overlap, slice_offset == 1 means maximum overlap.\n","def slice_text(text: str, slice_length, slice_offset, context_len) -> tensor:\n","    slices = []\n","    tokens = tokenizer(text)\n","\n","    for i in range(0, len(tokens), slice_offset):\n","        slices.append(tokens[i:i+slice_length])\n","\n","    output = torch.zeros([len(slices), context_len + 1]) # use context_len + 1 because we need to include the label\n","    for i, slice in enumerate(slices):\n","        output[i] = prep_tokens(slice, context_len + 1)\n","\n","    assert output.shape[1] == context_len + 1\n","    return output.to(device)\n","\n","def slice_by_line(text: str, context_len) -> tensor:\n","    slices = text.split(\"\\n\")\n","    tokens = [tokenizer(slice) for slice in slices]\n","\n","    output = torch.zeros([len(tokens), context_len + 1])\n","    for i, token_line in enumerate(tokens):\n","        output[i] = prep_tokens(token_line, context_len + 1)\n","\n","    return output.to(device)"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[],"source":["class PositionalEncoding(nn.Module):\n","    def __init__(self, dims, context_len):\n","        super().__init__()\n","        self.dims = dims\n","        self.context_len = context_len\n","        self.proj = nn.Linear(1, self.dims)\n","\n","        positional_matrix = torch.zeros([self.context_len, self.dims])\n","        for pos in range(0, self.context_len):\n","            for i in range(0, self.dims // 2):\n","                positional_matrix[pos][2 * i] = sin(torch.tensor(pos / (10000 ** (2 * i / self.dims))))\n","                positional_matrix[pos][2 * i + 1] = cos(torch.tensor(pos / (10000 ** (2 * i / self.dims))))\n","        positional_matrix = positional_matrix.to(device)\n","        self.register_buffer('positional_matrix', positional_matrix)\n","        self.positional_matrix = self.positional_matrix.to(device)\n","\n","\n","    def forward(self, x: tensor) -> tensor:\n","        # x is token ids. we'll say it's context_len integers packed into a tensor, where each one represents a token. it can also be batched.\n","        output = torch.zeros([x.shape[0], self.context_len, self.dims]).to(device)\n","        for batch in range(0, x.shape[0]):\n","            output[batch] = self.proj(x[batch].view(x.shape[1], -1))\n","            output[batch] += self.positional_matrix\n","        # print(f\"self.context_len={self.context_len}\")\n","        # print(f\"Shape of x before assert: {x.shape}\")\n","        # assert x.shape[1] == self.context_len\n","        # output = self.proj(x)\n","        # output += self.positional_matrix\n","        return output"]},{"cell_type":"markdown","metadata":{},"source":["Define the architecture of the model, including all subcomponents"]},{"cell_type":"code","execution_count":52,"metadata":{},"outputs":[],"source":["class AttentionHead(nn.Module):\n","    # For simplicity, I assume query, key, and value vectors have the same dimensionality\n","    def __init__(self, model_dim, vectors_dim):\n","        super().__init__()\n","        self.model_dim = model_dim\n","        self.vectors_dim = vectors_dim\n","        self.Q_proj = nn.Linear(model_dim, vectors_dim, bias=False)\n","        self.K_proj = nn.Linear(model_dim, vectors_dim, bias=False)\n","        self.V_proj = nn.Linear(model_dim, vectors_dim, bias=False)\n","\n","    def forward(self, x):\n","        # each row of x is a vector representing the meaning of the token at the corresponding position with whatever context we've attained so far.\n","        Q = self.Q_proj(x)\n","        K = self.K_proj(x)\n","        V = self.V_proj(x)\n","        # print(\"Shape of Q matrix: \", Q.shape)\n","        # print(\"Shape of K matrix: \", K.shape)\n","        # print(\"Shape of V matrix: \", V.shape)\n","        output = par_attention(Q, K, V, self.vectors_dim)\n","        return output\n","\n","class MultiHeadAttention(nn.Module):\n","    def __init__(self, model_dim, num_heads):\n","        super().__init__()\n","        self.att_heads = nn.ModuleList([AttentionHead(model_dim, model_dim // num_heads) for _ in range(num_heads)])\n","        self.proj = nn.Linear(model_dim, model_dim, bias=False)\n","\n","    def forward(self, x):\n","        head_outputs = [head(x) for head in self.att_heads]\n","        x = torch.concat(head_outputs, dim=2)\n","        x = self.proj(x)\n","        return x\n","        \n","class TransformerLayer(nn.Module):\n","    def __init__(self, model_dim, num_heads, ff_hidden_dim, context_len):\n","        super().__init__()\n","        self.attention_block = MultiHeadAttention(model_dim, num_heads)\n","        self.norm1 = nn.LayerNorm(normalized_shape=[context_len, model_dim])\n","        self.ff1 = nn.Linear(model_dim, ff_hidden_dim)\n","        self.ff_relu = nn.ReLU()\n","        self.ff2 = nn.Linear(ff_hidden_dim, model_dim)\n","        self.norm2 = nn.LayerNorm(normalized_shape=[context_len, model_dim])\n","\n","    def forward(self, x):\n","        x_res = x\n","        x = self.attention_block(x)\n","        x += x_res\n","        x = self.norm1(x)\n","\n","        x_res = x\n","        x = self.ff1(x)\n","        x = self.ff_relu(x)\n","        x = self.ff2(x)\n","        x += x_res\n","        x = self.norm2(x)\n","\n","        return x\n","\n","class TransformerNetwork(nn.Module):\n","    def __init__(self, num_layers, model_dim, att_heads, ff_hidden_dim, context_len, output_dict_size):\n","        super().__init__()\n","        self.encode_embed = PositionalEncoding(model_dim, context_len)\n","        self.trans_layers = nn.ModuleList([TransformerLayer(model_dim, att_heads, ff_hidden_dim, context_len) for _ in range(num_layers)])\n","\n","        # self.test_ff = nn.Linear(model_dim * context_len, 2048)\n","        # self.test_relu = nn.ReLU()\n","        # self.test_ff2 = nn.Linear(2048, model_dim * context_len)\n","        # self.test_relu2 = nn.ReLU()\n","\n","        self.word_predictor = nn.Linear(model_dim * context_len, output_dict_size)\n","        # print(f\"word_predictor input dimension: {model_dim * context_len}\\noutput dimension: {output_dict_size}\")\n","\n","    def forward(self, x):\n","        # print(f\"Received x of shape: {x.shape}\")\n","        x = self.encode_embed(x)\n","        for layer in self.trans_layers:\n","            x = layer.forward(x)\n","        x = x.view(x.shape[0], -1)\n","        # print(f\"Reshaped x to shape: {x.shape}\")\n","\n","        # x = self.test_ff(x)\n","        # x = self.test_relu(x)\n","        # x = self.test_ff2(x)\n","        # x = self.test_relu2(x)\n","\n","        x = self.word_predictor(x)\n","        return x"]},{"cell_type":"markdown","metadata":{},"source":["Tools to quickly build a dataset that can be fed into the model"]},{"cell_type":"code","execution_count":53,"metadata":{},"outputs":[],"source":["from torch.utils.data import Dataset\n","\n","class CompletionDataset(Dataset):\n","    def __init__(self, features, labels):\n","        self.features = features\n","        self.labels = labels.long()\n","\n","    def __len__(self):\n","        return self.features.shape[0]\n","\n","    def __getitem__(self, index):\n","        return self.features[index], self.labels[index]\n","\n","# Note: slices include features + label. So if you have context length 256, you can set slice length 257 and be fine.\n","def build_dataset(slices: tensor) -> CompletionDataset:    \n","    features = slices[:, :-1]\n","    labels = slices[:, -1]\n","    \n","    dataset = CompletionDataset(features, labels)\n","    return dataset\n","\n","context_len = 16\n","slice_length = context_len + 1\n","slice_offset = slice_length\n","\n","# note: slice_text returns an n by slice_length tensor of ints. (from vocab)\n","train_slices = [] # list of tensors\n","for text in train_texts:\n","    # train_slices.append(slice_by_line(text, context_len))\n","    train_slices.append(slice_text(text, slice_length, slice_offset, context_len))\n","    train_slices.append(slice_text(text, slice_length - 2, 1, context_len))\n","    train_slices.append(slice_text(text, 5, 1, context_len))\n","train_dataset = build_dataset(torch.cat(train_slices, dim=0))\n","\n","test_slices = [] # list of tensors\n","for text in test_texts:\n","    # test_slices.append(slice_by_line(text, context_len))\n","    test_slices.append(slice_text(text, slice_length - 3, 1, context_len))\n","test_dataset = build_dataset(torch.cat(test_slices, dim=0))"]},{"cell_type":"code","execution_count":54,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Features:\n","['<PAD>', '<PAD>', '<PAD>', 'MUCH', 'ADO', 'ABOUT', 'NOTHING', '\\n\\n', 'by', 'William', 'Shakespeare', '\\n\\n\\n\\n\\n', 'DRAMATIS', 'PERSONAE', '\\n\\n', 'DON']\n","Label:\n","PEDRO\n","Features:\n","['The', 'Complete', 'Works', 'of', 'William', 'Shakespeare', '\\n\\n', 'by', 'William', 'Shakespeare', '\\n\\n\\n\\n\\n                    ', 'Contents', '\\n\\n    ', 'THE', 'SONNETS', '\\n    ']\n","Label:\n","ALL\n"]},{"data":{"text/plain":["32167"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["def check_input_data(input):\n","    features = input[0].int().tolist()\n","    label = input[1].int().item()\n","    features_str = [reverse_vocab[f] for f in features]\n","    label_str = reverse_vocab[label]\n","    print(f\"Features:\\n{features_str}\")\n","    print(f\"Label:\\n{label_str}\")\n","\n","check_input_data(test_dataset[0])\n","check_input_data(train_dataset[0])\n","len(test_dataset)"]},{"cell_type":"markdown","metadata":{},"source":["Initialize model. Output dict size is the size of the final layer."]},{"cell_type":"code","execution_count":55,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["dictionary_len: 13075\n"]}],"source":["dictionary_len = len(vocab)\n","model = TransformerNetwork(num_layers=4, model_dim=256, att_heads=4, ff_hidden_dim=1024, context_len=context_len, output_dict_size=dictionary_len)\n","model.to(device)\n","print(f\"dictionary_len: {dictionary_len}\")\n","\n","loss_func = torch.nn.CrossEntropyLoss()\n","# optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n","optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)\n","test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False)"]},{"cell_type":"code","execution_count":56,"metadata":{},"outputs":[],"source":["def train_one_epoch(do_validation: bool):\n","    model.train(True)\n","    torch.set_printoptions(profile=\"short\")\n","    batches = 0\n","    avg_loss = 0\n","    for step, (features, labels) in enumerate(train_loader):\n","        features, labels = features.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        preds = model(features)\n","        # print(f\"preds:{preds}\\nlabels:{labels}\")\n","        loss = loss_func(preds, labels)\n","        loss.backward()\n","\n","        # if step % 10 == 0:  # Print every 10 batches\n","        #     for name, param in model.named_parameters():\n","        #         if param.requires_grad:\n","        #             print(f\"Gradient data for {name}:\", param.grad)\n","        #             print(f\"Checking if gradients are fully zeroed: {torch.all(param.grad == 0.0).item()}\")\n","        #             print(f\"Shape: {param.grad.shape}\")\n","        #             print(f\"Mean: {param.grad.mean()}\")\n","        #             print(f\"Std: {param.grad.std()}\")\n","        #             print(f\"Min: {param.grad.min()}\")\n","        #             print(f\"Max: {param.grad.max()}\")\n","\n","        optimizer.step()\n","\n","        # if step % 20 == 0:\n","        #     print(f\"Loss on batch {step}: {loss}\")\n","\n","        avg_loss += loss\n","        batches = step + 1\n","    \n","    avg_loss = avg_loss / batches\n","    print(f\"Average loss for training batches in this epoch: {avg_loss}\")\n","\n","    if do_validation:\n","        model.train(False)\n","        batches = 0\n","        avg_loss = 0\n","        for step, (features, labels) in enumerate(test_loader):\n","            features, labels = features.to(device), labels.to(device)\n","            preds = model(features)\n","            # print(f\"preds:{preds}\\nlabels:{labels}\")\n","            loss = loss_func(preds, labels)\n","            \n","            # if step % 20 == 0:\n","            #     print(f\"Loss on batch {step}: {loss}\")\n","\n","            avg_loss += loss\n","            batches = step + 1\n","\n","        avg_loss = avg_loss / batches\n","        print(f\"Average loss for validation batches in this epoch: {avg_loss}\")\n"]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 0\n","Average loss for training batches in this epoch: 6.214040756225586\n"]}],"source":["for i in range(1):\n","    print(\"Epoch\", i)\n","    train_one_epoch(False)\n","# train_one_epoch()\n"]},{"cell_type":"code","execution_count":58,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy on test set: 11.275530823514782%\n"]}],"source":["model.eval()  # Set the model to evaluation mode\n","correct = 0\n","total = 0\n","\n","with torch.no_grad():  # Deactivates autograd, reduces memory usage and speeds up computations\n","    for features, labels in test_loader:\n","        outputs = model(features)\n","        _, predicted = torch.max(outputs.data, 1)  # Get the index of the max log-probability\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","accuracy = 100 * correct / total\n","print(f\"Accuracy on test set: {accuracy}%\")\n"]},{"cell_type":"code","execution_count":59,"metadata":{},"outputs":[],"source":["def infer_completion(input_text: str, context_len):\n","    encoded_input = prep_input_string(input_text, context_len).unsqueeze(0).float()\n","    \n","    model.train(False)\n","    pred = model(encoded_input)\n","    return reverse_vocab[torch.argmax(softmax(pred, dim=1), dim=1).item()]"]},{"cell_type":"code","execution_count":60,"metadata":{},"outputs":[{"ename":"KeyError","evalue":"'+'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[1;32mc:\\src\\alignment-study\\transformer-implementation\\notebooks\\train-4.ipynb Cell 20\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-4.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m infer_completion(\u001b[39m\"\u001b[39;49m\u001b[39m1 + 1 =\u001b[39;49m\u001b[39m\"\u001b[39;49m, context_len)\n","\u001b[1;32mc:\\src\\alignment-study\\transformer-implementation\\notebooks\\train-4.ipynb Cell 20\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-4.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minfer_completion\u001b[39m(input_text: \u001b[39mstr\u001b[39m, context_len):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-4.ipynb#X25sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     encoded_input \u001b[39m=\u001b[39m prep_input_string(input_text, context_len)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-4.ipynb#X25sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     model\u001b[39m.\u001b[39mtrain(\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-4.ipynb#X25sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     pred \u001b[39m=\u001b[39m model(encoded_input)\n","\u001b[1;32mc:\\src\\alignment-study\\transformer-implementation\\notebooks\\train-4.ipynb Cell 20\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-4.ipynb#X25sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m in_pos \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(tokens)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-4.ipynb#X25sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     out_pos \u001b[39m=\u001b[39m context_len \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(tokens) \u001b[39m+\u001b[39m in_pos\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-4.ipynb#X25sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     output[out_pos] \u001b[39m=\u001b[39m vocab[tokens[in_pos]\u001b[39m.\u001b[39;49mtext]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/src/alignment-study/transformer-implementation/notebooks/train-4.ipynb#X25sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m \u001b[39mreturn\u001b[39;00m output\n","\u001b[1;31mKeyError\u001b[0m: '+'"]}],"source":["infer_completion(\"\", context_len)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
