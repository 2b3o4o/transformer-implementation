{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from torchtext.vocab import GloVe\n","import torch"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["# Load GloVe embeddings with 100-dimensional vectors\n","glove = GloVe(dim=300)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from torch import sin, cos\n","\n","def positional_embedding(word, pos):\n","    model_dims = 300\n","\n","    positional_encoding = torch.tensor([0.0] * model_dims)\n","    for i in range(0, model_dims // 2):\n","        positional_encoding[2 * i] = sin(torch.tensor(pos / (10000 ** (2 * i / model_dims))))\n","        positional_encoding[2 * i + 1] = cos(torch.tensor(pos / (10000 ** (2 * i / model_dims))))\n","\n","    embedding = glove[word]\n","    embedding += positional_encoding\n","    return embedding\n"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-0.029923180118203163\n","0.8881815075874329\n"]}],"source":["print(torch.nn.functional.cosine_similarity(glove[\"puppy\"].unsqueeze(0), glove[\"stochastic\"].unsqueeze(0)).item())\n","print(torch.nn.functional.cosine_similarity(glove[\"puppy\"].unsqueeze(0), glove[\"puppies\"].unsqueeze(0)).item())\n"]},{"cell_type":"markdown","metadata":{},"source":["There's not that much I can meaningfully test right now for the positional embedding function, but experimenting with the above shows roughly what I'd expect from the standard embedding function. I'll call this done for now."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
