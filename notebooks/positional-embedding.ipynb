{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from torchtext.vocab import GloVe\n","import torch\n","\n","glove = GloVe(dim=300)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from torch import sin, cos\n","\n","def positional_embedding(word, pos):\n","    model_dims = 300\n","\n","    positional_encoding = torch.tensor([0.0] * model_dims)\n","    for i in range(0, model_dims // 2):\n","        positional_encoding[2 * i] = sin(torch.tensor(pos / (10000 ** (2 * i / model_dims))))\n","        positional_encoding[2 * i + 1] = cos(torch.tensor(pos / (10000 ** (2 * i / model_dims))))\n","\n","    embedding = glove[word]\n","    embedding += positional_encoding\n","    return embedding\n"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["-0.029923180118203163\n","0.8881815075874329\n"]}],"source":["print(torch.nn.functional.cosine_similarity(glove[\"puppy\"].unsqueeze(0), glove[\"stochastic\"].unsqueeze(0)).item())\n","print(torch.nn.functional.cosine_similarity(glove[\"puppy\"].unsqueeze(0), glove[\"puppies\"].unsqueeze(0)).item())\n"]},{"cell_type":"markdown","metadata":{},"source":["There's not that much I can meaningfully test right now for the positional embedding function, but experimenting with the above shows roughly what I'd expect from the standard embedding function. I'll call this done for now."]},{"cell_type":"markdown","metadata":{},"source":["Update: Gonna experiment with getting a dictionary out of glove below. I need this dictionary because my model will use it to generate output probabilities for the next word."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Integer-to-string mapping\n","vocab_itos = glove.itos\n","\n","# String-to-integer mapping\n","vocab_stoi = glove.stoi\n","\n","# You can check the size of the vocabulary\n","vocab_size = len(vocab_itos)\n","\n","# Access a word by index\n","word_at_index_10 = vocab_itos[10]\n","\n","# Find index of a word\n","index_of_word = vocab_stoi['hello']\n"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/plain":["2196017"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["vocab_size"]},{"cell_type":"markdown","metadata":{},"source":["A dictionary size of over 2 million means the final layer of my model will have an equal number of parameters. I have to imagine that's going to be extremely expensive computationally. Therefore, I've decided to build a custom dictionary using shakespeare instead. Later I may implement my own custom embeddings, which is another potential solution to this problem."]},{"cell_type":"markdown","metadata":{},"source":["Slightly rethinking the approach to the above, but in the meantime I need to create a function that can encode a full block of text."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from torch import tensor\n","\n","# def encode_text(text: str) -> tensor:\n","    "]},{"cell_type":"markdown","metadata":{},"source":["So I've learned that glove doesn't have built in tokenization, and that's gonna be a huge pain. So I'm starting a new notebook with a simpler approach using a different embedding library. Goodbye forever!"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
