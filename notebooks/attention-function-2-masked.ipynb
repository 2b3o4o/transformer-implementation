{"cells":[{"cell_type":"markdown","metadata":{},"source":["In this notebook I'm updating my simpler attention function to incorporate masking, and also doing some more thorough testing of it. Later I'll need to ensure that the parallelized attention function is working and also implement masking/appropriate testing for that."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import torch\n","from torch import tensor\n","from torch.nn.functional import softmax\n","from math import sqrt"]},{"cell_type":"markdown","metadata":{},"source":["In order for masking to work, I need to know the position of the current token (query token) relative to the positions of all the key/value tokens. This information might actually be more accessible in the parallelized versions since query/key/value matrices might have the same indexing relative to the original input (meaning for example that the 5th value vector and 5th query vector are both associated with the 5th input token). I think in this case I need to manually pass in the position of the query token, as an index of the key/value vectors. So passing in position 5 would mean that the query token also has the value vector on row 5 of the values matrix, and I can therefore mask every row of the values matrix after that. In the parallelized function I think this will look more like cutting out a diagonal slice of the scaled weights matrix."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# I assume all rows have the same dimension, which is provided in dim\n","# keys, values are assumed to be structured such that each key/value is a row.\n","# Of course, there must also be an equal number of keys and values, so those matrices must have equal dimensions\n","from numpy import Infinity\n","\n","\n","def attention(query: tensor, keys: tensor, values: tensor, dim: int, query_pos: int) -> tensor:\n","    # query = query.view(1, -1)\n","    raw_weights = query @ keys.T\n","\n","    # Masking:\n","    for i in range(query_pos + 1, len(raw_weights)):\n","        raw_weights[i] = -1 * Infinity\n","\n","    scale_factor = sqrt(dim)\n","    scaled_weights = softmax(raw_weights / scale_factor, dim=0)\n","\n","    scaled_values = scaled_weights.view(-1, 1) * values\n","    contextualized_value = torch.sum(scaled_values, 0)\n","\n","    return contextualized_value\n"]},{"cell_type":"markdown","metadata":{},"source":["Simple test case that I did by hand. Expected result is approximately [3, 2], which is in fact what we see!"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([2.9997, 2.0003])\n","tensor([0., 5.])\n"]}],"source":["Q = tensor([1, 3]).float()\n","K = tensor([[2, 1], [3, 5]]).float()\n","V = tensor([[0, 5], [3, 2]]).float()\n","d = 2\n","print(attention(Q, K, V, d, 1))\n","print(attention(Q, K, V, d, 0))"]},{"cell_type":"markdown","metadata":{},"source":["Now I'll do the same thing with my parallel attention function, although I won't worry about adding masking quite yet."]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["def par_attention(queries: tensor, keys: tensor, values: tensor, dim: int) -> tensor:\n","    raw_weights = queries @ keys.T\n","    for query_pos in range(0, queries.shape[0]):\n","        for weight_pos in range(query_pos + 1, dim):\n","            raw_weights[query_pos][weight_pos] = -1 * Infinity\n","\n","    scale_factor = sqrt(dim)\n","    scaled_weights = softmax(raw_weights / scale_factor, dim=1)\n","\n","    # now scaled weights is a matrix where each row represents the scaled weights produced based on a given query.\n","    # meanwhile values just has a value vector on each row.\n","\n","    reshaped_scaled_weights = scaled_weights.view(scaled_weights.shape[0], scaled_weights.shape[1], 1)\n","    reshaped_values = values.view(1, values.shape[0], values.shape[1])\n","\n","    scaled_values = reshaped_scaled_weights * reshaped_values\n","\n","    contextualized_values = torch.sum(scaled_values, 1)\n","    return contextualized_values"]},{"cell_type":"code","execution_count":36,"metadata":{},"outputs":[{"data":{"text/plain":["tensor([[0.0000, 5.0000],\n","        [2.9150, 2.0850]])"]},"execution_count":36,"metadata":{},"output_type":"execute_result"}],"source":["Q = tensor([[1, 3], [1, 1]]).float()\n","K = tensor([[2, 1], [3, 5]]).float()\n","V = tensor([[0, 5], [3, 2]]).float()\n","d = 2\n","par_attention(Q, K, V, d)\n"]},{"cell_type":"markdown","metadata":{},"source":["That looks like it's working right to me, which is great! I now have masking implemented and a much more fully validated, parallel attention function."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
