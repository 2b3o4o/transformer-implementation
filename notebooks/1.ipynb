{"cells":[{"cell_type":"markdown","metadata":{},"source":["My initial goal is just to scope out some details of the transformer architecture in pytorch code. This should both help me understand the architecture and get a head start on the actual implementation. If possible I think I want to leave out attention until I have some kind of testing set up."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import torch.nn as nn\n","\n","class AttentionHead(nn.Module):\n","    def __init__(self, )\n","\n","class TransformerLayer(nn.Module):\n","    def __init__(self, transformer_dims, att_key_dims, att_value_dims):\n","        super().__init__()\n","\n","        # TODO: implement attention\n","        self.attention = nn.Linear(in_features=transformer_dims, out_features=transformer_dims)\n","        self.norm1 = nn.BatchNorm1d()\n","        self.linear = nn.Linear(in_features=transformer_dims, out_features=transformer_dims)\n","        self.norm2 = nn.BatchNorm1d()\n","\n","    def forward(self, x):\n","        x_res = x\n","        x = self.attention(x)\n","        x += x_res\n","        x = self.norm1(x)\n","\n","        x_res = x\n","        x = self.linear(x)\n","        x += x_res\n","        x = self.norm2(x)\n","\n","        return x\n","\n","class TransformerNetwork(nn.Module):\n","    def __init__(self, num_layers, model_dims, att_heads, att_key_dims, att_value_dims, ff_dims):\n","        super().__init__()\n","\n","        params = {\"att_heads\": att_heads, \"transformer_dims\": model_dims, \"att_key_dims\": att_key_dims,\n","            \"att_value_dims\": att_value_dims, \"ff_dims\": ff_dims}\n","        self.trans_layers = [TransformerLayer(**params) for _ in range(0, num_layers)]\n","\n","    def forward(self, x):\n","        for layer in self.trans_layers:\n","            x = self.layer.forward(x)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model = TransformerNetwork(num_layers=6, model_dims=512, att_heads=8, att_key_dims=512//8, att_value_dims=512//8, ff_dims=2048)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
